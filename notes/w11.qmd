---
title: "Lecture Note Week 12"
filters: 
  - webr
execute: 
  echo: false
---

::: callout-note
Learning Objective:

LO1: Evaluate out-of-sample predictive performance of a model.

LO2: Compute and interpret performance measures for classification methods.

LO3: Describe and apply logistic regression.

LO4: Describe and apply classification trees.
:::

```{r}
#| include: false

library(tidyverse)
library(knitr)
```

### 1. Introduction to

Evaluating the out-of-sample predictive performance of a model is crucial in determining its generalizability to new, unseen data. Two common methods for this evaluation are the static holdout method and k-fold cross-validation. This lecture note will cover both methods, their implementation, advantages, and limitations.

### 2. Static Holdout Method

#### 2.1. Concept

The static holdout method involves splitting the dataset into two distinct sets:

-   **Training Set:** Used to train the model.

-   **Testing (Holdout) Set:** Used to evaluate the model’s performance on unseen data.

#### 2.2. Implementation

1.  **Split the Dataset:**

    -   Typically, a common split ratio is 70-80% for training and 20-30% for testing.

    ```         
    r
    ```

    Copy code

    `# Assuming data is in a data frame df and target variable is y set.seed(42) sample <- sample.int(n = nrow(df), size = floor(.8*nrow(df)), replace = F) train <- df[sample, ] test <- df[-sample, ]`

2.  **Train the Model:**

    -   Fit the model on the training data.

    ```         
    r
    ```

    Copy code

    `# Assuming using a linear model model <- lm(y ~ ., data = train)`

3.  **Evaluate the Model:**

    -   Predict on the test data and evaluate performance.

    ```         
    r
    ```

    Copy code

    `predictions <- predict(model, newdata = test) actuals <- test$y mse <- mean((predictions - actuals)^2)  # Mean Squared Error`

#### 2.3. Advantages and Limitations

-   **Advantages:**

    -   Simple to implement and understand.

    -   Computationally efficient.

-   **Limitations:**

    -   Performance estimate can be highly dependent on the particular train-test split.

    -   Not ideal for small datasets.

### 3. K-Fold Cross-Validation

#### 3.1. Concept

K-fold cross-validation involves splitting the dataset into `k` equally sized folds. The model is trained `k` times, each time using `k-1` folds for training and the remaining fold for testing. The average performance across all `k` iterations is then used as the final performance estimate.

#### 3.2. Implementation

1.  **Split the Dataset into K Folds:**

    -   Typically, `k` is set to 5 or 10.

    ```         
    r
    ```

    Copy code

    `library(caret) set.seed(42) folds <- createFolds(df$y, k = 5, list = TRUE)`

2.  **Train and Evaluate the Model:**

    -   Iterate over each fold, train the model on `k-1` folds and evaluate on the remaining fold.

    ```         
    r
    ```

    Copy code

    \`results \<- lapply(folds, function(fold) { train \<- df\[-fold, \] test \<- df\[fold, \]

    model \<- lm(y \~ ., data = train) predictions \<- predict(model, newdata = test) actuals \<- test\$y mse \<- mean((predictions - actuals)\^2) return(mse) })

    mean_mse \<- mean(unlist(results)) \# Average Mean Squared Error\`

#### 3.3. Advantages and Limitations

-   **Advantages:**

    -   More reliable performance estimate as it reduces variability due to data splitting.

    -   Utilizes the entire dataset for both training and testing, ensuring no data is wasted.

-   **Limitations:**

    -   More computationally intensive due to multiple training iterations.

    -   Not as simple as the static holdout method.

### 4. Conclusion

Both the static holdout method and k-fold cross-validation are essential tools for evaluating the out-of-sample predictive performance of a model. The static holdout method is simpler and faster, suitable for large datasets. In contrast, k-fold cross-validation provides a more robust performance estimate, especially valuable for small to medium-sized datasets. Understanding and correctly implementing these methods will enhance the reliability of your model's performance assessment.

## Lecture Notes: Computing and Interpreting Performance Measures for Classification Methods

### 1. Introduction

Evaluating the performance of classification models is crucial for understanding their effectiveness in predicting categorical outcomes. This lecture note covers key performance measures, their computation, and interpretation for classification methods.

### 2. Key Performance Measures

#### 2.1. Confusion Matrix

A confusion matrix is a table that summarizes the performance of a classification model by comparing the actual and predicted classifications.

-   **True Positive (TP):** Correctly predicted positive cases.

-   **True Negative (TN):** Correctly predicted negative cases.

-   **False Positive (FP):** Incorrectly predicted positive cases (Type I error).

-   **False Negative (FN):** Incorrectly predicted negative cases (Type II error).

#### 2.2. Accuracy

Accuracy measures the proportion of correct predictions (both true positives and true negatives) among the total number of cases.

Accuracy=TP+TNTP+TN+FP+FN\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}Accuracy=TP+TN+FP+FNTP+TN​

```         
r
```

Copy code

`accuracy <- (TP + TN) / (TP + TN + FP + FN)`

#### 2.3. Precision

Precision (also called Positive Predictive Value) measures the proportion of true positive predictions among all positive predictions.

Precision=TPTP+FP\text{Precision} = \frac{TP}{TP + FP}Precision=TP+FPTP​

```         
r
```

Copy code

`precision <- TP / (TP + FP)`

#### 2.4. Recall

Recall (also called Sensitivity or True Positive Rate) measures the proportion of true positive cases among all actual positive cases.

Recall=TPTP+FN\text{Recall} = \frac{TP}{TP + FN}Recall=TP+FNTP​

```         
r
```

Copy code

`recall <- TP / (TP + FN)`

#### 2.5. F1 Score

The F1 Score is the harmonic mean of precision and recall, providing a balance between the two.

F1 Score=2⋅Precision⋅RecallPrecision+Recall\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}F1 Score=2⋅Precision+RecallPrecision⋅Recall​

```         
r
```

Copy code

`f1_score <- 2 * (precision * recall) / (precision + recall)`

#### 2.6. Specificity

Specificity (also called True Negative Rate) measures the proportion of true negative cases among all actual negative cases.

Specificity=TNTN+FP\text{Specificity} = \frac{TN}{TN + FP}Specificity=TN+FPTN​

```         
r
```

Copy code

`specificity <- TN / (TN + FP)`

### 3. Implementation in R

Assuming you have a binary classification model and a dataset with actual and predicted values, here's how to compute these performance measures in R.

1.  **Generate Predictions:**

    ```         
    r
    ```

    Copy code

    `# Example using a logistic regression model model <- glm(y ~ ., data = train, family = binomial) predictions <- predict(model, newdata = test, type = "response") predicted_classes <- ifelse(predictions > 0.5, 1, 0)`

2.  **Create a Confusion Matrix:**

    ```         
    r
    ```

    Copy code

    `library(caret) confusion_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(test$y))`

3.  **Extract Performance Measures:**

    ```         
    r
    ```

    Copy code

    \`confusion \<- confusion_matrix\$table TP \<- confusion\[2, 2\] TN \<- confusion\[1, 1\] FP \<- confusion\[1, 2\] FN \<- confusion\[2, 1\]

    accuracy \<- (TP + TN) / (TP + TN + FP + FN) precision \<- TP / (TP + FP) recall \<- TP / (TP + FN) f1_score \<- 2 \* (precision \* recall) / (precision + recall) specificity \<- TN / (TN + FP)\`

### 4. Interpretation

-   **Accuracy:** High accuracy indicates that the model correctly classifies most cases. However, it may be misleading if the classes are imbalanced.

-   **Precision:** High precision means that when the model predicts a positive case, it is usually correct. It is important when the cost of false positives is high.

-   **Recall:** High recall means that the model can identify most actual positive cases. It is important when the cost of false negatives is high.

-   **F1 Score:** A balanced measure that is useful when you need to balance precision and recall.

-   **Specificity:** High specificity means that the model can identify most actual negative cases. It is important when the cost of false positives is high.

### 5. Conclusion

Understanding and correctly interpreting performance measures are essential for evaluating the effectiveness of classification models. Each measure provides unique insights, and the choice of which to prioritize depends on the specific context and costs associated with false positives and false negatives.

## Logistic Regression

### 1. Introduction

Logistic regression is a powerful statistical method used for binary classification problems. It is widely applied to scenarios where the outcome variable is binary (e.g., yes/no, success/failure). This lecture note will describe the logistic regression model and demonstrate its application using examples inspired by the "Business Analytics" book by Jeffrey D. Camm and others.

### 2. Logistic Regression Model

#### 2.1. Concept

Logistic regression models the probability of a binary outcome using a logistic function. Unlike linear regression, which predicts continuous outcomes, logistic regression predicts the probability that a given input point belongs to a particular class.

The logistic function (also known as the sigmoid function) is defined as:

P(Y=1∣X)=11+e−(β0+β1X1+β2X2+…+βkXk)P(Y=1\|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_kX_k)}}P(Y=1∣X)=1+e−(β0​+β1​X1​+β2​X2​+…+βk​Xk​)1​

Where:

-   P(Y=1∣X)P(Y=1\|X)P(Y=1∣X) is the probability that the outcome YYY is 1 given the input XXX.

-   β0,β1,…,βk\beta\_0, \beta\_1, \ldots, \beta\_kβ0​,β1​,…,βk​ are the coefficients of the model.

#### 2.2. Odds and Logit

-   **Odds:** The odds of an event are the ratio of the probability of the event occurring to the probability of it not occurring.

Odds=P(Y=1∣X)1−P(Y=1∣X)\text{Odds} = \frac{P(Y=1|X)}{1 - P(Y=1|X)}Odds=1−P(Y=1∣X)P(Y=1∣X)​

-   **Logit:** The logit function is the natural logarithm of the odds.

Logit(P)=log⁡(P1−P)=β0+β1X1+β2X2+…+βkXk\text{Logit}(P) = \log\left(\frac{P}{1-P}\right) = \beta\_0 + \beta\_1X_1 + \beta\_2X_2 + \ldots + \beta\_kX_kLogit(P)=log(1−PP​)=β0​+β1​X1​+β2​X2​+…+βk​Xk​

### 3. Applying Logistic Regression in R

#### 3.1. Data Preparation

1.  **Load the Data:**

    ```         
    r
    ```

    Copy code

    `# Example dataset data <- read.csv("path_to_dataset.csv")`

2.  **Split the Data into Training and Testing Sets:**

    ```         
    r
    ```

    Copy code

    `set.seed(123) sample <- sample.int(n = nrow(data), size = floor(0.8*nrow(data)), replace = FALSE) train <- data[sample, ] test <- data[-sample, ]`

#### 3.2. Model Training

1.  **Fit the Logistic Regression Model:**

    ```         
    r
    ```

    Copy code

    `model <- glm(binary_outcome ~ ., data = train, family = binomial)`

2.  **View Model Summary:**

    ```         
    r
    ```

    Copy code

    `summary(model)`

#### 3.3. Model Evaluation

1.  **Make Predictions:**

    ```         
    r
    ```

    Copy code

    `predictions <- predict(model, newdata = test, type = "response") predicted_classes <- ifelse(predictions > 0.5, 1, 0)`

2.  **Create a Confusion Matrix:**

    ```         
    r
    ```

    Copy code

    `library(caret) confusion_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(test$binary_outcome)) print(confusion_matrix)`

3.  **Calculate Performance Metrics:**

    ```         
    r
    ```

    Copy code

    \`confusion \<- confusion_matrix\$table TP \<- confusion\[2, 2\] TN \<- confusion\[1, 1\] FP \<- confusion\[1, 2\] FN \<- confusion\[2, 1\]

    accuracy \<- (TP + TN) / (TP + TN + FP + FN) precision \<- TP / (TP + FP) recall \<- TP / (TP + FN) f1_score \<- 2 \* (precision \* recall) / (precision + recall) specificity \<- TN / (TN + FP)

    cat("Accuracy:", accuracy, "\n") cat("Precision:", precision, "\n") cat("Recall:", recall, "\n") cat("F1 Score:", f1_score, "\n") cat("Specificity:", specificity, "\n")\`

### 4. Interpretation

-   **Coefficients:** The coefficients (β values) in the logistic regression model indicate the change in the log odds of the outcome for a one-unit increase in the predictor variable.

-   **Odds Ratio:** The exponentiated coefficients (e\^β) represent the odds ratio, showing how the odds of the outcome change with a one-unit increase in the predictor.

-   **Performance Metrics:** The confusion matrix and derived metrics (accuracy, precision, recall, F1 score, specificity) help evaluate the model's performance on the test data.

#### 5.1. Example Scenario: Predicting Customer Churn

-   **Dataset:** Customer data with features like age, account length, service usage, etc.

-   **Binary Outcome:** Whether the customer churned (1) or not (0).

```         
r
```

Copy code

\`# Example code based on a hypothetical dataset data \<- read.csv("customer_churn.csv") set.seed(123) sample \<- sample.int(n = nrow(data), size = floor(0.8\*nrow(data)), replace = FALSE) train \<- data\[sample, \] test \<- data\[-sample, \]

# Fit logistic regression model

model \<- glm(churn \~ age + account_length + service_usage, data = train, family = binomial)

# Summary of the model

summary(model)

# Predictions

predictions \<- predict(model, newdata = test, type = "response") predicted_classes \<- ifelse(predictions \> 0.5, 1, 0)

# Confusion matrix

library(caret) confusion_matrix \<- confusionMatrix(as.factor(predicted_classes), as.factor(test\$churn)) print(confusion_matrix)

# Performance metrics

confusion \<- confusion_matrix\$table TP \<- confusion\[2, 2\] TN \<- confusion\[1, 1\] FP \<- confusion\[1, 2\] FN \<- confusion\[2, 1\]

accuracy \<- (TP + TN) / (TP + TN + FP + FN) precision \<- TP / (TP + FP) recall \<- TP / (TP + FN) f1_score \<- 2 \* (precision \* recall) / (precision + recall) specificity \<- TN / (TN + FP)

cat("Accuracy:", accuracy, "\n") cat("Precision:", precision, "\n") cat("Recall:", recall, "\n") cat("F1 Score:", f1_score, "\n") cat("Specificity:", specificity, "\n")\`

### 6. Conclusion

Logistic regression is a fundamental tool for binary classification problems. Understanding how to apply and interpret logistic regression models is essential for making informed business decisions. By using the examples and techniques from the "Business Analytics" book, you can effectively implement logistic regression in your data analysis projects.

### . Introduction

A classification tree is a decision tree that is used to classify observations into categories. It is a non-parametric method that splits the data into subsets based on the value of input features, leading to a tree structure where each node represents a decision rule and each leaf represents a class label. This lecture note will describe the classification tree model and demonstrate its application using examples inspired by the "Business Analytics" book by Jeffrey D. Camm and others.

### 2. Classification Tree Model

#### 2.1. Concept

A classification tree is built by recursively partitioning the dataset into subsets based on the value of input features. The objective is to create subsets that are as homogeneous as possible concerning the target variable.

#### 2.2. Tree Structure

-   **Root Node:** The topmost node representing the entire dataset.

-   **Internal Nodes:** Nodes that split into further nodes based on a decision rule.

-   **Leaf Nodes:** Terminal nodes that represent the final class label.

#### 2.3. Splitting Criteria

Common criteria for splitting nodes include:

-   **Gini Index:** Measures impurity. Lower values indicate more homogeneous nodes.

-   **Entropy:** Measures disorder. Lower values indicate more homogeneous nodes.

-   **Misclassification Error:** Measures the frequency of incorrect classifications.

### 3. Applying Classification Trees in R

#### 3.1. Data Preparation

1.  **Load the Data:**

    ```         
    r
    ```

    Copy code

    `# Example dataset data <- read.csv("path_to_dataset.csv")`

2.  **Split the Data into Training and Testing Sets:**

    ```         
    r
    ```

    Copy code

    `set.seed(123) sample <- sample.int(n = nrow(data), size = floor(0.8*nrow(data)), replace = FALSE) train <- data[sample, ] test <- data[-sample, ]`

#### 3.2. Model Training

1.  **Fit the Classification Tree:**

    ```         
    r
    ```

    Copy code

    `library(rpart) model <- rpart(binary_outcome ~ ., data = train, method = "class")`

2.  **View the Tree:**

    ```         
    r
    ```

    Copy code

    `library(rpart.plot) rpart.plot(model)`

#### 3.3. Model Evaluation

1.  **Make Predictions:**

    ```         
    r
    ```

    Copy code

    `predictions <- predict(model, newdata = test, type = "class")`

2.  **Create a Confusion Matrix:**

    ```         
    r
    ```

    Copy code

    `library(caret) confusion_matrix <- confusionMatrix(predictions, as.factor(test$binary_outcome)) print(confusion_matrix)`

3.  **Calculate Performance Metrics:**

    ```         
    r
    ```

    Copy code

    \`confusion \<- confusion_matrix\$table TP \<- confusion\[2, 2\] TN \<- confusion\[1, 1\] FP \<- confusion\[1, 2\] FN \<- confusion\[2, 1\]

    accuracy \<- (TP + TN) / (TP + TN + FP + FN) precision \<- TP / (TP + FP) recall \<- TP / (TP + FN) f1_score \<- 2 \* (precision \* recall) / (precision + recall) specificity \<- TN / (TN + FP)

    cat("Accuracy:", accuracy, "\n") cat("Precision:", precision, "\n") cat("Recall:", recall, "\n") cat("F1 Score:", f1_score, "\n") cat("Specificity:", specificity, "\n")\`

### 4. Interpretation

-   **Tree Structure:** The tree diagram provides a visual representation of the decision rules used to classify observations.

-   **Decision Rules:** Each internal node represents a decision rule based on a feature and a threshold value.

-   **Performance Metrics:** The confusion matrix and derived metrics (accuracy, precision, recall, F1 score, specificity) help evaluate the model's performance on the test data.

### 5. Example from "Business Analytics"

In "Business Analytics" by Jeffrey D. Camm and others, classification trees are often applied to real-world business problems, such as customer segmentation, fraud detection, or risk assessment.

#### 5.1. Example Scenario: Predicting Customer Churn

-   **Dataset:** Customer data with features like age, account length, service usage, etc.

-   **Binary Outcome:** Whether the customer churned (1) or not (0).

```         
r
```

Copy code

\`# Example code based on a hypothetical dataset data \<- read.csv("customer_churn.csv") set.seed(123) sample \<- sample.int(n = nrow(data), size = floor(0.8\*nrow(data)), replace = FALSE) train \<- data\[sample, \] test \<- data\[-sample, \]

# Fit classification tree

library(rpart) model \<- rpart(churn \~ age + account_length + service_usage, data = train, method = "class")

# Visualize the tree

library(rpart.plot) rpart.plot(model)

# Predictions

predictions \<- predict(model, newdata = test, type = "class")

# Confusion matrix

library(caret) confusion_matrix \<- confusionMatrix(predictions, as.factor(test\$churn)) print(confusion_matrix)

# Performance metrics

confusion \<- confusion_matrix\$table TP \<- confusion\[2, 2\] TN \<- confusion\[1, 1\] FP \<- confusion\[1, 2\] FN \<- confusion\[2, 1\]

accuracy \<- (TP + TN) / (TP + TN + FP + FN) precision \<- TP / (TP + FP) recall \<- TP / (TP + FN) f1_score \<- 2 \* (precision \* recall) / (precision + recall) specificity \<- TN / (TN + FP)

cat("Accuracy:", accuracy, "\n") cat("Precision:", precision, "\n") cat("Recall:", recall, "\n") cat("F1 Score:", f1_score, "\n") cat("Specificity:", specificity, "\n")\`

### 6. Conclusion

Classification trees are intuitive and powerful tools for binary classification problems. They provide a clear visual representation of decision rules, making them easy to interpret.
