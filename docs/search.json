[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETF2121/5912: Data Analysis in Business",
    "section": "",
    "text": "This page provides a comprehensive outline of the topics, content, and assignments planned for the semester. Please note that this schedule is subject to updates as the semester progresses. Any changes or adjustments will be documented here to ensure you have the most current information available. In the event of any discrepancies between the information on this website and the exam timetable or course handbook, the exam timetable and course handbook will serve as the authoritative sources.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nSlides\nQuiz/Assignments\nReading\n\n\n\n\n1\nTue, Jul 23\nBusiness Analytics & Database Management System\n📖\n📋\n📝\n📕 Camm 1.1, 1.2, 1.3 and 1.5\n\n\n2\nTue, Jul 30\nDatabase Query Language: SQL\n📖\n📋\n📝\n✍️\n\n\n3\nTue, Aug 6\nData Wrangling\n📖\n📋\n📝\n\n\n\n4\nTue, Aug 13\nData Visualization\n📖\n📋\n📝\n📕 Camm 3.1, 3.2, 3.6\n\n\n5\nTue, Aug 20\nDescriptive Statistics\n📖\n📋 📋\n📝\n📕 Camm Chapter 2\n\n\n6\nTue, Aug 27\nProbability: An Introduction to Modelling Uncertainty\n📖\n📋\n📝\n📕 Camm Chapter 5\n\n\n7\nTue, Sep 3\nDescriptive Analytics: Clustering\n\n\n\n\n\n\n\nWed, Sep 4\nIndividual assignment submission deadline\n\n\n\n\n\n\n8\nTue, Sep 10\nLinear Regression\n\n\n\n\n\n\n9\nTue, Sep 17\nTime Series Analysis and Forecasting\n\n\n\n\n\n\nSemester Break\nSep 23 - Sep 27\nMid semester break\n\n\n\n\n\n\n10\nTue, Oct 1\nPredictive Analytics: Continuous Outcome Variables\n\n\n\n\n\n\n11\nTue, Oct 8\nPredictive Analytics: Binary Outcome Variables\n\n\n\n\n\n\n\nWed, Oct 9\nGroup Assignment Submission Deadline\n\n\n\n\n\n\n12\nTue, Oct 15\nRevision/Presentation\n\n\n\n\n\n\nSWOT\nOct 21 - Oct 25\nSWOT\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "ETF 2121/5912: Data Analysis in Business",
    "section": "",
    "text": "Welcome to Data Analysis in Business. This unit provides an overview of fundamental tools of data and statistical analysis used in the Business and Economics disciplines. The methods covered are widely used in industry and academia, providing the necessary foundation for more advanced approaches as you advance your training and career.\n\nThis course delves deeper into basic statistical concepts, with a focus on their application in finance, accounting, and other sectors. It introduces cutting-edge software and programming languages such as R, Power-BI, SQL and Excel for robust data analysis and predictive modelling. Students will engage with modern data sources and advanced sampling techniques, learn to perform hypothesis testing, and apply modelling techniques such as regression and time series analysis in business contexts. The course places a strong emphasis on practical applications, aiming to equip students with the skills to interpret and leverage statistical data for solving business problems and making informed decisions in industry.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-syllabus.html",
    "href": "course-syllabus.html",
    "title": "Syllabus/Outline/Course Information",
    "section": "",
    "text": "This unit equips students with statistical techniques and the use of modern business software, including R, Excel, Power BI, for business applications. Students will learn to process and analyse extensive datasets using methods such as regression, time series analysis and classification. Key topics include modeling, regression analysis, and time series analysis in the business context.",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-synopsis",
    "href": "course-syllabus.html#unit-synopsis",
    "title": "Syllabus/Outline/Course Information",
    "section": "",
    "text": "This unit equips students with statistical techniques and the use of modern business software, including R, Excel, Power BI, for business applications. Students will learn to process and analyse extensive datasets using methods such as regression, time series analysis and classification. Key topics include modeling, regression analysis, and time series analysis in the business context.",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-objectives",
    "href": "course-syllabus.html#unit-objectives",
    "title": "Syllabus/Outline/Course Information",
    "section": "Unit Objectives",
    "text": "Unit Objectives\nOn successful completion of this unit, you should be able to:\n\nGain proficiency in business reporting tools and data analysis tool.\nLearn to perform complex statistical analyses, including hypothesis testing and confidence intervals.\nUse various predictive modeling technique to enhance forecasting and decision-making in business context.\nAcquire skills to analyze and interpret real-time data for better business strategic planning and decision.",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "course-syllabus.html#unit-outline",
    "href": "course-syllabus.html#unit-outline",
    "title": "Syllabus/Outline/Course Information",
    "section": "Unit Outline",
    "text": "Unit Outline\n\nBusiness Analytics & Database Management System\nStructured Query Language\nData Wrangling with R and Power BI\nData Visualization with Power BI\nDescriptive Statistics with R and Power BI\nModelling Uncertainty\nDescriptive Analytics\nLinear Regression\nTime Series Analysis\nPredictive Analytics: Continuous Outcome Variables\nPredictive Analytics: Binary Outcome Variables\nRevision",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "course-syllabus.html#textbooks",
    "href": "course-syllabus.html#textbooks",
    "title": "Syllabus/Outline/Course Information",
    "section": "Textbooks",
    "text": "Textbooks\nThe textbook for this course is just a supplement to the lectures and labs. The main resources for this course are the lecture notes, lab materials, and the course website. However, the following textbooks are recommended for additional reading:\n\nBusiness Analytics by Camm, Cochran, Fry, Ohlmann, Anderson & Sweeney, 2024, 5th Edition.\nAdvanced Analytics with Power BI and R by Dr. Leila Etaati, 2020.",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "course-syllabus.html#assessment",
    "href": "course-syllabus.html#assessment",
    "title": "Syllabus/Outline/Course Information",
    "section": "Assessment",
    "text": "Assessment\n\nWithin semester assessment: 50%\n\n\nExamination: 50%\n\n\n\nAssessments\nWeight\nDue Date\nLearning Outcomes\n\n\n\n\nQuizzes\n10%\nWeekly\nWeeks 2-12\n\n\nIndividual Assignment\n20%\nWeek 7\nWeeks 1-5\n\n\nGroup Assignment*\n20%\nWeek 11\nWeeks 1-10\n\n\nFinal exam\n50%\nExam period\nWeeks 1-11\n\n\n\n\nThe lowest quiz grade will be dropped at the end of the semester.\nIt is your responsibility to check your assignment grading and to report any discrepancies within one week of the grade being posted.\n*Maximum of 3 persons and minimum of 2 per group.",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "course-syllabus.html#course-info",
    "href": "course-syllabus.html#course-info",
    "title": "Syllabus/Outline/Course Information",
    "section": "Course info",
    "text": "Course info\n\n\n\n\nDay\nTime\nLocation\n\n\n\n\nLectures\nTue\n2.00 pm - 3.30 pm\nK321, Caulfield Campus\n\n\nTutorial 01\nTue\n5.00 pm - 6:30 pm\nC302A\n\n\nTutorial 02\nTue\n9.30 am - 12:00 pm\nN122\n\n\nTutorial 03\nTue\n11.00 am - 12:30 pm\nN122\n\n\nTutorial 04\nTue\n6.30 pm - 7:30 pm\nC302A\n\n\nTutorial 05\nWed\n9.30 am - 10:30 pm\nN122\n\n\nTutorial 06\nWed\n9.30 am - 10:30 pm\nN122\n\n\nTutorial 07\nWed\n3.30 am - 5:00 pm\nH226\n\n\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website at here.\nI will regularly send course announcements via Ed discussion board, make sure to check one or the other of these regularly. It is your responsibility to stay informed about the course progress and any changes to the schedule.\n\n\nWhere to get help\n\nIf you have a question during lecture or tutorial, feel free to ask your teaching team or post your question in the discussion board. There are likely other students with the same question, and it is helpful to have the answer available to everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend the designated consultation hours to ask questions about the course content and assignments. Those consultation hours are available every day except weekend!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the Ed discussion board here. Please check whether your question has already been answered before posting. Please help each others by answering other questions, you will notice that you learn a lot by posting/responding to others. Do not expect the teaching team to answer questions that are posted during the weekend in the discussion board.\nEmails should be reserved for personal questions that are not appropriate to be discuss in the forum. Please include “ETF2121/5912” in the email subject line. Response time may be slower for emails with 24 hours being the maximum response time except weekend.",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "course-syllabus.html#lectures-and-tutorials",
    "href": "course-syllabus.html#lectures-and-tutorials",
    "title": "Syllabus/Outline/Course Information",
    "section": "Lectures and tutorials",
    "text": "Lectures and tutorials\nOur goal in both lectures and labs is to encourage maximum interaction. As your instructor, I will introduce you to new tools and techniques, but it’s up to you to apply them effectively. Coding is best learned through practice, so much of our coursework will involve hands-on coding tasks. Each session will include various activities and assignments designed to enhance your skills.\nAttendance and active participation in both lectures and tutorials are expected. Attending lectures and tutorials in person will provide the best learning experience. Some lectures will feature application exercises and periodic activities to foster a supportive learning community. These activities will be brief and enjoyable, aimed at strengthening connections among classmates throughout the semester.\n\nExams\nFinal exam. The exam will be held during the exam period, and it will be a closed-book exam with 5 pieces of A4 paper (10 pages) of handwritten/typed notes allowed. It is a supervised exam, and you will not be allowed to access the internet.\n\n\nSoftware\nAll software used in this course can be downloaded for free. However, Power BI will not be available for Mac users, so I would encourage you to access it via MOVE. Please refer to this instruction.\n\n\nLate work submission policy\nAssignment deadlines are set to facilitate your progression through the course content and to enable timely feedback from the teaching team. I understand that unforeseen circumstances may occasionally arise, hindering your ability to meet a deadline. In such cases, you can request an extension through the Monash extension unit for extenuating circumstances. Please note that I do not grant individual approvals for late submissions, and requests should solely be made through the Monash extension unit.\n\nThere will be a 10% deduction for each 24-hour period the assignment is late.\nIf you submit an assessment task more than seven days after the due date, you’ll receive a mark of zero and you won’t get any feedback.\nStrictly no late submission without approval.\nI will not reply to any email related to request on late submission.",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "course-syllabus.html#important-dates",
    "href": "course-syllabus.html#important-dates",
    "title": "Syllabus/Outline/Course Information",
    "section": "Important dates",
    "text": "Important dates\n\n23rd July 2024: Classes begin (Tuesday)\nEvery Sunday: Quiz closed at 11.55pm\n4th September 2024: Individual assignment submission deadline\n9th October 2024: Group assignment submission deadline",
    "crumbs": [
      "Course information",
      "Course structure"
    ]
  },
  {
    "objectID": "exams/exam-2.html",
    "href": "exams/exam-2.html",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 25 at 9 am ET and must be completed by Mon, Feb 28 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-2- repo to complete Part 2 of your exam. Add your answers to the exam-2.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 28 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-2.html#overview",
    "href": "exams/exam-2.html#overview",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 25 at 9 am ET and must be completed by Mon, Feb 28 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-2- repo to complete Part 2 of your exam. Add your answers to the exam-2.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 28 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-2.html#academic-integrity",
    "href": "exams/exam-2.html#academic-integrity",
    "title": "Exam 2",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-2.html#rules-notes",
    "href": "exams/exam-2.html#rules-notes",
    "title": "Exam 2",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am)."
  },
  {
    "objectID": "exams/exam-2.html#submission",
    "href": "exams/exam-2.html#submission",
    "title": "Exam 2",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-1.html",
    "href": "exams/exam-1.html",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am).\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-1.html#overview",
    "href": "exams/exam-1.html#overview",
    "title": "Exam 1",
    "section": "",
    "text": "Exam 1 will be released on Friday, Feb 4 at 9 am ET and must be completed by Mon, Feb 7 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes. This portion may only be submitted one time. Go here to complete Part 1 of the exam.\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope (like a usual lab and homework). Go to the GitHub organization for the course and find the exam-1- repo to complete Part 3 of your exam. Add your answers to the exam-1.qmd file in your repo.\n\nBoth portions must be completed and submitted by Mon, Feb 7 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-1.html#academic-integrity",
    "href": "exams/exam-1.html#academic-integrity",
    "title": "Exam 1",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-1.html#rules-notes",
    "href": "exams/exam-1.html#rules-notes",
    "title": "Exam 1",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor or the head TA (Rick Presman) on Slack or post on Sakai Conversations for “Instructors in this site” only or attend my office hours (Monday, 10:30 am - 11:30 am)."
  },
  {
    "objectID": "exams/exam-1.html#submission",
    "href": "exams/exam-1.html#submission",
    "title": "Exam 1",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "course-faq.html",
    "href": "course-faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "href": "course-faq.html#how-do-i-export-my-assignment-pdf-from-rstudio-to-upload-to-gradescope",
    "title": "FAQ",
    "section": "",
    "text": "Go to your Files tab, check the box next to the file you want to download, then click on the blue gear icon on the Files tab to reveal the drop down menu, and select Export… If you have selected multiple files to export, RStudio will zip them up into a single zip file for you. If you’ve selected just a single file, it will only download that. The downloaded file will go to wherever files you download off the internet goes on your computer (usually your Downloads folder)."
  },
  {
    "objectID": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "href": "course-faq.html#how-can-i-submit-my-assignment-to-gradescope",
    "title": "FAQ",
    "section": "How can I submit my assignment to Gradescope?",
    "text": "How can I submit my assignment to Gradescope?\nThe instructions for submitting your assignment to Gradescope can be found here. In a nutshell, you’ll upload your PDF and them mark the page(s) where each question can be found. It’s OK if a question spans multiple pages, just mark them all. It’s also OK if a page includes multiple questions."
  },
  {
    "objectID": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "href": "course-faq.html#can-i-use-a-local-install-of-r-and-rstudio-instead-of-using-the-rstudio-containers",
    "title": "FAQ",
    "section": "Can I use a local install of R and RStudio instead of using the RStudio containers?",
    "text": "Can I use a local install of R and RStudio instead of using the RStudio containers?\nThe short answer is, I’d rather you didn’t, to save yourself some headache. But, the long answer is, sure! But you will need to install a specific versions of R and RStudio for everything to work as expected. You will also need to install the R packages we’re using as well as have Git installed on your computer. These are not extremely challenging things to get right, but they are not trivial either, particularly on certain operating systems. Myself and the TAs are always happy to provide help with any computational questions when you’re working in the containers we have provided for you. If you’re working on your local setup, we can’t guarantee being able to resolve your issues, though we’re happy to try.\nIf you want to take this path, here is what you need to do:\n\nDownload and install R 4.1.2: https://cran.r-project.org/\nDownload and install a daily build of RStudio: https://dailies.rstudio.com/\nInstall Quarto CLI: https://quarto.org/docs/getting-started/installation.html\nInstall Git: https://happygitwithr.com/install-git.html\nInstall any necessary packages with install.packages(\"___\")\n\nAnd I’d like to reiterate again that successful installation of these software is not a learning goal of this course. So if any of this seems tedious or intimidating in any way, just use the computing environment we have set up for you. More on that here."
  },
  {
    "objectID": "slides/lec-1.html",
    "href": "slides/lec-1.html",
    "title": "Welcome to ETF2121/5912",
    "section": "",
    "text": "Dr. Joan Tan (she/her)\n\n\n\n\n🎓 PhD in Applied Econometrics, Monash University, Australia\n🎓 Certified Analytics Professional (CAP), INFORMS, USA\n🎓 Master of Applied Statistics\n🎓 Bachelor of Economics (Major in Applied Statistics)\nWorking Experience: Associate director, Consultant, data analyst, lecturer\n💌 joan.tan@monash.edu\n\n\n\n\n\n\n\n\n\n\n\nMr. Chin Quek (he/him)\n\n\n\n\n🎓 Master of Public Health, Monash University, Australia\n🎓 Bachelor in Mechanical Engineering, Commerce and Finance\nWorking Experience: Senior analyst, Business analyst\n💌 chin.quek@monash.edu"
  },
  {
    "objectID": "slides/lec-1.html#lecturer-joan-tan",
    "href": "slides/lec-1.html#lecturer-joan-tan",
    "title": "Welcome to ETF2121/5912",
    "section": "Lecturer: Joan Tan",
    "text": "Lecturer: Joan Tan\n\n\n\n\n\nDr. Joan Tan (she/her)\n\n\n\n\n🎓 PhD in Applied Econometrics, Monash University, Australia\n🎓 Certified Analytics Professional (CAP), INFORMS, USA\n🎓 Master of Applied Statistics\n🎓 Bachelor of Economics (Major in Applied Statistics)\nWorking Experience: Associate director, Consultant, data analyst, lecturer\n💌 joan.tan@monash.edu\nConsultation hours: 12-1pm, Tuesday"
  },
  {
    "objectID": "slides/lec-1.html#head-tutor-chin-quek",
    "href": "slides/lec-1.html#head-tutor-chin-quek",
    "title": "Welcome to ETF2121/5912",
    "section": "Head Tutor: Chin Quek",
    "text": "Head Tutor: Chin Quek\n\n\n\n\n\nMr. Chin Quek (he/him)\n\n\n\n\n🎓 Master of Public Health, Monash University, Australia\n🎓 Bachelor in Mechanical Engineering, Commerce and Finance\nWorking Experience: Senior analyst, Business analyst\n💌 chin.quek@monash.edu"
  },
  {
    "objectID": "slides/lec-1.html#course-information",
    "href": "slides/lec-1.html#course-information",
    "title": "Welcome to ETF2121/5912",
    "section": "Course Information",
    "text": "Course Information\n\nCourse Website\nCommunication: Ed Discussion Forum 💬\n\nPlease use the forum for all course-related questions\nWe will aim to respond to questions within 24 hours\nWe do not answer questions via email!\nPlease be respectful and starts the conversation by saying “Hi teaching team”. This course is all about business, so you have to learn the manner in the business world.\nYou can also help each other out by answering others questions! Who answered the most questions will have a bonus mark for participation."
  },
  {
    "objectID": "slides/lec-1.html#course-toolkit",
    "href": "slides/lec-1.html#course-toolkit",
    "title": "Welcome to ETF2121/5912",
    "section": "Course toolkit",
    "text": "Course toolkit\n\n\n\n\n\n\nImportant Software\n\n\nPOWER BI, R, RStudio, SQL\n\n\n\n\nPlease install all the necessary softwares on your computer before your first tutorial.\nWe will provide you with the necessary instructions on how to install the software. The detail can be found in the here."
  },
  {
    "objectID": "slides/lec-1.html#cadence",
    "href": "slides/lec-1.html#cadence",
    "title": "Welcome to ETF2121/5912",
    "section": "Cadence",
    "text": "Cadence\n\n\nTutorial: Try to solve the weekly tutorial questions before the tutorial, and bring your questions to the tutorial!\nTutorial Solution: will be released on Friday.\nLecture: Every Tuesday 2-4pm\nAssignments: There are 2 individual assignments. All deadlines are on Wednesday 11.55pm. Late assignments are strictly not allowed."
  },
  {
    "objectID": "slides/lec-1.html#grading",
    "href": "slides/lec-1.html#grading",
    "title": "Welcome to ETF2121/5912",
    "section": "Grading",
    "text": "Grading\n\n\n\nAssessments\nWeight\n\n\n\n\nQuizzes\n10%\n\n\nAssignment 1\n20%\n\n\nAssignment 2\n20%\n\n\nFinal exam\n50%"
  },
  {
    "objectID": "slides/lec-1.html#course-learning-objectives",
    "href": "slides/lec-1.html#course-learning-objectives",
    "title": "Welcome to ETF2121/5912",
    "section": "Course learning objectives",
    "text": "Course learning objectives\n\nGain proficiency in business reporting tools and data analysis tool.\nLearn to perform complex statistical analyses, including hypothesis testing and confidence intervals.\nUse various predictive modeling technique to enhance forecasting and decision-making in business context.\nAcquire skills to analyze and interpret real-time data for better business strategic planning and decision."
  },
  {
    "objectID": "slides/lec-1.html#five-tips-for-success",
    "href": "slides/lec-1.html#five-tips-for-success",
    "title": "Welcome to ETF2121/5912",
    "section": "Five tips for success 💪",
    "text": "Five tips for success 💪\n\nAsk questions.\nDo the readings about lecture notes, complete all tutorial questions.\nPractice, practice and practice…\nDon’t procrastinate!"
  },
  {
    "objectID": "slides/lec-1.html#week-1-learning-outcome",
    "href": "slides/lec-1.html#week-1-learning-outcome",
    "title": "Welcome to ETF2121/5912",
    "section": "Week 1 learning outcome",
    "text": "Week 1 learning outcome\n\nAble to install Power BI, mySQL, and R.\nAble to use Power BI\nUnderstand and able to identify descriptive, predictive, and prescriptive analytics.\nDescribe applications of analytics for decision making.\nAble to identify, describe various data types"
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n✓ tibble  3.1.6     ✓ dplyr   1.0.7\n✓ tidyr   1.1.4     ✓ stringr 1.4.0\n✓ readr   2.1.1     ✓ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(tidymodels)\n\nRegistered S3 method overwritten by 'tune':\n  method                   from   \n  required_pkgs.model_spec parsnip\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 0.1.4 ──\n\n\n✓ broom        0.7.10         ✓ rsample      0.1.1     \n✓ dials        0.0.10         ✓ tune         0.1.6     \n✓ infer        1.0.1.9000     ✓ workflows    0.2.4     \n✓ modeldata    0.1.1          ✓ workflowsets 0.1.0     \n✓ parsnip      0.1.7          ✓ yardstick    0.0.9     \n✓ recipes      0.2.0          \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(knitr)"
  },
  {
    "objectID": "computing-pipelines.html#simple-linear-regression",
    "href": "computing-pipelines.html#simple-linear-regression",
    "title": "Pipelines",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nModel fitting\nFit model:\n\npenguins_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(body_mass_g ~ flipper_length_mm, data = penguins)\n\nTidy model output:\n\ntidy(penguins_fit)\n\n# A tibble: 2 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5781.     306.       -18.9 5.59e- 55\n2 flipper_length_mm     49.7      1.52      32.7 4.37e-107\n\n\nFormat model output as table:\n\ntidy(penguins_fit) %&gt;%\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5780.831\n305.815\n-18.903\n0\n\n\nflipper_length_mm\n49.686\n1.518\n32.722\n0\n\n\n\n\n\nAugment data with model:\n\naugment(penguins_fit$fit)\n\n# A tibble: 342 × 9\n   .rownames body_mass_g flipper_length_… .fitted  .resid    .hat .sigma .cooksd\n   &lt;chr&gt;           &lt;int&gt;            &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 1                3750              181   3212.  538.   0.00881   394. 8.34e-3\n 2 2                3800              186   3461.  339.   0.00622   394. 2.33e-3\n 3 3                3250              195   3908. -658.   0.00344   393. 4.83e-3\n 4 5                3450              193   3808. -358.   0.00385   394. 1.60e-3\n 5 6                3650              190   3659.   -9.43 0.00469   395. 1.35e-6\n 6 7                3625              181   3212.  413.   0.00881   394. 4.91e-3\n 7 8                4675              195   3908.  767.   0.00344   393. 6.56e-3\n 8 9                3475              193   3808. -333.   0.00385   394. 1.39e-3\n 9 10               4250              190   3659.  591.   0.00469   394. 5.31e-3\n10 11               3300              186   3461. -161.   0.00622   395. 5.23e-4\n# … with 332 more rows, and 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\n\nStatistical inference"
  },
  {
    "objectID": "notes/w1.html",
    "href": "notes/w1.html",
    "title": "Lecture Note Week 1",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Understand business analytics\nLO2: Understand the concept of database management system\nLO3: Understand the concept of SQL"
  },
  {
    "objectID": "notes/w1.html#introduction-to-data-analysis",
    "href": "notes/w1.html#introduction-to-data-analysis",
    "title": "Lecture Note 1",
    "section": "Introduction to Data Analysis",
    "text": "Introduction to Data Analysis"
  },
  {
    "objectID": "notes/w1.html#introduction-to-r-language",
    "href": "notes/w1.html#introduction-to-r-language",
    "title": "Lecture Note 1",
    "section": "Introduction to R Language",
    "text": "Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.\n\n\nBasic of R\n\nR as a Calculator\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nR Objects\nIn R, everything is an object. These objects serve as containers for various types of data. Whether you’re dealing with a single number, a character string (like a word), or a complex structure like the output of a plot or a statistical analysis summary, it’s all represented as an object.\nCreating Objects:\nTo create an object, you simply give it a name. For instance\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this example, an object is created called x and it is assigned the value 5. The &lt;- is the assignment operator. It assigns the value on the right to the object on the left. You can also use = to assign values to objects, but it’s considered bad practice.\nViewing Objects:\nTo view the value of an object, you simply type the name of the object and press enter. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR keeps track of all objects in the current workspace during the session. You can see all the objects in the current workspace by typing ls() in the console.\nOpearations with Objects:\nYou can perform operations with objects. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nObjects names:\nObject names can contain letters, numbers, periods, and underscores. However, they can only starts with letters or underscore and nothing else. They are case-sensitive, so x and X are different objects. They cannot start with a number or a period. If you would like to insist to have numbers or period as the first character, you can use backticks to define the object name. It is called nonsyntactic names. For instance, you can define the following:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExercises:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCreate an object called a and assign the value 10 to it.\nCreate an object called b and assign the value 20 to it.\n\nCreate an object called star and assign the value a + b to it.\nView the value of star.\nWhich of the following is not the syntactic name for an object?\n\nx\nX\n1x\nx1\nx.y\nx_y\n\n\n\n\nData Types\nR has several data types. The most common data types are:\n\nNumeric (double)\n\n\nrepresents eal numbers (e.g., 3.14, 0.0001, 1000.0).\ncan be positive or negative.\ncan be in scientific notation (e.g., 1.23e-5).\nused for continuous data like measurements, weights, heights, etc.\n\n\nCharacter\n\n\nrepresents text data (e.g., “hello”, “world”, “R is fun”).\nmust be enclosed in quotes.\nused for categorical data (e.g., “High School”, “Primary School”, “University”).\n\n\nLogical (boolean)\n\n\nrepresents binary data (e.g., TRUE or FALSE).\nused for logical operations.\n\n\nInteger\n\n\nrepresents whole numbers (e.g., 1, 2, 3, 1000).\ncan be positive or negative.\nused for counting data like number of students, number of cars, etc.\nsometimes you will see it ends with L (e.g., 1L, 2L, 3L, 1000L). This is to indicate that the number is an integer.\n\n\nFactors\n\n\nrepresents categorical data (e.g., “High School”, “Primary School”, “University”).\nused for categorical data.\n\n\nComplex\n\n\nrepresents complex numbers (e.g., 1 + 2i, 3 + 4i, 5 + 6i).\nused for complex data like electrical engineering, physics, etc.\n\nHowever, we seldom deal with complex data types. We will focus on the first four data types.\nExercises:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat is the data type of 3.14?\nWhat is the data type of \"hello\"?\nWhat is the data type of TRUE?\nWhat is the data type of 1L?\nWhat is the data type of factor(\"High School\")?\nWhat is the data type of 1 + 2i?\nWhat is the data type of \"1\"?\n\nNote: You can check your answers using typeof() function.\n\n\nFunctions\nA function is a block of code that performs a specific task. R has a large number of in-built functions and also allows users to define their own functions. We will learn more about how to create functions in the coming weeks. But so far, we will use some of the in-built functions. Anything that starts with ( and end with ) is a function.\nExercises:\nDefine which one is a function below:\n\nmean()\nmedian\nsd()\nvar()\nsum[]\n\n\n\nVectors\n\n\nData Frames\n\n\nPackages\nR packages are collections of functions and data sets developed by the community. They increase the power of R by improving existing base R functionalities, or by adding new ones.\nExercises:\n\nInstall tidyverse package. Explain what do you observe from the console.\nLoad the tidyverse package. Explain what do you observe from the console.\nInstall palmerpenguins package. Explain what do you observe from the console.\nLoad the palmerpenguins package. Explain what do you observe from the console.\nLoad the lmer package. Explain what do you observe from the console."
  },
  {
    "objectID": "exams/exam-3.html",
    "href": "exams/exam-3.html",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 3 is released on Friday, April 15 at 9 am ET and must be completed by Mon, April 18 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes.\n\nGo here to complete Part 1 of the exam.\nThis portion is comprised of 10 multiple choice / fill in the blank questions may only be submitted one time, so start it when you can set aside ~30 minutes to work on it. You will likely be done quicker but it’s best to set aside ample time.\n\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope, like a usual lab and homework.\n\nGo to the GitHub organization for the course and find the exam-3- repo to complete Part 2 of your exam.\nAdd your answers to the exam-3.qmd file in your repo.\nYou can work on this portion at your own pace and come back to it however many times you like until the deadline. I recommend setting aside ~2 hours. You will likely be done quicker, but it’s best to set aside ample time.\n\n\nBoth portions must be completed and submitted by Mon, April 18 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀\n\n\n\nBy taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nThis is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor on Slack or email or request an appointment to meet on Zoom.\n\n\n\n\n\nPart 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "exams/exam-3.html#overview",
    "href": "exams/exam-3.html#overview",
    "title": "Exam 2",
    "section": "",
    "text": "Exam 3 is released on Friday, April 15 at 9 am ET and must be completed by Mon, April 18 at 11:59 pm.\nThe exam will consist of two parts:\n\nPart 1 - Conceptual: Multiple choice questions on Sakai test and quizzes.\n\nGo here to complete Part 1 of the exam.\nThis portion is comprised of 10 multiple choice / fill in the blank questions may only be submitted one time, so start it when you can set aside ~30 minutes to work on it. You will likely be done quicker but it’s best to set aside ample time.\n\nPart 2 - Applied: Data analysis in RStudio and submitted in Gradescope, like a usual lab and homework.\n\nGo to the GitHub organization for the course and find the exam-3- repo to complete Part 2 of your exam.\nAdd your answers to the exam-3.qmd file in your repo.\nYou can work on this portion at your own pace and come back to it however many times you like until the deadline. I recommend setting aside ~2 hours. You will likely be done quicker, but it’s best to set aside ample time.\n\n\nBoth portions must be completed and submitted by Mon, April 18 at 11:59 pm. Late work will only be accepted in the case of of extenuating circumstances and notification from your academic dean.\n🍀 Good luck! 🍀"
  },
  {
    "objectID": "exams/exam-3.html#academic-integrity",
    "href": "exams/exam-3.html#academic-integrity",
    "title": "Exam 2",
    "section": "",
    "text": "By taking this exam, you pledge to uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised."
  },
  {
    "objectID": "exams/exam-3.html#rules-notes",
    "href": "exams/exam-3.html#rules-notes",
    "title": "Exam 2",
    "section": "",
    "text": "This is an individual assignment. Everything in your repository and on Sakai is for your eyes only.\nYou may not collaborate or communicate anything about this exam to anyone except the instructor and the head TA (Rick Presman). For example, you may not communicate with other students, other TAs, or post/solicit help on the internet, email or via any other method of communication.\nThe exam is open-book, open-note, so you may use any materials from class as you take the exam.\nOnly head TA and professor office hours will be held during the exam period.\nYou may not email the TAs questions about the exam.\nIf you have questions, direct message the professor on Slack or email or request an appointment to meet on Zoom."
  },
  {
    "objectID": "exams/exam-3.html#submission",
    "href": "exams/exam-3.html#submission",
    "title": "Exam 2",
    "section": "",
    "text": "Part 1 - Conceptual: The answers to the multiple choice/short answer questions should be submitted under Test & Quizzes in Sakai.\nPart 2 - Applied: The PDF document for the data analysis portion must be uploaded to Gradescope.\n\nYou must submit a PDF to Gradescope that corresponds to the .qmd file on your GitHub repository in order to receive credit for this portion of the exam.\nYou must upload a PDF, not HTML. Any non-PDF submissions will not be graded.\nMark the pages associated with each question. If any answer for a question spans multiple pages, mark all associated pages.\nMake sure to associate the “Workflow & formatting” section with the first page.\nFailure to mark the pages in Gradescope will result in point deductions. Any pages that are not marked will not be graded.\nMake sure that your uploaded PDF document matches your .qmd and the PDF in your GitHub repository exactly. Points will be deducted for documents that are not reproducible."
  },
  {
    "objectID": "project-description.html",
    "href": "project-description.html",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#timeline",
    "href": "project-description.html#timeline",
    "title": "Project description",
    "section": "",
    "text": "Topic ideas due Fri, Feb 18\nProposal due Fri, Mar 18\nDraft report due Fri, Apr 8\nPeer review due Fri, Apr 15\nFinal report due Mon, Apr 25\nVideo presentation + slides and final GitHub repo due Thu, Apr 28\nPresentation comments due Sat, Apr 30"
  },
  {
    "objectID": "project-description.html#introduction",
    "href": "project-description.html#introduction",
    "title": "Project description",
    "section": "Introduction",
    "text": "Introduction\nTL;DR: Pick a data set and do a regression analysis. That is your final project.\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group’s interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio, and all components of the project must be reproducible (with the exception of the presentation).\n\nLogistics\nYou will work on the project with your lab groups.\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis\nA GitHub repository corresponding to your report\nSlides + a video presentation\nFormal peer review on another team’s project"
  },
  {
    "objectID": "project-description.html#topic-ideas",
    "href": "project-description.html#topic-ideas",
    "title": "Project description",
    "section": "Topic ideas",
    "text": "Topic ideas\nIdentify 2-3 data sets you’re interested in potentially using for the final project. If you’re unsure where to find data, you can use the list of potential data sources in the Tips + Resources section as a starting point. It may also help to think of topics you’re interested in investigating and find data sets on those topics.\nThe purpose of submitting project ideas is to give you time to find data for the project and to make sure you have a data set that can help you be successful in the project. Therefore, you must use one of the data sets submitted as a topic idea, unless otherwise notified by the teaching team.\nThe data sets should meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns\nAt least 6 of the columns must be useful and unique predictor variables.\n\nIdentifier variables such as “name”, “social security number”, etc. are not useful predictor variables.\nIf you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nObservations should reasonably meet the independence condition. Therefore, avoid data with repeated measures, data collected over time, etc.\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\nPlease ask a member of the teaching team if you’re unsure whether your data set meets the criteria.\nFor each data set, include the following:\n\nIntroduction and data\n\nState the source of the data set.\nDescribe when and how it was originally collected (by the original data curator, not necessarily how you found the data)\nDescribe the observations and the general characteristics being measured in the data\n\n\n\nResearch question\n\nDescribe a research question you’re interested in answering using this data.\n\n\n\nGlimpse of data\n\nUse the glimpse function to provide an overview of each data set\n\nSubmit the PDF of the topic ideas to Gradescope. Mark all pages associated with each data set."
  },
  {
    "objectID": "project-description.html#project-proposal",
    "href": "project-description.html#project-proposal",
    "title": "Project description",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is to help you think about your analysis strategy early.\nInclude the following in the proposal:\n\nSection 1 - Introduction\nThe introduction section includes\n\nan introduction to the subject matter you’re investigating\nthe motivation for your research question (citing any relevant literature)\nthe general research question you wish to explore\nyour hypotheses regarding the research question of interest.\n\n\n\nSection 2 - Data description\nIn this section, you will describe the data set you wish to explore. This includes\n\ndescription of the observations in the data set,\ndescription of how the data was originally collected (not how you found the data but how the original curator of the data collected it).\n\n\n\nSection 3 - Analysis approach\nIn this section, you will provide a brief overview of your analysis approach. This includes:\n\nDescription of the response variable.\nVisualization and summary statistics for the response variable.\nList of variables that will be considered as predictors\nRegression model technique (multiple linear regression and logistic regression)\n\n\n\nData dictionary (aka code book)\nSubmit a data dictionary for all the variables in your data set in the README of your project repo, in the data folder. Link to this file from your proposal writeup.\n\n\nSubmission\nPush all of your final changes to the GitHub repo, and submit the PDF of your proposal to Gradescope.\n\n\nProposal grading\n\n\n\nTotal\n10 pts\n\n\n\n\nIntroduction\n3 pts\n\n\nData description\n2 pts\n\n\nAnalysis plan\n4 pts\n\n\nData dictionary\n1 pts\n\n\n\nEach component will be graded as follows:\n\nMeets expectations (full credit): All required elements are completed and are accurate. The narrative is written clearly, all tables and visualizations are nicely formatted, and the work would be presentable in a professional setting.\nClose to expectations (half credit): There are some elements missing and/or inaccurate. There are some issues with formatting.\nDoes not meet expectations (no credit): Major elements missing. Work is not neatly formatted and would not be presentable in a professional setting."
  },
  {
    "objectID": "project-description.html#draft-report",
    "href": "project-description.html#draft-report",
    "title": "Project description",
    "section": "Draft report",
    "text": "Draft report\nThe purpose of the draft and peer review is to give you an opportunity to get early feedback on your analysis. Therefore, the draft and peer review will focus primarily on the exploratory data analysis, modeling, and initial interpretations.\nWrite the draft in the written-report.qmd file in your project repo. You do not need to submit the draft on Gradescope.\nBelow is a brief description of the sections to focus on in the draft:\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, any variable transformations (if needed), and any other relevant considerations that were part of the model fitting process.\n\n\nResults\nIn this section, you will output the final model and include a brief discussion of the model assumptions, diagnostics, and any relevant model fit statistics.\nThis section also includes initial interpretations and conclusions drawn from the model."
  },
  {
    "objectID": "project-description.html#peer-review",
    "href": "project-description.html#peer-review",
    "title": "Project description",
    "section": "Peer review",
    "text": "Peer review\nCritically reviewing others’ work is a crucial part of the scientific process, and STA 210 is no exception. Each lab team will be assigned two other teams’s projects to review. Each team should push their draft to their GitHub repo by the due date. One lab in the following week will be dedicated to the peer review, and all reviews will be due by the end of that lab session.\nDuring the peer review process, you will be provided read-only access to your partner teams’ GitHub repos. Provide your review in the form of GitHub issues to your partner team’s GitHub repo using the issue template provided. The peer review will be graded on the extent to which it comprehensively and constructively addresses the components of the partner team’s report: the research context and motivation, exploratory data analysis, modeling, interpretations, and conclusions.\n\nPairings\n\nSection 1 - M 1:45PM - 3:00PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\nchaa_chaa_chaa\nyay_stats\nstat_over_flow\n\n\ndekk\nchaa_chaa_chaa\nyay_stats\n\n\neight\ndekk\nchaa_chaa_chaa\n\n\nhousecats\neight\ndekk\n\n\nkrafthouse\nhousecats\neight\n\n\nrrawr\nkrafthouse\nhousecats\n\n\nstat_over_flow\nrrawr\nkrafthouse\n\n\nyay_stats\nstat_over_flow\nrrawr\n\n\n\n\n\nSection 2 - M 3:30PM - 4:45PM\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\na_plus_plus_plus\nwe_r\ntina\n\n\npredictors\na_plus_plus_plus\nwe_r\n\n\nsixers\npredictors\na_plus_plus_plus\n\n\nsoy_nuggets\nsixers\npredictors\n\n\ntina\nsoy_nuggets\nsixers\n\n\nwe_r\ntina\nsoy_nuggets\n\n\n\n\n\nSection 3 - M 5:15PM - 6:30PM\n\n\n\n\n\n\n\n\nTeam being reviewed\nReviewer 1\nReviewer 2\n\n\n\n\ndown_to_earth_goats\nthe_three_musketeers\nteam_five\n\n\nginger_and_stats\ndown_to_earth_goats\nthe_three_musketeers\n\n\npineapple_wedge_and_diced_papaya\nginger_and_stats\ndown_to_earth_goats\n\n\nstatchelorettes\npineapple_wedge_and_diced_papaya\nginger_and_stats\n\n\nstatisix\nstatchelorettes\npineapple_wedge_and_diced_papaya\n\n\nstats_squad\nstatisix\nstatchelorettes\n\n\nteam_five\nstats_squad\nstatisix\n\n\nthe_three_musketeers\nteam_five\nstats_squad\n\n\n\n\n\n\nProcess and questions\nSpend ~30 mins to review each team’s project.\n\nFind your team name on the Reviewer 1 and Reviewer 2 columns.\nFor each of the columns, find the name of the team to review in the Team being reviewed column. You should already have access to this team’s repo.\nOpen the repo of the team you’re reviewing, read their project draft, and browser around the rest of their repo.\nThen, go to the Issues tab in that repo, click on New issue, and click on Get started for the Peer review issue. Fill out this issue, answering the following questions:\n\nPeer review by: [NAME OF TEAM DOING THE REVIEW]\nNames of team members that participated in this review: [FULL NAMES OF TEAM MEMBERS DOING THE REVIEW]\nDescribe the goal of the project.\nDescribe the data used or collected, if any. If the proposal does not include the use of a specific dataset, comment on whether the project would be strengthened by the inclusion of a dataset.\nDescribe the approaches, tools, and methods that will be used.\nIs there anything that is unclear from the proposal?\nProvide constructive feedback on how the team might be able to improve their project. Make sure your feedback includes at least one comment on the statistical modeling aspect of the project, but do feel free to comment on aspects beyond the modeling.\nWhat aspect of this project are you most interested in and would like to see highlighted in the presentation.\nProvide constructive feedback on any issues with file and/or code organization.\n(Optional) Any further comments or feedback?"
  },
  {
    "objectID": "project-description.html#written-report",
    "href": "project-description.html#written-report",
    "title": "Project description",
    "section": "Written report",
    "text": "Written report\nYour written report must be completed in the written-report.qmd file and must be reproducible. All team members should contribute to the GitHub repository, with regular meaningful commits.\nBefore you finalize your write up, make sure the printing of code chunks is off with the option echo = FALSE.\nYou will submit the PDF of your final report on Gradescope.\nThe PDF you submit must match the files in your GitHub repository exactly. The mandatory components of the report are below. You are free to add additional sections as necessary. The report, including visualizations, should be no more than 10 pages long. is no minimum page requirement; however, you should comprehensively address all of the analysis and report.\nBe selective in what you include in your final write-up. The goal is to write a cohesive narrative that demonstrates a thorough and comprehensive analysis rather than explain every step of the analysis.\nYou are welcome to include an appendix with additional work at the end of the written report document; however, grading will largely be based on the content in the main body of the report. You should assume the reader will not see the material in the appendix unless prompted to view it in the main body of the report. The appendix should be neatly formatted and easy for the reader to navigate. It is not included in the 10-page limit.\nThe written report is worth 40 points, broken down as follows\n\n\n\nTotal\n40 pts\n\n\n\n\nIntroduction/data\n6 pts\n\n\nMethodology\n10 pts\n\n\nResults\n14 pts\n\n\nDiscussion + conclusion\n6 pts\n\n\nOrganization + formatting\n4 pts\n\n\n\nClick here for a PDF of the written report rubric.\n\nIntroduction and data\nThis section includes an introduction to the project motivation, data, and research question. Describe the data and definitions of key variables. It should also include some exploratory data analysis. All of the EDA won’t fit in the paper, so focus on the EDA for the response variable and a few other interesting variables and relationships.\n\nGrading criteria\nThe research question and motivation are clearly stated in the introduction, including citations for the data source and any external research. The data are clearly described, including a description about how the data were originally collected and a concise definition of the variables relevant to understanding the report. The data cleaning process is clearly described, including any decisions made in the process (e.g., creating new variables, removing observations, etc.) The explanatory data analysis helps the reader better understand the observations in the data along with interesting and relevant relationships between the variables. It incorporates appropriate visualizations and summary statistics.\n\n\n\nMethodology\nThis section includes a brief description of your modeling process. Explain the reasoning for the type of model you’re fitting, predictor variables considered for the model including any interactions. Additionally, show how you arrived at the final model by describing the model selection process, interactions considered, variable transformations (if needed), assessment of conditions and diagnostics, and any other relevant considerations that were part of the model fitting process.\n\nGrading criteria\nThe analysis steps are appropriate for the data and research question. The group used a thorough and careful approach to select the final model; the approach is clearly described in the report. The model selection process took into account potential interaction effects and addressed any violations in model conditions. The model conditions and diagnostics are thoroughly and accurately assessed for their model. If violations of model conditions are still present, there was a reasonable attempt to address the violations based on the course content.\n\n\n\nResults\nThis is where you will output the final model with any relevant model fit statistics.\nDescribe the key results from the model. The goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research questions, using the interpretations to support your conclusions. Focus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nGrading criteria\nThe model fit is clearly assessed, and interesting findings from the model are clearly described. Interpretations of model coefficients are used to support the key findings and conclusions, rather than merely listing the interpretation of every model coefficient. If the primary modeling objective is prediction, the model’s predictive power is thoroughly assessed.\n\n\n\nDiscussion + Conclusion\nIn this section you’ll include a summary of what you have learned about your research question along with statistical arguments supporting your conclusions. In addition, discuss the limitations of your analysis and provide suggestions on ways the analysis could be improved. Any potential issues pertaining to the reliability and validity of your data and appropriateness of the statistical analysis should also be discussed here. Lastly, this section will include ideas for future work.\n\nGrading criteria\nOverall conclusions from analysis are clearly described, and the model results are put into the larger context of the subject matter and original research question. There is thoughtful consideration of potential limitations of the data and/or analysis, and ideas for future work are clearly described.\n\n\n\nOrganization + formatting\nThis is an assessment of the overall presentation and formatting of the written report.\n\nGrading criteria\nThe report neatly written and organized with clear section headers and appropriately sized figures with informative labels. Numerical results are displayed with a reasonable number of digits, and all visualizations are neatly formatted. All citations and links are properly formatted. If there is an appendix, it is reasonably organized and easy for the reader to find relevant information. All code, warnings, and messages are suppressed. The main body of the written report (not including the appendix) is no longer than 10 pages."
  },
  {
    "objectID": "project-description.html#video-presentation-slides",
    "href": "project-description.html#video-presentation-slides",
    "title": "Project description",
    "section": "Video presentation + slides",
    "text": "Video presentation + slides\n\nSlides\nIn addition to the written report, your team will also create presentation slides and record a video presentation that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and discuss the primary conclusions. These slides should serve as a brief visual addition to your written report and will be graded for content and quality.\nFor submission, convert these slides to a .pdf document, and submit the PDF of the slides on Gradescope.\nThe slide deck should have no more than 6 content slides + 1 title slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3: Highlights from EDA\nSlide 4: Final model\nSlide 5: Interesting findings from the model\nSlide 6: Conclusions + future work\n\n\n\nVideo presentation\nFor the video presentation, you can speak over your slide deck, similar to the lecture content videos. The video presentation must be no longer than 8 minutes. It is fine if the video is shorter than 8 minutes, but it cannot exceed 8 minutes. You may use can use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire, then embed the video in an new discussion post on Conversations.\n\nTo upload your video to Warpwire:\n\nClick the Warpwire tab in the course Sakai site.\nClick the “+” and select “Upload files”.\nLocate the video on your computer and click to upload.\nOnce you’ve uploaded the video to Warpwire, click to share the video and copy the video’s URL. You will need this when you post the video in the discussion forum.\n\n\n\nTo post the video to the discussion forum\n\nClick the Presentations tab in the course Sakai site.\nClick the Presentations topic.\nClick “Start a new conversation”.\nMake the title “Your Team Name: Project Title”. For example, “Teaching Team: Our Awesome Presentation”.\nClick the Warpwire icon (between the table and shopping cart icons).\nSelect your video, then click “Insert 1 item.” This will embed your video in the conversation.\nUnder the video, paste the URL to your video.\nYou’re done!"
  },
  {
    "objectID": "project-description.html#presentation-comments",
    "href": "project-description.html#presentation-comments",
    "title": "Project description",
    "section": "Presentation comments",
    "text": "Presentation comments\nEach student will be assigned 2 presentations to watch. Your viewing assignments will be posted later in the semester.\nWatch the group’s video, then click “Reply” to post a question for the group. You may not post a question that’s already been asked on the discussion thread. Additionally, the question should be (i) substantive (i.e. it shouldn’t be “Why did you use a bar plot instead of a pie chart”?), (ii) demonstrate your understanding of the content from the course, and (iii) relevant to that group’s specific presentation, i.e demonstrating that you’ve watched the presentation.\nThis portion of the project will be assessed individually.\n\nPairings\nFind your team name in the first column, watch videos from teams in the second column and leave comments.\n\n\n\n\n\n\n\n\nReviewer\nFirst video to review\nSecond video to review\n\n\n\n\nGinger and Stats\nEight\nWe R\n\n\nKrafthouse\nGinger and Stats\nEight\n\n\nSoy Nuggets\nKrafthouse\nGinger and Stats\n\n\nDown To Earth Goats\nSoy Nuggets\nKrafthouse\n\n\nA+++\nDown To Earth Goats\nSoy Nuggets\n\n\nTeam Five\nA+++\nDown To Earth Goats\n\n\nRrawr\nTeam Five\nA+++\n\n\nHousecats\nRrawr\nTeam Five\n\n\nDekk\nHousecats\nRrawr\n\n\nStat OverFlow\nDekk\nHousecats\n\n\nThe Three Musketeers\nStat OverFlow\nDekk\n\n\nPredictors\nThe Three Musketeers\nStat OverFlow\n\n\nStats Squad\nPredictors\nThe Three Musketeers\n\n\nStatisix\nStats Squad\nPredictors\n\n\nSixers\nStatisix\nStats Squad\n\n\nYay Stats\nSixers\nStatisix\n\n\nTINA\nYay Stats\nSixers\n\n\nStatchelorettes\nTINA\nYay Stats\n\n\nPineapple Wedge and Diced Papaya\nStatchelorettes\nTINA\n\n\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\nStatchelorettes\n\n\nWe R\nChaa Chaa Chaa\nPineapple Wedge and Diced Papaya\n\n\nEight\nWe R\nChaa Chaa Chaa"
  },
  {
    "objectID": "project-description.html#reproducibility-organization",
    "href": "project-description.html#reproducibility-organization",
    "title": "Project description",
    "section": "Reproducibility + organization",
    "text": "Reproducibility + organization\nAll written work (with exception of presentation slides) should be reproducible, and the GitHub repo should be neatly organized.\nThe GitHub repo should have the following structure:\n\nREADME: Short project description and data dictionary\nwritten-report.qmd & written-report.pdf: Final written report\n/data: Folder that contains the data set for the final project.\n/previous-work: Folder that contains the topic-ideas and project-proposal files.\n/presentation: Folder with the presentation slides.\n\nIf your presentation slides are online, you can put a link to the slides in a README.md file in the presentation folder.\n\n\nPoints for reproducibility + organization will be based on the reproducibility of the written report and the organization of the project GitHub repo. The repo should be neatly organized as described above, there should be no extraneous files, all text in the README should be easily readable."
  },
  {
    "objectID": "project-description.html#peer-teamwork-evaluation",
    "href": "project-description.html#peer-teamwork-evaluation",
    "title": "Project description",
    "section": "Peer teamwork evaluation",
    "text": "Peer teamwork evaluation\nYou will be asked to fill out a survey where you rate the contribution and teamwork of each team member by assigning a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation. If you are suggesting that an individual did less than half the expected contribution given your team size (e.g., for a team of four students, if a student contributed less than 12.5% of the total effort), please provide some explanation. If any individual gets an average peer score indicating that this was the case, their grade will be assessed accordingly.\nIf you have concerns with the teamwork and/or contribution from any team members, please email me by the project video deadline. You only need to email me if you have concerns. Otherwise, I will assume everyone on the team equally contributed and will receive full credit for the teamwork portion of the grade."
  },
  {
    "objectID": "project-description.html#overall-grading",
    "href": "project-description.html#overall-grading",
    "title": "Project description",
    "section": "Overall grading",
    "text": "Overall grading\nThe grade breakdown is as follows:\n\n\n\nTotal\n100 pts\n\n\n\n\nTopic ideas\n5 pts\n\n\nProject proposal\n10 pts\n\n\nPeer review\n10 pts\n\n\nWritten report\n40 pts\n\n\nSlides + video presentation\n20 pts\n\n\nReproducibility + organization\n5 pts\n\n\nVideo comments\n5 pts\n\n\nPeer teamwork evaluation\n5 pts\n\n\n\nNote: No late project reports or videos are accepted.\n\nGrading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.\n\n\n\nLate work policy\nThere is no late work accepted on this project. Be sure to turn in your work early to avoid any technological mishaps."
  },
  {
    "objectID": "slides/intro.html#etf21215912-data-analysis-in-business",
    "href": "slides/intro.html#etf21215912-data-analysis-in-business",
    "title": "Welcome",
    "section": "ETF2121/5912 Data Analysis in Business",
    "text": "ETF2121/5912 Data Analysis in Business"
  },
  {
    "objectID": "slides/intro.html#what-is-this-course-about",
    "href": "slides/intro.html#what-is-this-course-about",
    "title": "Welcome",
    "section": "What is this course about?",
    "text": "What is this course about?\n\nProblem-based learning\nBusiness analytics\nPreparing you with tools for data analysis regardless of your major\nData wrangling, visualization, and analysis\nThis unit is designed for the non-technical student with no coding background!"
  },
  {
    "objectID": "slides/intro.html#example-case-problem",
    "href": "slides/intro.html#example-case-problem",
    "title": "Welcome",
    "section": "Example: Case Problem",
    "text": "Example: Case Problem\nAlumni donations are an important source of revenue for colleges and universities. If administrators could determine the factors that could lead to increases in the percentage of alumni who make a donation, they might be able to implement policies that could lead to increased revenues. Research shows that students who are more satisfied with their contact with teachers are more likely to graduate. As a result, one might suspect that smaller class sizes and lower student/faculty ratios might lead to a higher percentage of satisfied graduates, which in turn might lead to increases in the percentage of alumni who make a donation.\nProblem to solve: Predict the alumni giving rate, given the graduation rate. Discuss your findings."
  },
  {
    "objectID": "slides/intro.html#what-you-will-learn",
    "href": "slides/intro.html#what-you-will-learn",
    "title": "Welcome",
    "section": "What you will learn?",
    "text": "What you will learn?\n\nHow to use R programming language for data wrangling, visualization, and analysis;\nHow to use SQL language to query databases;\nHow to use POWER BI as data dashboard to showcase your analysis;\nHow to apply analytics skill to solve the business problem."
  },
  {
    "objectID": "slides/intro.html#at-the-end-of-this-course-you-are-able-to",
    "href": "slides/intro.html#at-the-end-of-this-course-you-are-able-to",
    "title": "Welcome",
    "section": "At the end of this course, you are able to",
    "text": "At the end of this course, you are able to\n\n\n\nsource: https://community.fabric.microsoft.com/t5/Data-Stories-Gallery/Healthcare-Expenditure/td-p/3437485"
  },
  {
    "objectID": "slides/intro.html#see-you",
    "href": "slides/intro.html#see-you",
    "title": "Welcome",
    "section": "See you 🎡",
    "text": "See you 🎡\n\n\n\n\n\n\n\n\n\n\nCourse Website"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "📖 Read the syllabus",
    "crumbs": [
      "Weekly Schedule",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#pre-class-activities-own-time",
    "href": "weeks/week-1.html#pre-class-activities-own-time",
    "title": "Week 1",
    "section": "",
    "text": "📖 Read the syllabus",
    "crumbs": [
      "Weekly Schedule",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#lecture-real-time",
    "href": "weeks/week-1.html#lecture-real-time",
    "title": "Week 1",
    "section": "Lecture (real-time)",
    "text": "Lecture (real-time)\nNote: The page will only display properly and clickable if you sign in using your monash account in google chrome. It is not tested on safari.\n🖥️ Introduction to Data Analysis\n\n\n📋 Note - Introduction to SQL",
    "crumbs": [
      "Weekly Schedule",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#tutorial-real-time",
    "href": "weeks/week-1.html#tutorial-real-time",
    "title": "Week 1",
    "section": "Tutorial (real-time)",
    "text": "Tutorial (real-time)\n📋 Tutorial 1 - Software Installation",
    "crumbs": [
      "Weekly Schedule",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#supplementary-own-time",
    "href": "weeks/week-1.html#supplementary-own-time",
    "title": "Week 1",
    "section": "Supplementary (own-time)",
    "text": "Supplementary (own-time)\n⌨️ Read Camm Chapter 1.1, 1.2, 1.3 and 1.5\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly Schedule",
      "Week 1"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Joan Tan (she/her) works as Senior Teaching Fellow at the Department of Business Statistics and Econometrics at Monash University and also currently working in a bank as an Associate Director. Joan’s work focuses on bringing industry experience into teaching with student centered innovative learning.\nEmail me\n☎️ Consultation hours: 12-1pm Tuesday, H4.76 / Zoom",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#instructor",
    "href": "course-team.html#instructor",
    "title": "Teaching team",
    "section": "",
    "text": "Dr. Joan Tan (she/her) works as Senior Teaching Fellow at the Department of Business Statistics and Econometrics at Monash University and also currently working in a bank as an Associate Director. Joan’s work focuses on bringing industry experience into teaching with student centered innovative learning.\nEmail me\n☎️ Consultation hours: 12-1pm Tuesday, H4.76 / Zoom",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-team.html#teaching-associates",
    "href": "course-team.html#teaching-associates",
    "title": "Teaching team",
    "section": "Teaching Associates",
    "text": "Teaching Associates\n\n\n\nContact\nTutorial Slots (Location)\nConsultation Hours\n\n\n\n\nMr. Elvis Yang (Head Tutor)\n\n📧 elvis.yang@monash.edu\nWednesday 9.30am (T6), N122\nWednesday: 4 - 4.45pm Zoom\n\n\nMr. Chin Quek\n\n📧 chin.quek@monash.edu\nTuesday 5pm (T1), C302A\nTuesday 6.30pm (T4), C302A\nThursday: 6.15-7pm Zoom\nFriday: 6.15-7pm Zoom\n\n\nMr. Krisanat Anukarnsakulchularp\n\n📧 Krisanat.Anukarnsakulchularp@monash.edu\nTuesday 9.30am (T2), N122\nTuesday 11am (T3), N122\nWednesday 3.30pm (T7), N122\nWednesday: 1.30 - 3pm. H4.62 / Zoom\nThursday: 3.15 - 4pm Zoom",
    "crumbs": [
      "Course information",
      "Teaching team"
    ]
  },
  {
    "objectID": "notes/w1.html#introduction-to-power-bi",
    "href": "notes/w1.html#introduction-to-power-bi",
    "title": "Lecture Note 1",
    "section": "Introduction to Power BI",
    "text": "Introduction to Power BI\nWelcome to Power BI!\nPower BI is a powerful business analytics tool that enables you to visualize data, share insights, and make data-driven decisions. This introductory document will guide you through the basics of Power BI and help you get started with creating your first report.\n\nWhat is Power BI?\nPower BI is a suite of business analytics tools by Microsoft that includes software, services, and connectors. It transforms your unrelated sources of data into coherent, visually immersive, and interactive insights. Your data may be an Excel spreadsheet, or a collection of cloud-based and on-premises hybrid data warehouses.\n\n\nGetting Started\n\n1. Download and Install Power BI Desktop\n\nGo to the Power BI website and download the Power BI Desktop.\nFollow the installation instructions to install Power BI Desktop on your computer.\n\n\n\n2. Open Power BI Desktop\nOnce installed, open Power BI Desktop. You will see the following interface:\n\n\n\n3. Connect to Data\nTo start working with data, you need to connect Power BI to your data source. Here’s how:\n\nClick on the “Home” tab.\nClick on the “Get Data” button.\n\n\n\nSelect your data source from the list (e.g., Excel, SQL Server, Web, etc.).\nFollow the prompts to load your data into Power BI.\n\n\n\n4. Transform Data with Power Query Editor\nAfter loading your data, you might need to clean and transform it. Use Power Query Editor for this purpose:\n\nClick on the “Home” tab.\nClick on the “Transform Data” button.\n\n\nThe Power Query Editor will open, where you can clean and prepare your data.\n\n\n\n5. Create Visualizations\nOnce your data is ready, you can start creating visualizations:\n\nGo back to the main Power BI Desktop window.\nIn the “Visualizations” pane, select the type of visualization you want to create (e.g., bar chart, line chart, pie chart, etc.).\nDrag and drop fields from the “Fields” pane to the “Values”, “Axis”, “Legend”, etc., in the “Visualizations” pane.\n\n\n\n\n6. Build Reports\nCombine multiple visualizations into a report:\n\nUse the “Report” view in Power BI Desktop.\nArrange your visualizations on the canvas.\nAdd titles, labels, and other elements to enhance your report.\n\n\n\n7. Save and Publish Your Report\n\nClick on the “File” tab.\nSelect “Save As” to save your report locally.\nTo share your report, click on the “Publish” button.\n\n\nFollow the prompts to publish your report to the Power BI service, where you can share it with others.\n\n\n\nConclusion\nCongratulations! You have successfully created your first Power BI report. Power BI is a versatile tool with many advanced features, so continue exploring and experimenting to unlock its full potential. Happy data analyzing!"
  },
  {
    "objectID": "notes/w1.html#section-1-introduction-to-power-bi",
    "href": "notes/w1.html#section-1-introduction-to-power-bi",
    "title": "Lecture Note 1",
    "section": "Section 1: Introduction to Power BI",
    "text": "Section 1: Introduction to Power BI\nWelcome to Power BI!\nPower BI is a powerful business analytics tool that enables you to visualize data, share insights, and make data-driven decisions. This introductory document will guide you through the basics of Power BI and help you get started with creating your first report.\n\nWhat is Power BI?\nPower BI is a suite of business analytics tools by Microsoft that includes software, services, and connectors. It transforms your unrelated sources of data into coherent, visually immersive, and interactive insights. Your data may be an Excel spreadsheet, or a collection of cloud-based and on-premises hybrid data warehouses.\n\n\nGetting Started\n\n1. Download and Install Power BI Desktop\n\nGo to the Power BI website and download the Power BI Desktop.\nFollow the installation instructions to install Power BI Desktop on your computer.\n\n\n\n2. Open Power BI Desktop\nOnce installed, open Power BI Desktop. You will see the following interface:\n\n\n\n3. Connect to Data\nTo start working with data, you need to connect Power BI to your data source. Here’s how:\n\nClick on the “Home” tab.\nClick on the “Get Data” button.\n\n\n\nSelect your data source from the list (e.g., Excel, SQL Server, Web, etc.).\nFollow the prompts to load your data into Power BI.\n\n\n\n4. Transform Data with Power Query Editor\nAfter loading your data, you might need to clean and transform it. Use Power Query Editor for this purpose:\n\nClick on the “Home” tab.\nClick on the “Transform Data” button.\n\n\nThe Power Query Editor will open, where you can clean and prepare your data.\n\n\n\n5. Create Visualizations\nOnce your data is ready, you can start creating visualizations:\n\nGo back to the main Power BI Desktop window.\nIn the “Visualizations” pane, select the type of visualization you want to create (e.g., bar chart, line chart, pie chart, etc.).\nDrag and drop fields from the “Fields” pane to the “Values”, “Axis”, “Legend”, etc., in the “Visualizations” pane.\n\n\n\n\n6. Build Reports\nCombine multiple visualizations into a report:\n\nUse the “Report” view in Power BI Desktop.\nArrange your visualizations on the canvas.\nAdd titles, labels, and other elements to enhance your report.\n\n\n\n7. Save and Publish Your Report\n\nClick on the “File” tab.\nSelect “Save As” to save your report locally.\nTo share your report, click on the “Publish” button.\n\n\nFollow the prompts to publish your report to the Power BI service, where you can share it with others.\n\n\n\nConclusion\nCongratulations! You have successfully created your first Power BI report. Power BI is a versatile tool with many advanced features, so continue exploring and experimenting to unlock its full potential. Happy data analyzing!"
  },
  {
    "objectID": "notes/w1.html#section-2-introduction-to-r-language",
    "href": "notes/w1.html#section-2-introduction-to-r-language",
    "title": "Lecture Note 1",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "supplementary/supp1.html",
    "href": "supplementary/supp1.html",
    "title": "Lecture Note 1",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:"
  },
  {
    "objectID": "supplementary/supp1.html#section-1-introduction-to-power-bi",
    "href": "supplementary/supp1.html#section-1-introduction-to-power-bi",
    "title": "Lecture Note 1",
    "section": "Section 1: Introduction to Power BI",
    "text": "Section 1: Introduction to Power BI\nWelcome to Power BI!\nPower BI is a powerful business analytics tool that enables you to visualize data, share insights, and make data-driven decisions. This introductory document will guide you through the basics of Power BI and help you get started with creating your first report.\n\nWhat is Power BI?\nPower BI is a suite of business analytics tools by Microsoft that includes software, services, and connectors. It transforms your unrelated sources of data into coherent, visually immersive, and interactive insights. Your data may be an Excel spreadsheet, or a collection of cloud-based and on-premises hybrid data warehouses.\n\n\nGetting Started\n\n1. Download and Install Power BI Desktop\n\nGo to the Power BI website and download the Power BI Desktop.\nFollow the installation instructions to install Power BI Desktop on your computer.\n\n\n\n2. Open Power BI Desktop\nOnce installed, open Power BI Desktop. You will see the following interface:\n\n\n\n3. Connect to Data\nTo start working with data, you need to connect Power BI to your data source. Here’s how:\n\nClick on the “Home” tab.\nClick on the “Get Data” button.\n\n\n\nSelect your data source from the list (e.g., Excel, SQL Server, Web, etc.).\nFollow the prompts to load your data into Power BI.\n\n\n\n4. Transform Data with Power Query Editor\nAfter loading your data, you might need to clean and transform it. Use Power Query Editor for this purpose:\n\nClick on the “Home” tab.\nClick on the “Transform Data” button.\n\n\nThe Power Query Editor will open, where you can clean and prepare your data.\n\n\n\n5. Create Visualizations\nOnce your data is ready, you can start creating visualizations:\n\nGo back to the main Power BI Desktop window.\nIn the “Visualizations” pane, select the type of visualization you want to create (e.g., bar chart, line chart, pie chart, etc.).\nDrag and drop fields from the “Fields” pane to the “Values”, “Axis”, “Legend”, etc., in the “Visualizations” pane.\n\n\n\n\n6. Build Reports\nCombine multiple visualizations into a report:\n\nUse the “Report” view in Power BI Desktop.\nArrange your visualizations on the canvas.\nAdd titles, labels, and other elements to enhance your report.\n\n\n\n7. Save and Publish Your Report\n\nClick on the “File” tab.\nSelect “Save As” to save your report locally.\nTo share your report, click on the “Publish” button.\n\n\nFollow the prompts to publish your report to the Power BI service, where you can share it with others.\n\n\n\nConclusion\nCongratulations! You have successfully created your first Power BI report. Power BI is a versatile tool with many advanced features, so continue exploring and experimenting to unlock its full potential. Happy data analyzing!"
  },
  {
    "objectID": "supplementary/supp1.html#section-2-introduction-to-r-language",
    "href": "supplementary/supp1.html#section-2-introduction-to-r-language",
    "title": "Lecture Note 1",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.\n\n\nBasic of R\n\nR as a Calculator\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nR Objects\nIn R, everything is an object. These objects serve as containers for various types of data. Whether you’re dealing with a single number, a character string (like a word), or a complex structure like the output of a plot or a statistical analysis summary, it’s all represented as an object.\nCreating Objects:\nTo create an object, you simply give it a name. For instance\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this example, an object is created called x and it is assigned the value 5. The &lt;- is the assignment operator. It assigns the value on the right to the object on the left. You can also use = to assign values to objects, but it’s considered bad practice.\nViewing Objects:\nTo view the value of an object, you simply type the name of the object and press enter. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR keeps track of all objects in the current workspace during the session. You can see all the objects in the current workspace by typing ls() in the console.\nOpearations with Objects:\nYou can perform operations with objects. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nObjects names:\nObject names can contain letters, numbers, periods, and underscores. However, they can only starts with letters or underscore and nothing else. They are case-sensitive, so x and X are different objects. They cannot start with a number or a period. If you would like to insist to have numbers or period as the first character, you can use backticks to define the object name. It is called nonsyntactic names. For instance, you can define the following:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExercises:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCreate an object called a and assign the value 10 to it.\nCreate an object called b and assign the value 20 to it.\n\nCreate an object called star and assign the value a + b to it.\nView the value of star.\nWhich of the following is not the syntactic name for an object?\n\nx\nX\n1x\nx1\nx.y\nx_y\n\n\n\n\nData Types\nR has several data types. The most common data types are:\n\nNumeric (double)\n\n\nrepresents eal numbers (e.g., 3.14, 0.0001, 1000.0).\ncan be positive or negative.\ncan be in scientific notation (e.g., 1.23e-5).\nused for continuous data like measurements, weights, heights, etc.\n\n\nCharacter\n\n\nrepresents text data (e.g., “hello”, “world”, “R is fun”).\nmust be enclosed in quotes.\nused for categorical data (e.g., “High School”, “Primary School”, “University”).\n\n\nLogical (boolean)\n\n\nrepresents binary data (e.g., TRUE or FALSE).\nused for logical operations.\n\n\nInteger\n\n\nrepresents whole numbers (e.g., 1, 2, 3, 1000).\ncan be positive or negative.\nused for counting data like number of students, number of cars, etc.\nsometimes you will see it ends with L (e.g., 1L, 2L, 3L, 1000L). This is to indicate that the number is an integer.\n\n\nFactors\n\n\nrepresents categorical data (e.g., “High School”, “Primary School”, “University”).\nused for categorical data.\n\n\nComplex\n\n\nrepresents complex numbers (e.g., 1 + 2i, 3 + 4i, 5 + 6i).\nused for complex data like electrical engineering, physics, etc.\n\nHowever, we seldom deal with complex data types. We will focus on the first four data types.\nExercises:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat is the data type of 3.14?\nWhat is the data type of \"hello\"?\nWhat is the data type of TRUE?\nWhat is the data type of 1L?\nWhat is the data type of factor(\"High School\")?\nWhat is the data type of 1 + 2i?\nWhat is the data type of \"1\"?\n\nNote: You can check your answers using typeof() function.\n\n\nFunctions\nA function is a block of code that performs a specific task. R has a large number of in-built functions and also allows users to define their own functions. We will learn more about how to create functions in the coming weeks. But so far, we will use some of the in-built functions. Anything that starts with ( and end with ) is a function.\nExercises:\nDefine which one is a function below:\n\nmean()\nmedian\nsd()\nvar()\nsum[]\n\n\n\nVectors\n\n\nData Frames\n\n\nPackages\nR packages are collections of functions and data sets developed by the community. They increase the power of R by improving existing base R functionalities, or by adding new ones.\nExercises:\n\nInstall tidyverse package. Explain what do you observe from the console.\nLoad the tidyverse package. Explain what do you observe from the console.\nInstall palmerpenguins package. Explain what do you observe from the console.\nLoad the palmerpenguins package. Explain what do you observe from the console.\nLoad the lmer package. Explain what do you observe from the console."
  },
  {
    "objectID": "supplementary/supp1.html#introduction-to-r-language",
    "href": "supplementary/supp1.html#introduction-to-r-language",
    "title": "Lecture Note 1",
    "section": "Introduction to R Language",
    "text": "Introduction to R Language\n\nBasic of R\n\nR as a Calculator\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nR Objects\nIn R, everything is an object. These objects serve as containers for various types of data. Whether you’re dealing with a single number, a character string (like a word), or a complex structure like the output of a plot or a statistical analysis summary, it’s all represented as an object.\nCreating Objects:\nTo create an object, you simply give it a name. For instance\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this example, an object is created called x and it is assigned the value 5. The &lt;- is the assignment operator. It assigns the value on the right to the object on the left. You can also use = to assign values to objects, but it’s considered bad practice.\nViewing Objects:\nTo view the value of an object, you simply type the name of the object and press enter. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR keeps track of all objects in the current workspace during the session. You can see all the objects in the current workspace by typing ls() in the console.\nOpearations with Objects:\nYou can perform operations with objects. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nObjects names:\nObject names can contain letters, numbers, periods, and underscores. However, they can only starts with letters or underscore and nothing else. They are case-sensitive, so x and X are different objects. They cannot start with a number or a period. If you would like to insist to have numbers or period as the first character, you can use backticks to define the object name. It is called nonsyntactic names. For instance, you can define the following:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExercises:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCreate an object called a and assign the value 10 to it.\nCreate an object called b and assign the value 20 to it.\n\nCreate an object called star and assign the value a + b to it.\nView the value of star.\nWhich of the following is not the syntactic name for an object?\n\nx\nX\n1x\nx1\nx.y\nx_y\n\n\n\n\nData Types\nR has several data types. The most common data types are:\n\nNumeric (double)\n\n\nrepresents eal numbers (e.g., 3.14, 0.0001, 1000.0).\ncan be positive or negative.\ncan be in scientific notation (e.g., 1.23e-5).\nused for continuous data like measurements, weights, heights, etc.\n\n\nCharacter\n\n\nrepresents text data (e.g., “hello”, “world”, “R is fun”).\nmust be enclosed in quotes.\nused for categorical data (e.g., “High School”, “Primary School”, “University”).\n\n\nLogical (boolean)\n\n\nrepresents binary data (e.g., TRUE or FALSE).\nused for logical operations.\n\n\nInteger\n\n\nrepresents whole numbers (e.g., 1, 2, 3, 1000).\ncan be positive or negative.\nused for counting data like number of students, number of cars, etc.\nsometimes you will see it ends with L (e.g., 1L, 2L, 3L, 1000L). This is to indicate that the number is an integer.\n\n\nFactors\n\n\nrepresents categorical data (e.g., “High School”, “Primary School”, “University”).\nused for categorical data.\n\n\nComplex\n\n\nrepresents complex numbers (e.g., 1 + 2i, 3 + 4i, 5 + 6i).\nused for complex data like electrical engineering, physics, etc.\n\nHowever, we seldom deal with complex data types. We will focus on the first four data types.\nExercises:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat is the data type of 3.14?\nWhat is the data type of \"hello\"?\nWhat is the data type of TRUE?\nWhat is the data type of 1L?\nWhat is the data type of factor(\"High School\")?\nWhat is the data type of 1 + 2i?\nWhat is the data type of \"1\"?\n\nNote: You can check your answers using typeof() function.\n\n\nFunctions\nA function is a block of code that performs a specific task. R has a large number of in-built functions and also allows users to define their own functions. We will learn more about how to create functions in the coming weeks. But so far, we will use some of the in-built functions. Anything that starts with ( and end with ) is a function.\nExercises:\nDefine which one is a function below:\n\nmean()\nmedian\nsd()\nvar()\nsum[]\n\n\n\nVectors\n\n\nData Frames\n\n\nPackages\nR packages are collections of functions and data sets developed by the community. They increase the power of R by improving existing base R functionalities, or by adding new ones.\nExercises:\n\nInstall tidyverse package. Explain what do you observe from the console.\nLoad the tidyverse package. Explain what do you observe from the console.\nInstall palmerpenguins package. Explain what do you observe from the console.\nLoad the palmerpenguins package. Explain what do you observe from the console.\nLoad the lmer package. Explain what do you observe from the console."
  },
  {
    "objectID": "slides/lec-1.html#chin-quek",
    "href": "slides/lec-1.html#chin-quek",
    "title": "Welcome to ETF2121/5912",
    "section": "Chin Quek",
    "text": "Chin Quek\n\n\n\n\n\nMr. Chin Quek (he/him)\n\n\n\n🎓 Master of Public Health, Monash University, Australia\n🎓 Bachelor in Mechanical Engineering, Commerce and Finance.\nCurrently working as a Manager at Deloitte\n💌 chin.quek@monash.edu\n\n\n - 🎓 Phd candidate in BusEco Monash University, Australia\n\n🎓 Bachelor of Commerce\n💌 zhixiang.yang@monash.edu\n\n\n - 🎓 Master of Business Analytics, Monash University, Australia - 🎓 Bachelor in xxx, Commerce and Finance - Working Experience: Tutor - 💌 Krisanat.Anukarnsakulchularp@monash.edu"
  },
  {
    "objectID": "slides/lec-1.html#elvis-yang",
    "href": "slides/lec-1.html#elvis-yang",
    "title": "Welcome to ETF2121/5912",
    "section": "Elvis Yang",
    "text": "Elvis Yang\n\n\n\n🎓 xxxMonash University, Australia\n🎓 xx\nWorking Experience: Senior analyst, Business analyst\n💌 zhixiang.yang@monash.edu"
  },
  {
    "objectID": "slides/lec-1.html#krisanat",
    "href": "slides/lec-1.html#krisanat",
    "title": "Welcome to ETF2121/5912",
    "section": "Krisanat",
    "text": "Krisanat\n\n\n\n\n\nMr. Krisanat (he/him)\n\n\n\n\n🎓 Master of Business Analytics, Monash University, Australia\n🎓 Bachelor in xxx, Commerce and Finance\nWorking Experience: Tutor\n💌 Krisanat.Anukarnsakulchularp@monash.edu"
  },
  {
    "objectID": "slides/lec-1.html#what-is-business-analytics",
    "href": "slides/lec-1.html#what-is-business-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "What is Business Analytics?",
    "text": "What is Business Analytics?\nBusiness analytics is the scientific process of transforming data into insights to facilitate better decision-making. This approach leverages data-driven or fact-based methods, often considered more objective than other decision-making alternatives.\nThe tools of business analytics support decision-making by:\n\nGenerating insights from data\nEnhancing our forecasting abilities for more accurate planning\nQuantifying risks\nProviding better alternatives through analysis and optimization"
  },
  {
    "objectID": "slides/lec-1.html#types-of-business-analytics",
    "href": "slides/lec-1.html#types-of-business-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Types of Business Analytics",
    "text": "Types of Business Analytics\n\nDescriptive Analytics: What happened?\nPredictive Analytics: What will happen?\nPrescriptive Analytics: What should we do?"
  },
  {
    "objectID": "slides/lec-1.html#descriptive-analytics",
    "href": "slides/lec-1.html#descriptive-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Descriptive Analytics",
    "text": "Descriptive Analytics\n\nDescriptive analytics is the interpretation of historical data to better understand changes that have occurred in a business.\nIt is the most basic form of analytics and is used to understand what has happened in the past.\nDescriptive analytics is used to understand the current state of a business and to track key performance indicators (KPIs)."
  },
  {
    "objectID": "slides/lec-1.html#predictive-analytics",
    "href": "slides/lec-1.html#predictive-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Predictive Analytics",
    "text": "Predictive Analytics\n\nPredictive analytics is the use of data, statistical algorithms, and machine learning techniques to identify the likelihood of future outcomes based on historical data.\nIt is used to forecast future trends, behaviors, and events.\n\nPredictive analytics uses many techniques from data mining, statistics, modeling, machine learning, and artificial intelligence to analyze current data to make predictions about the future."
  },
  {
    "objectID": "slides/lec-1.html#prescriptive-analytics",
    "href": "slides/lec-1.html#prescriptive-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Prescriptive Analytics",
    "text": "Prescriptive Analytics\n\nPrescriptive analytics is the final stage of business analytics.\nIt involves using optimization and simulation algorithms to advise on possible outcomes and answer the question, “What should we do?”\nPrescriptive analytics is used to make decisions based on the insights generated by descriptive and predictive analytics."
  },
  {
    "objectID": "slides/lec-1.html#data-types",
    "href": "slides/lec-1.html#data-types",
    "title": "Welcome to ETF2121/5912",
    "section": "Data Types",
    "text": "Data Types\n\n\n\n\n\nflowchart LR\n  A[Data] --&gt; B(Qualitative)\n  A --&gt; C(Quantitative)\n  B --&gt; D[Nominal]\n  B --&gt; E[Ordinal]\n  C --&gt; F[Discrete]\n  C --&gt; G[Continous]"
  },
  {
    "objectID": "slides/lec-1.html#forms-of-data",
    "href": "slides/lec-1.html#forms-of-data",
    "title": "Welcome to ETF2121/5912",
    "section": "Forms of data",
    "text": "Forms of data\n\nTime series data: Data that is collected at regular intervals over time.\nCross-sectional data: Data that is collected at a single point in time.\nPanel data: Data that is collected over time from multiple entities.\n\n\n\n\ncourse website"
  },
  {
    "objectID": "slides/lec-1.html#forms-of-data-collection",
    "href": "slides/lec-1.html#forms-of-data-collection",
    "title": "Welcome to ETF2121/5912",
    "section": "Forms of data collection",
    "text": "Forms of data collection\n\nTime series data: Data that is collected at regular intervals over time.\nCross-sectional data: Data that is collected at a single point in time.\nPanel data: Data that is collected over time from multiple entities."
  },
  {
    "objectID": "slides/lec-1.html#what-is-power-bi",
    "href": "slides/lec-1.html#what-is-power-bi",
    "title": "Welcome to ETF2121/5912",
    "section": "What is Power BI?",
    "text": "What is Power BI?\n\nPower BI is a business analytics tool developed by Microsoft that provides interactive visualizations and business intelligence capabilities.\nIt is a cloud-based business analytics service that enables anyone to visualize and analyze data with greater speed, efficiency, and understanding.\nPower BI is a collection of software services, apps, and connectors that work together to turn your unrelated sources of data into coherent, visually immersive, and interactive insights."
  },
  {
    "objectID": "slides/lec-1.html#why-power-bi",
    "href": "slides/lec-1.html#why-power-bi",
    "title": "Welcome to ETF2121/5912",
    "section": "Why Power BI?",
    "text": "Why Power BI?\n\nEasy to use: Power BI is designed to be user-friendly and intuitive.\nInteractive: Power BI allows you to interact with your data in real-time.\nCustomizable: Power BI allows you to customize your reports and dashboards to suit your needs.\nCollaborative: Power BI allows you to share your reports and dashboards with others."
  },
  {
    "objectID": "slides/lec-1.html#power-bi-components",
    "href": "slides/lec-1.html#power-bi-components",
    "title": "Welcome to ETF2121/5912",
    "section": "Power BI Components",
    "text": "Power BI Components\n\nPower BI Desktop: A desktop application that allows you to create reports and dashboards.\nPower BI Service: A cloud-based service that allows you to publish, share, and collaborate on reports and dashboards.\nPower BI Mobile: A mobile app that allows you to view and interact with your reports and dashboards on the go."
  },
  {
    "objectID": "slides/lec-1.html#how-to-use",
    "href": "slides/lec-1.html#how-to-use",
    "title": "Welcome to ETF2121/5912",
    "section": "How to use?",
    "text": "How to use?\n\n\n\n\ncourse website"
  },
  {
    "objectID": "slides/lec-1.html#quiz",
    "href": "slides/lec-1.html#quiz",
    "title": "Welcome to ETF2121/5912",
    "section": "Quiz",
    "text": "Quiz\n\nName\nAge\nSchooling: Primary, Secondary, Tertiary\nHeight\nTemperature\nSales\nBonds grade"
  },
  {
    "objectID": "slides/lec-1.html#how-to-use-demo-time",
    "href": "slides/lec-1.html#how-to-use-demo-time",
    "title": "Welcome to ETF2121/5912",
    "section": "How to use? Demo Time",
    "text": "How to use? Demo Time\n\n\n\n\n\ncourse website"
  },
  {
    "objectID": "weeks/week-5.html",
    "href": "weeks/week-5.html",
    "title": "Week 5",
    "section": "",
    "text": "Important\n\n\n\n\nIndividual assignment 1 due in 2 weeks!",
    "crumbs": [
      "Weekly Schedule",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#prepare",
    "href": "weeks/week-5.html#prepare",
    "title": "Week 5",
    "section": "Prepare",
    "text": "Prepare\n📖 Read Business Analytics, Sec 5.4: Random Variables\n📖 Read Business Analytics, Sec 5.5: Discrete Probability Distributions\n📖 Read Business Analytics, Sec 5.6: Continuous Probability Distributions"
  },
  {
    "objectID": "weeks/week-5.html#participate",
    "href": "weeks/week-5.html#participate",
    "title": "Week 5",
    "section": "Participate",
    "text": "Participate\n🖥️ Lab 3 - Coffee ratings\n🖥️ Lecture 5 - Introduction to Modeling Uncertainty"
  },
  {
    "objectID": "weeks/week-5.html#practice",
    "href": "weeks/week-5.html#practice",
    "title": "Week 5",
    "section": "Practice",
    "text": "Practice\n📋 Application Exercise 4 - Exam 1 Review"
  },
  {
    "objectID": "weeks/week-5.html#perform",
    "href": "weeks/week-5.html#perform",
    "title": "Week 5",
    "section": "Perform",
    "text": "Perform\n⌨️ Lab 3 - Coffee ratings\n✅ Exam 1\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "slides/lec-5.html#announcements",
    "href": "slides/lec-5.html#announcements",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Announcements",
    "text": "Announcements\n\nGroup assignment will be posted tomorrow, due on Week 11.\nIndividual assignment 1 due next week!"
  },
  {
    "objectID": "slides/lec-5.html#computational-setup",
    "href": "slides/lec-5.html#computational-setup",
    "title": "SLR: Simulation based-inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for pretty tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/lec-5.html#terminology",
    "href": "slides/lec-5.html#terminology",
    "title": "SLR: Simulation based-inference",
    "section": "Terminology",
    "text": "Terminology\n\nOutcome: y\nPredictor: x\nObserved y, \\(y\\): truth\nPredicted y, \\(\\hat{y}\\): fitted, estimated\nResidual: difference between observed and predicted outcome for a given value of predictor"
  },
  {
    "objectID": "slides/lec-5.html#model-evaluation",
    "href": "slides/lec-5.html#model-evaluation",
    "title": "SLR: Simulation based-inference",
    "section": "Model evaluation",
    "text": "Model evaluation\n\nOne concern in evaluating models is how well they do for prediction\nWe’re generally interested in how well a model might do for predicting the outcome for a new observation, not for predicting the outcome for an observation we used to fit the model (and already know its observed value)\nEvaluating predictive performance: Split the data into testing and training sets, build models using only the training set, and evaluate their performance on the testing set, and repeat many times to see how your model holds up to “new” data\nQuantifying variability of of estimates: Bootstrap the data, fit a model, obtain coefficient estimates and/or measures of strength of fit, and repeat many times to see how your model holds up to “new” data\nToday we introduced these concepts, throughout the semester we’ll learn how to implement them (i.e., write the code) and how to interpret their results"
  },
  {
    "objectID": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-nc",
    "href": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-nc",
    "title": "SLR: Simulation based-inference",
    "section": "Uninsurance vs. HS graduation in NC",
    "text": "Uninsurance vs. HS graduation in NC"
  },
  {
    "objectID": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-ny",
    "href": "slides/lec-5.html#uninsurance-vs.-hs-graduation-in-ny",
    "title": "SLR: Simulation based-inference",
    "section": "Uninsurance vs. HS graduation in NY",
    "text": "Uninsurance vs. HS graduation in NY\n\n\nCode\ncounty_2019_ny &lt;- county_2019 %&gt;%\n  as_tibble() %&gt;%\n  filter(state == \"New York\") %&gt;%\n  select(name, hs_grad, uninsured)\n\nggplot(county_2019_ny,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"New York counties, 2015 - 2019\"\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"pink\")"
  },
  {
    "objectID": "slides/lec-5.html#data-splitting",
    "href": "slides/lec-5.html#data-splitting",
    "title": "SLR: Simulation based-inference",
    "section": "Data splitting",
    "text": "Data splitting"
  },
  {
    "objectID": "slides/lec-5.html#bootstrapping",
    "href": "slides/lec-5.html#bootstrapping",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrapping",
    "text": "Bootstrapping"
  },
  {
    "objectID": "slides/lec-5.html#comparing-ny-and-nc",
    "href": "slides/lec-5.html#comparing-ny-and-nc",
    "title": "SLR: Simulation based-inference",
    "section": "Comparing NY and NC",
    "text": "Comparing NY and NC\n\nWhy are the fits from the NY models more variable than those from the NC models?"
  },
  {
    "objectID": "slides/lec-5.html#data-sale-prices-of-houses-in-duke-forest",
    "href": "slides/lec-5.html#data-sale-prices-of-houses-in-duke-forest",
    "title": "SLR: Simulation based-inference",
    "section": "Data: Sale prices of houses in Duke Forest",
    "text": "Data: Sale prices of houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest"
  },
  {
    "objectID": "slides/lec-5.html#exploratory-analysis",
    "href": "slides/lec-5.html#exploratory-analysis",
    "title": "SLR: Simulation based-inference",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis"
  },
  {
    "objectID": "slides/lec-5.html#modeling",
    "href": "slides/lec-5.html#modeling",
    "title": "SLR: Simulation based-inference",
    "section": "Modeling",
    "text": "Modeling\n\ndf_fit &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) %&gt;%\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, on average, for $116,652.\nSlope: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159."
  },
  {
    "objectID": "slides/lec-5.html#sample-to-population",
    "href": "slides/lec-5.html#sample-to-population",
    "title": "SLR: Simulation based-inference",
    "section": "Sample to population",
    "text": "Sample to population\n\nFor each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159.\n\n\nThis estimate is valid for the single sample of 98 houses.\nBut what if we’re not interested quantifying the relationship between the size and price of a house in this single sample?\nWhat if we want to say something about the relationship between these variables for all houses in Duke Forest?"
  },
  {
    "objectID": "slides/lec-5.html#statistical-inference-1",
    "href": "slides/lec-5.html#statistical-inference-1",
    "title": "SLR: Simulation based-inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical inference allows provide methods and tools for us to use the single sample we have observed to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be random and representative of the population we’re interested in"
  },
  {
    "objectID": "slides/lec-5.html#inference-for-simple-linear-regression",
    "href": "slides/lec-5.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation based-inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the interval, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval",
    "href": "slides/lec-5.html#confidence-interval",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval-for-the-slope-1",
    "href": "slides/lec-5.html#confidence-interval-for-the-slope-1",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nA confidence interval will allow us to make a statement like “For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take X dollars.”\n\nShould X be $10? $100? $1000?\nIf we were to take another sample of 98 would we expect the slope calculated based on that sample to be exactly $159? Off by $10? $100? $1000?\nThe answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\nWe need a way to quantify the variability of the sample statistic"
  },
  {
    "objectID": "slides/lec-5.html#quantify-the-variability-of-the-slope",
    "href": "slides/lec-5.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation based-inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor estimation\n\nTwo approaches:\n\nVia simulation (what we’ll do today)\nVia mathematical models (what we’ll do in the next class)\n\nBootstrapping to quantify the variability of the slope for the purpose of estimation:\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-1",
    "href": "slides/lec-5.html#bootstrap-sample-1",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 1",
    "text": "Bootstrap sample 1"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-2",
    "href": "slides/lec-5.html#bootstrap-sample-2",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 2",
    "text": "Bootstrap sample 2"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-3",
    "href": "slides/lec-5.html#bootstrap-sample-3",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 3",
    "text": "Bootstrap sample 3"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-4",
    "href": "slides/lec-5.html#bootstrap-sample-4",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 4",
    "text": "Bootstrap sample 4"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-sample-5",
    "href": "slides/lec-5.html#bootstrap-sample-5",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap sample 5",
    "text": "Bootstrap sample 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nso on and so forth…"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-samples-1---5",
    "href": "slides/lec-5.html#bootstrap-samples-1---5",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap samples 1 - 5",
    "text": "Bootstrap samples 1 - 5"
  },
  {
    "objectID": "slides/lec-5.html#bootstrap-samples-1---100",
    "href": "slides/lec-5.html#bootstrap-samples-1---100",
    "title": "SLR: Simulation based-inference",
    "section": "Bootstrap samples 1 - 100",
    "text": "Bootstrap samples 1 - 100"
  },
  {
    "objectID": "slides/lec-5.html#slopes-of-bootstrap-samples",
    "href": "slides/lec-5.html#slopes-of-bootstrap-samples",
    "title": "SLR: Simulation based-inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take ___ dollars."
  },
  {
    "objectID": "slides/lec-5.html#slopes-of-bootstrap-samples-1",
    "href": "slides/lec-5.html#slopes-of-bootstrap-samples-1",
    "title": "SLR: Simulation based-inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, give or take ___ dollars."
  },
  {
    "objectID": "slides/lec-5.html#confidence-level",
    "href": "slides/lec-5.html#confidence-level",
    "title": "SLR: Simulation based-inference",
    "section": "Confidence level",
    "text": "Confidence level\n\nHow confident are you that the true slope is between $0 and $250? How about $150 and $170? How about $90 and $210?"
  },
  {
    "objectID": "slides/lec-5.html#confidence-interval-1",
    "href": "slides/lec-5.html#confidence-interval-1",
    "title": "SLR: Simulation based-inference",
    "section": "95% confidence interval",
    "text": "95% confidence interval\n\n\nA 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\nWe are 95% confident that For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.43 to $205.77."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-i",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-i",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope I",
    "text": "Computing the CI for the slope I\nCalculate the observed slope:\n\nobserved_fit &lt;- duke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  fit()\n\nobserved_fit\n\n# A tibble: 2 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-ii",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-ii",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope II",
    "text": "Computing the CI for the slope II\nTake 100 bootstrap samples and fit models to each one:\n\n# #| code-line-numbers: \"1,5,6\"\n\nset.seed(1120)\n\nboot_fits &lt;- duke_forest %&gt;%\n  specify(price ~ area) %&gt;%\n  generate(reps = 100, type = \"bootstrap\") %&gt;%\n  fit()\n\nboot_fits\n\n# A tibble: 200 × 3\n# Groups:   replicate [100]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept   47819.\n 2         1 area          191.\n 3         2 intercept  144645.\n 4         2 area          134.\n 5         3 intercept  114008.\n 6         3 area          161.\n 7         4 intercept  100639.\n 8         4 area          166.\n 9         5 intercept  215264.\n10         5 area          125.\n# … with 190 more rows"
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-iii",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-iii",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope III",
    "text": "Computing the CI for the slope III\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\n# #| code-line-numbers: \"5\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/lec-5.html#computing-the-ci-for-the-slope-iv",
    "href": "slides/lec-5.html#computing-the-ci-for-the-slope-iv",
    "title": "SLR: Simulation based-inference",
    "section": "Computing the CI for the slope IV",
    "text": "Computing the CI for the slope IV\nStandard error method: Alternatively, compute the 95% CI as the point estimate \\(\\pm\\) ~2 standard deviations of the bootstrap distribution:\n\n# #| code-line-numbers: \"5\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"se\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          90.8     228.\n2 intercept -56788.   290093."
  },
  {
    "objectID": "slides/lec-5.html#precision-vs.-accuracy",
    "href": "slides/lec-5.html#precision-vs.-accuracy",
    "title": "SLR: Simulation based-inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?"
  },
  {
    "objectID": "slides/lec-5.html#precision-vs.-accuracy-1",
    "href": "slides/lec-5.html#precision-vs.-accuracy-1",
    "title": "SLR: Simulation based-inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nHow can we get best of both worlds – high precision and high accuracy?"
  },
  {
    "objectID": "slides/lec-5.html#changing-confidence-level",
    "href": "slides/lec-5.html#changing-confidence-level",
    "title": "SLR: Simulation based-inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n\n\n# #| code-line-numbers: \"|4\"\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/lec-5.html#changing-confidence-level-1",
    "href": "slides/lec-5.html#changing-confidence-level-1",
    "title": "SLR: Simulation based-inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          104.     212.\n2 intercept  -24380.  256730.\n\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          56.3     226.\n2 intercept -61950.   370395."
  },
  {
    "objectID": "slides/lec-5.html#recap",
    "href": "slides/lec-5.html#recap",
    "title": "SLR: Simulation based-inference",
    "section": "Recap",
    "text": "Recap\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\nWe report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\nSince we can’t continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability"
  },
  {
    "objectID": "slides/lec-5.html#sampling-is-natural",
    "href": "slides/lec-5.html#sampling-is-natural",
    "title": "SLR: Simulation based-inference",
    "section": "Sampling is natural",
    "text": "Sampling is natural\n\n\nWhen you taste a spoonful of soup and decide the spoonful you tasted isn’t salty enough, that’s exploratory analysis\nIf you generalize and conclude that your entire soup needs salt, that’s an inference\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)"
  },
  {
    "objectID": "slides/lec-5.html#statistical-significance",
    "href": "slides/lec-5.html#statistical-significance",
    "title": "SLR: Simulation based-inference",
    "section": "Statistical significance",
    "text": "Statistical significance\n\n\nDo the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?"
  },
  {
    "objectID": "slides/lec-5.html#hypotheses",
    "href": "slides/lec-5.html#hypotheses",
    "title": "SLR: Simulation based-inference",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nWe want to answer the question “Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”\nNull hypothesis - \\(H_0: \\beta_1 = 0\\), there is no linear relationship between area and price\nAlternative hypothesis - \\(H_A: \\beta_1 \\ne 0\\), there is a linear relationship between area and price"
  },
  {
    "objectID": "slides/lec-5.html#hypothesis-testing-as-a-court-trial",
    "href": "slides/lec-5.html#hypothesis-testing-as-a-court-trial",
    "title": "SLR: Simulation based-inference",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?”\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "slides/lec-5.html#hypothesis-testing-framework",
    "href": "slides/lec-5.html#hypothesis-testing-framework",
    "title": "SLR: Simulation based-inference",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e. what we’re testing for\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of observed or more extreme outcome given that the null hypothesis is true)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative\n\n\n\n\n\nsta210-s22.github.io/website"
  },
  {
    "objectID": "slides/lec-5.html#random-variables",
    "href": "slides/lec-5.html#random-variables",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Random Variables",
    "text": "Random Variables\n\n\n\n\n\n\nDefinition\n\n\nA random variable is a variable whose value is the outcome of a random phenomenon.\n\n\n\nTypes of Random Variables\n\nDiscrete random variables\nContinuous random variables\n\nImportance in Business\n\nunderstanding and predicting business outcomes\ndecision-making under uncertainty\nrisk management/assessment"
  },
  {
    "objectID": "slides/lec-5.html#discrete-random-variables",
    "href": "slides/lec-5.html#discrete-random-variables",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\n\n\n\n\n\nDefinition\n\n\nA discrete random variable is a random variable that can take on a finite or infinite sequence of values.\n\n\n\n\nCharacteristics: countable, finite or infinite\n\n\n\n\n\n\n\n\n\n\nRandom Experiment\nRandom Variable (x)\nPossible Values for the Random Variable\n\n\n\n\nFlip a coin\nOutcome of the coin flip\n1 if heads; 0 if tails\n\n\nRoll a die\nNumber of dots showing on the die\n1, 2, 3, 4, 5, 6\n\n\nContact five customers\nNumber of customers who place an order\n0, 1, 2, 3, 4, 5\n\n\nOperate a health care clinic for one day\nNumber of patients arriving\n0, 1, 2, 3, and so on\n\n\nOffer a customer the choice of two products\nChoice made by the customer\n0 if none; 1 if product A is chosen; 2 if product B is chosen\n\n\n\n\n\n\nFlip a coin: The random variable here represents the outcome of a single coin flip. It can either be heads (1) or tails (0).\nRoll a die: The random variable represents the number of dots showing on the top face of a die after a roll. Possible values range from 1 to 6.\nContact five customers: The random variable represents the number of customers out of five who place an order. Possible values are any whole number from 0 to 5.\nOperate a health care clinic for one day: The random variable represents the number of patients who arrive at the clinic in a single day. The number of patients can be any non-negative integer.\nOffer a customer the choice of two products: The random variable represents the choice made by the customer. If the customer chooses neither product, the value is 0; if they choose product A, the value is 1; if they choose product B, the value is 2."
  },
  {
    "objectID": "slides/lec-5.html#continuous-random-variables",
    "href": "slides/lec-5.html#continuous-random-variables",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\n\n\n\n\n\nDefinition\n\n\nA continuous random variable is a random variable that can take on any value in an interval.\n\n\n\n\nCharacteristics: uncountable, infinite\n\n\n\n\n\n\n\n\n\n\nScenario\nRandom Variable (x)\nPossible Values for the Random Variable\n\n\n\n\nCustomer visits a web page\nTime spent on the web page (minutes)\nx ≥ 0\n\n\nFill a soft drink can (max capacity = 12.1 ounces)\nAmount of drink in the can (ounces)\n0 ≤ x ≤ 12.1\n\n\nTest a new chemical process\nTemperature for the desired reaction (°F)\n150 ≤ x ≤ 212\n\n\nInvest $10,000 in the stock market\nValue of the investment after one year ($)\nx ≥ 0\n\n\n\n\n\n\nCustomer visits a web page: The random variable represents the time a customer spends on the web page, measured in minutes. This time can be any non-negative number (x ≥ 0).\nFill a soft drink can (max capacity = 12.1 ounces): The random variable represents the amount of drink in the can, measured in ounces. This amount can range from 0 to 12.1 ounces (0 ≤ x ≤ 12.1).\nTest a new chemical process: The random variable represents the temperature at which the desired reaction occurs, measured in degrees Fahrenheit. This temperature can range from 150°F to 212°F (150 ≤ x ≤ 212).\nInvest $10,000 in the stock market: The random variable represents the value of the investment after one year, measured in dollars. This value can be any non-negative number (x ≥ 0)."
  },
  {
    "objectID": "slides/lec-5.html#continuous-probability-distributions",
    "href": "slides/lec-5.html#continuous-probability-distributions",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Continuous Probability Distributions",
    "text": "Continuous Probability Distributions\nType of Continuous Probability Distributions: - Triangular Distribution - Uniform Distribution - Normal Distribution - Exponential Distribution"
  },
  {
    "objectID": "slides/lec-5.html#examples-of-continuous-probability-distributions",
    "href": "slides/lec-5.html#examples-of-continuous-probability-distributions",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Examples of Continuous Probability Distributions",
    "text": "Examples of Continuous Probability Distributions\nBusiness Examples: 1. Normal Distribution:\n- Scenario: Heights of employees in a large company.\n- Random Variable (x): Height.\n- Possible Values: Any real number within a reasonable range.\n\nExponential Distribution:\n\nScenario: Time between customer arrivals at a service desk.\nRandom Variable (x): Time between arrivals.\nPossible Values: x ≥ 0.\n\nUniform Distribution:\n\nScenario: Delivery time for a package within a specified window.\nRandom Variable (x): Delivery time.\nPossible Values: a ≤ x ≤ b (specified window).\n\nLog-Normal Distribution:\n\nScenario: Stock prices over time.\nRandom Variable (x): Stock price.\nPossible Values: x &gt; 0.\n\n\n\n\n\nWeek 5 - Uncertainty"
  },
  {
    "objectID": "slides/lec-5.html#business-examples",
    "href": "slides/lec-5.html#business-examples",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Business Examples",
    "text": "Business Examples\n\nNormal Distribution:\n\nScenario: Heights of employees in a large company.\n\nRandom Variable (x): Height.\n\nPossible Values: Any real number within a reasonable range.\n\nExponential Distribution:\n\nScenario: Time between customer arrivals at a service desk.\nRandom Variable (x): Time between arrivals.\nPossible Values: x ≥ 0."
  },
  {
    "objectID": "slides/lec-5.html#discrete-probability-distributions",
    "href": "slides/lec-5.html#discrete-probability-distributions",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Discrete Probability Distributions",
    "text": "Discrete Probability Distributions\nType of Discrete Probability Distributions: - Uniform Distribution - Binomial Distribution - Poisson Distribution"
  },
  {
    "objectID": "slides/lec-5.html#business-examples-discrete",
    "href": "slides/lec-5.html#business-examples-discrete",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Business Examples (Discrete)",
    "text": "Business Examples (Discrete)\n\nUniform Distribution\n\nScenario: Rolling a fair die.\nRandom Variable (x): Number of dots showing on the die.\nPossible Values: 1, 2, 3, 4, 5, 6.\n\nBinomial Distribution\n\nScenario: Number of successful sales calls out of a fixed number of attempts.\nRandom Variable (x): Number of successful calls.\nPossible Values: 0, 1, 2, …, n."
  },
  {
    "objectID": "slides/lec-5.html#business-examples-continuous",
    "href": "slides/lec-5.html#business-examples-continuous",
    "title": "An Introduction to Modeling Uncertainty",
    "section": "Business Examples (Continuous)",
    "text": "Business Examples (Continuous)\n\nTriangular Distribution\n\nScenario: Estimating the duration of a project.\nRandom Variable (x): Duration of the project.\nPossible Values: Minimum, most likely, maximum.\n\nNormal Distribution\n\nScenario: Distribution of human heights.\nRandom Variable (x): Height of a person.\nPossible Values: Any real number."
  },
  {
    "objectID": "weeks/week-5.html#pre-class-activities-own-time",
    "href": "weeks/week-5.html#pre-class-activities-own-time",
    "title": "Week 5",
    "section": "Pre-class activities (own-time)",
    "text": "Pre-class activities (own-time)\n📖 Read Business Analytics, 5th Edition: Chapter 2",
    "crumbs": [
      "Weekly Schedule",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#lecture-real-time",
    "href": "weeks/week-5.html#lecture-real-time",
    "title": "Week 5",
    "section": "Lecture (real-time)",
    "text": "Lecture (real-time)\n🖥️ Lecture 5 - Descriptive Statistics\n\n\n📋 Case study - Descriptive Statistics",
    "crumbs": [
      "Weekly Schedule",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#tutorial-real-time",
    "href": "weeks/week-5.html#tutorial-real-time",
    "title": "Week 5",
    "section": "Tutorial (real-time)",
    "text": "Tutorial (real-time)\n📋 Tutorial 5 - Power BI",
    "crumbs": [
      "Weekly Schedule",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-5.html#supplementary-own-time",
    "href": "weeks/week-5.html#supplementary-own-time",
    "title": "Week 5",
    "section": "Supplementary (own-time)",
    "text": "Supplementary (own-time)\n🔕 You do not have to complete these but if you wish you can. This book has a good discussion on descriptive statistics.\n⌨️ Supp 5 - Descriptive Statistics Using R\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly Schedule",
      "Week 5"
    ]
  },
  {
    "objectID": "slides/lec-1.html#accounting-analytics",
    "href": "slides/lec-1.html#accounting-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Accounting Analytics",
    "text": "Accounting Analytics\nDescriptive Analytics: This involves data visualization techniques to monitor financial performance metrics such as stock returns, trading volumes, and market volatility indicators.\nPredictive Analytics: Predictive models are employed to forecast financial performance, evaluate the risk associated with investment portfolios and projects, and develop financial instruments such as derivatives.\nPrescriptive Analytics: These models are used for optimizing investment portfolios, allocating assets efficiently, and creating optimal capital budgeting plans."
  },
  {
    "objectID": "slides/lec-1.html#financial-analytics",
    "href": "slides/lec-1.html#financial-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Financial Analytics",
    "text": "Financial Analytics"
  },
  {
    "objectID": "slides/lec-1.html#human-resource-analytics",
    "href": "slides/lec-1.html#human-resource-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Human Resource Analytics",
    "text": "Human Resource Analytics\nThe HR function is responsible for ensuring that the organization has\n- the necessary mix of skill sets to meet its needs,\n- hires the highest-quality talent,\n- provides an environment to retain it,\n- and achieves its organizational diversity goals.\nGoal: Effective HR practices ensure that the organization is equipped with skilled professionals who can drive the company’s success while fostering a supportive and inclusive workplace.\nGoogle uses “people analytics” to analyze data on its employees\n- to determine the characteristics of great leaders,\n- assess factors contributing to productivity,\n- and evaluate potential new hires.\nGoal: By leveraging data analytics, Google can identify and nurture leadership qualities, understand what drives employee performance, and make informed hiring decisions.\n."
  },
  {
    "objectID": "slides/lec-1.html#marketing-analytics",
    "href": "slides/lec-1.html#marketing-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Marketing Analytics",
    "text": "Marketing Analytics\nMarketing is one of the fastest-growing areas for the application of analytics. - The use of scanner data and data generated from social media has led to a better understanding of consumer behavior and increased interest in marketing analytics. - As a result, descriptive, predictive, and prescriptive analytics are all heavily used in marketing.\nPredictive models and optimization are used to better align advertising to specific target audiences making marketing efforts more effective and efficient.\nSentiment analysis allows companies to monitor better “the voice of the customer” and use the data to adjust their services and products."
  },
  {
    "objectID": "slides/lec-1.html#health-care-analytics",
    "href": "slides/lec-1.html#health-care-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Health Care Analytics",
    "text": "Health Care Analytics\nThe use of analytics in health care is increasing because of pressure to control costs and provide more effective treatment simultaneously. Descriptive, predictive, and prescriptive analytics are used to improve patient, staff, and facility scheduling, patient flow, purchasing, and inventory control. The use of prescriptive analytics for diagnosis and treatment is the most critical application of analytics in health care."
  },
  {
    "objectID": "slides/lec-1.html#supply-chain-analytics",
    "href": "slides/lec-1.html#supply-chain-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Supply Chain Analytics",
    "text": "Supply Chain Analytics\nAnalytics has long been used to achieve the efficient delivery of goods: the core service of logistic companies. To UPS and FedEx, the optimal sorting of goods, vehicle and staff scheduling, and vehicle routing are all key to profitability. Supply chain problems caused by the COVID-19 pandemic and world conflicts focused on using analytics to increase the resiliency of the supply chain. Descriptive analytics is used to monitor supply chain performance. Predictive analytics is used to quantify risk. Prescriptive analytics with scenario analysis is used to prepare supply chain solutions that can handle a high degree of disruption."
  },
  {
    "objectID": "slides/lec-1.html#analytics-for-government-and-nonprofits",
    "href": "slides/lec-1.html#analytics-for-government-and-nonprofits",
    "title": "Welcome to ETF2121/5912",
    "section": "Analytics for Government and Nonprofits",
    "text": "Analytics for Government and Nonprofits\nGovernment agencies use analytics to increase the effectiveness and accountability of programs. The U.S. Internal Revenue Service uses data mining to identify patterns that distinguish questionable annual personal income tax filings. Likewise, nonprofit agencies use analytics to ensure their effectiveness and accountability to donors and clients. Descriptive and predictive analytics monitor agency performance, track donor behavior, and forecast donations. Data mining helps identify potential donors and minimize donor attrition. Optimization allocates scarce resources in capital budgeting."
  },
  {
    "objectID": "slides/lec-1.html#sports-analytics",
    "href": "slides/lec-1.html#sports-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Sports Analytics",
    "text": "Sports Analytics\nProfessional sports teams use analytics to assess players for the amateur drafts, decide how much to offer players in contract negotiations, and assist with on-field decisions. Sports franchises also use analytics for off-the-field business decisions. Based on fan survey data, a predictive technique known as conjoint analysis is used to design stadium premium seating. Prescriptive analytics is used to adjust ticket prices throughout the season dynamically."
  },
  {
    "objectID": "slides/lec-1.html#web-analytics",
    "href": "slides/lec-1.html#web-analytics",
    "title": "Welcome to ETF2121/5912",
    "section": "Web Analytics",
    "text": "Web Analytics\nThe analysis of online activity includes, but is not limited to, visits to websites and social media sites such as Facebook and LinkedIn. Leading companies apply descriptive and advanced analytics to data collected in online experiments to determine the best way to configure websites, position ads, and utilize social networks to promote products and services. Because of the massive pool of Internet users, experiments can be conducted without risking disrupting the company’s overall business."
  },
  {
    "objectID": "hw/hw-5.html",
    "href": "hw/hw-5.html",
    "title": "Tutorial 5 - Introduction to Modeling Uncertainty",
    "section": "",
    "text": "Case Problem: McNeil’s Auto Mall"
  },
  {
    "objectID": "notes/w1a.html",
    "href": "notes/w1a.html",
    "title": "Lecture Note 1a",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:"
  },
  {
    "objectID": "notes/w1a.html#section-1-introduction-to-sql",
    "href": "notes/w1a.html#section-1-introduction-to-sql",
    "title": "Lecture Note 1a",
    "section": "Section 1: Introduction to SQL",
    "text": "Section 1: Introduction to SQL\n\nWhat is SQL?\nSQL stands for Structured Query Language. It is a language used to communicate with databases. It is the standard language for relational database management systems. Why we need SQL? Let’s say you are suppose to design for a system that will store the information of students in a university. How would you store the information of students? You can store the information in a spreadsheet, but what if you have to store the information of thousands of students? It will be very difficult to manage the data in a spreadsheet. Also, where do you want to store the information? In a table? Will a table suffice? What if you have to store the information of the courses that the students are enrolled in? How will you manage the data? This is where databases come in.\nDatabases are used to store large amount of data. SQL is used to communicate with the database!\nThe database can be of any type like MySQL, PostgreSQL, SQLite, etc. By ignoring all those, let’s focus on the SQL language itself, which is used to communicate with the database. Once you master SQL, you can work with any database. The most popular database is MySQL. It is an open-source database. It is used by many companies like Facebook, Twitter, etc. Oracle is also a popular database. It is used by many big companies. SQL is used to communicate with the database. It is used to perform all types of operations on the database. It is used to create a database, create a table in the database, insert data into the table, update the data, delete the data, etc.\n\n\nSQL Syntax\nSQL syntax is the set of rules that defines how a SQL query should be written. It is the set of rules that all SQL queries should follow. SQL syntax is similar to the English language, which makes it easier to write, read, and understand. SQL syntax is divided into several categories. These categories are:\n\nData Definition Language (DDL)\nData Manipulation Language (DML)\nData Query Language (DQL)\nData Control Language (DCL)\nTransaction Control Language (TCL)\n\n\n\nData Definition Language (DDL)\nData Definition Language (DDL) is used to define the structure that holds the data. It is used to create tables, columns, etc. DDL commands are auto-committed. It means that the changes made by the DDL command are saved to the database automatically. The following are the DDL commands:\n\nCREATE\nALTER\nDROP\nTRUNCATE\nCOMMENT\n\n\n\nData Manipulation Language (DML)\nData Manipulation Language (DML) is used to manipulate the data itself. It is used to insert, update, delete, and retrieve data from the database. DML commands are not auto-committed. It means that the changes made by the DML command are not saved to the database automatically. You have to use the COMMIT command to save the changes. The following are the DML commands:\n\nSELECT\nINSERT\nUPDATE\nDELETE\n\n\n\nData Query Language (DQL)\nData Query Language (DQL) is used to retrieve the data from the database. The SELECT statement is the most commonly used DQL command. The following are the DQL commands:\n\nSELECT\nWHERE\nGROUP BY\nHAVING\nORDER BY\n\n\n\nData Control Language (DCL)\nData Control Language (DCL) is used to control the visibility of data. It is used to control the access of data. The following are the DCL commands:\n\nGRANT\nREVOKE\n\n\n\nTransaction Control Language (TCL)\nTransaction Control Language (TCL) is used to manage the transactions in the database. It is used to manage the changes made by the DML commands. The following are the TCL commands:\n\nCOMMIT\nROLLBACK\nSAVEPOINT"
  },
  {
    "objectID": "notes/w1b.html",
    "href": "notes/w1b.html",
    "title": "Lecture Note 1b",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:"
  },
  {
    "objectID": "notes/w1b.html#section-1-introduction-to-power-bi",
    "href": "notes/w1b.html#section-1-introduction-to-power-bi",
    "title": "Lecture Note 1b",
    "section": "Section 1: Introduction to Power BI",
    "text": "Section 1: Introduction to Power BI\nWelcome to Power BI!\nPower BI is a powerful business analytics tool that enables you to visualize data, share insights, and make data-driven decisions. This introductory document will guide you through the basics of Power BI and help you get started with creating your first report.\n\nWhat is Power BI?\nPower BI is a suite of business analytics tools by Microsoft that includes software, services, and connectors. It transforms your unrelated sources of data into coherent, visually immersive, and interactive insights. Your data may be an Excel spreadsheet, or a collection of cloud-based and on-premises hybrid data warehouses.\n\n\nGetting Started\n\n1. Download and Install Power BI Desktop\n\nGo to the Power BI website and download the Power BI Desktop.\nFollow the installation instructions to install Power BI Desktop on your computer.\n\n\n\n2. Open Power BI Desktop\nOnce installed, open Power BI Desktop. You will see the following interface:\n\n\n\n3. Connect to Data\nTo start working with data, you need to connect Power BI to your data source. Here’s how:\n\nClick on the “Home” tab.\nClick on the “Get Data” button.\n\n\n\nSelect your data source from the list (e.g., Excel, SQL Server, Web, etc.).\nFollow the prompts to load your data into Power BI.\n\n\n\n4. Transform Data with Power Query Editor\nAfter loading your data, you might need to clean and transform it. Use Power Query Editor for this purpose:\n\nClick on the “Home” tab.\nClick on the “Transform Data” button.\n\n\nThe Power Query Editor will open, where you can clean and prepare your data.\n\n\n\n5. Create Visualizations\nOnce your data is ready, you can start creating visualizations:\n\nGo back to the main Power BI Desktop window.\nIn the “Visualizations” pane, select the type of visualization you want to create (e.g., bar chart, line chart, pie chart, etc.).\nDrag and drop fields from the “Fields” pane to the “Values”, “Axis”, “Legend”, etc., in the “Visualizations” pane.\n\n\n\n\n6. Build Reports\nCombine multiple visualizations into a report:\n\nUse the “Report” view in Power BI Desktop.\nArrange your visualizations on the canvas.\nAdd titles, labels, and other elements to enhance your report.\n\n\n\n7. Save and Publish Your Report\n\nClick on the “File” tab.\nSelect “Save As” to save your report locally.\nTo share your report, click on the “Publish” button.\n\n\nFollow the prompts to publish your report to the Power BI service, where you can share it with others.\n\n\n\nConclusion\nCongratulations! You have successfully created your first Power BI report. Power BI is a versatile tool with many advanced features, so continue exploring and experimenting to unlock its full potential. Happy data analyzing!"
  },
  {
    "objectID": "notes/w1b.html#section-2-introduction-to-r-language",
    "href": "notes/w1b.html#section-2-introduction-to-r-language",
    "title": "Lecture Note 1b",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w1a.html#live-sql-oracle",
    "href": "notes/w1a.html#live-sql-oracle",
    "title": "Lecture Note 1a",
    "section": "Live SQL Oracle",
    "text": "Live SQL Oracle\nOracle Live SQL is a free, interactive, web-based tool that allows users to write, run, and share SQL scripts and PL/SQL code. It provides a convenient platform for learning and practicing SQL without needing to install Oracle Database software locally. Key benefits include:\n\nEase of Use: The web-based interface is user-friendly, making it accessible for both beginners and experienced users to write and execute SQL code.\nEducational Resources: Live SQL offers tutorials, sample scripts, and documentation to help users learn SQL and PL/SQL concepts and best practices.\nCollaboration: Users can share their scripts and solutions with others, fostering collaboration and knowledge sharing within the community.\nReal-Time Execution: Scripts run on an Oracle Database in real-time, providing immediate feedback and results.\nAccess Anywhere: As a web-based tool, Live SQL can be accessed from any device with an internet connection, making it convenient for users to practice and develop their SQL skills on the go.\n\nOracle Live SQL is an excellent resource for anyone looking to improve their SQL proficiency, offering hands-on experience with Oracle Database in a flexible and collaborative environment.\n\nHow to Sign up and Register for Oracle Live SQL?"
  },
  {
    "objectID": "notes/w1a.html#step-1-visit-the-oracle-live-sql-website",
    "href": "notes/w1a.html#step-1-visit-the-oracle-live-sql-website",
    "title": "Lecture Note 1a",
    "section": "Step 1: Visit the Oracle Live SQL Website",
    "text": "Step 1: Visit the Oracle Live SQL Website\n\nOpen your web browser and go to the Oracle Live SQL website: Oracle Live SQL."
  },
  {
    "objectID": "notes/w1a.html#step-2-create-an-oracle-account",
    "href": "notes/w1a.html#step-2-create-an-oracle-account",
    "title": "Lecture Note 1a",
    "section": "Step 2: Create an Oracle Account",
    "text": "Step 2: Create an Oracle Account\n\nOn the Oracle Live SQL homepage, click the Sign In button located at the top right corner of the page.\nIf you already have an Oracle account, you can simply enter your login credentials (email and password) and click Sign In. If you do not have an Oracle account, click the Create Account link below the Sign In button."
  },
  {
    "objectID": "notes/w1a.html#step-3-fill-out-the-registration-form",
    "href": "notes/w1a.html#step-3-fill-out-the-registration-form",
    "title": "Lecture Note 1a",
    "section": "Step 3: Fill Out the Registration Form",
    "text": "Step 3: Fill Out the Registration Form\n\nYou will be redirected to the Oracle account creation page. Fill out the registration form with the required information:\n\nEmail Address: Enter a valid email address that you have access to.\nPassword: Create a strong password that meets the specified criteria.\nCountry: Select your country of residence from the dropdown menu.\nName: Enter your first and last name.\nJob Title: (Optional) Enter your job title.\nWork Phone: (Optional) Enter your work phone number.\nCompany Name: (Optional) Enter your company name.\n\nReview the Oracle Terms of Use and Privacy Policy. If you agree to the terms, check the box indicating your acceptance.\nComplete the CAPTCHA to verify you are not a robot.\nClick the Create Account button to submit your registration."
  },
  {
    "objectID": "notes/w1a.html#step-4-verify-your-email-address",
    "href": "notes/w1a.html#step-4-verify-your-email-address",
    "title": "Lecture Note 1a",
    "section": "Step 4: Verify Your Email Address",
    "text": "Step 4: Verify Your Email Address\n\nAfter submitting your registration, Oracle will send a verification email to the email address you provided. Check your inbox for an email from Oracle with the subject line “Verify your email address.”\nOpen the email and click the verification link to confirm your email address. You will be redirected to the Oracle website, and your account will be activated."
  },
  {
    "objectID": "notes/w1a.html#step-5-sign-in-to-oracle-live-sql",
    "href": "notes/w1a.html#step-5-sign-in-to-oracle-live-sql",
    "title": "Lecture Note 1a",
    "section": "Step 5: Sign In to Oracle Live SQL",
    "text": "Step 5: Sign In to Oracle Live SQL\n\nReturn to the Oracle Live SQL website: Oracle Live SQL.\nClick the Sign In button at the top right corner of the page.\nEnter your Oracle account credentials (email and password) and click Sign In."
  },
  {
    "objectID": "notes/w1a.html#step-6-start-using-oracle-live-sql",
    "href": "notes/w1a.html#step-6-start-using-oracle-live-sql",
    "title": "Lecture Note 1a",
    "section": "Step 6: Start Using Oracle Live SQL",
    "text": "Step 6: Start Using Oracle Live SQL\n\nOnce signed in, you will be directed to the Oracle Live SQL main interface.\nYou can now start writing and running SQL and PL/SQL scripts. The interface provides a variety of features, including:\n\nScript Editor: Write and execute your SQL scripts.\nSample Scripts: Explore and run sample scripts provided by Oracle to learn SQL and PL/SQL concepts.\nTutorials: Access educational resources and tutorials to improve your skills.\nCommunity Sharing: Share your scripts with the community or explore scripts shared by other users."
  },
  {
    "objectID": "notes/w1a.html#additional-tips",
    "href": "notes/w1a.html#additional-tips",
    "title": "Lecture Note 1a",
    "section": "Additional Tips:",
    "text": "Additional Tips:\n\nSave Your Work: Remember to save your scripts periodically to avoid losing your work.\nExplore Documentation: Utilize the extensive Oracle documentation and resources available on the platform to deepen your understanding of SQL and PL/SQL.\nEngage with the Community: Participate in the Oracle Live SQL community by sharing your scripts, providing feedback, and learning from others.\n\nBy following these steps, you can successfully sign up and register for Oracle Live SQL, gaining access to a powerful tool for learning and practicing SQL in an Oracle Database environment. Happy coding!\nChatGPT can make mistakes. Check important info."
  },
  {
    "objectID": "notes/w1a.html#section-2-live-sql-oracle",
    "href": "notes/w1a.html#section-2-live-sql-oracle",
    "title": "Lecture Note 1a",
    "section": "Section 2: Live SQL Oracle",
    "text": "Section 2: Live SQL Oracle\nOracle Live SQL is a free, interactive, web-based tool that allows users to write, run, and share SQL scripts and PL/SQL code. It provides a convenient platform for learning and practicing SQL without needing to install Oracle Database software locally. Key benefits include:\n\nEase of Use: The web-based interface is user-friendly, making it accessible for both beginners and experienced users to write and execute SQL code.\nEducational Resources: Live SQL offers tutorials, sample scripts, and documentation to help users learn SQL and PL/SQL concepts and best practices.\nCollaboration: Users can share their scripts and solutions with others, fostering collaboration and knowledge sharing within the community.\nReal-Time Execution: Scripts run on an Oracle Database in real-time, providing immediate feedback and results.\nAccess Anywhere: As a web-based tool, Live SQL can be accessed from any device with an internet connection, making it convenient for users to practice and develop their SQL skills on the go.\n\nOracle Live SQL is an excellent resource for anyone looking to improve their SQL proficiency, offering hands-on experience with Oracle Database in a flexible and collaborative environment."
  },
  {
    "objectID": "labs/lab-0.html",
    "href": "labs/lab-0.html",
    "title": "Tutorial 1 - Meet + greet + Software Installation",
    "section": "",
    "text": "Section 1: Installing R and RStudio\n\n1.1 Installing R\nR is a powerful statistical programming language used for data analysis and visualization. Follow these steps to install R on your computer:\n\nDownload R:\n\nGo to the CRAN R Project website.\nClick on the link for your operating system (Windows, macOS, or Linux).\nFor Windows: Click on “Download R for Windows” and then click on “base”. Click on the download link to get the installer.\nFor macOS: Click on “Download R for (Mac) OS X”. Choose the appropriate file for your macOS version and download the installer.\nFor Linux: Follow the instructions provided for your specific distribution (Debian, Ubuntu, Fedora, etc.).\n\nInstall R:\n\nWindows: Run the downloaded .exe file and follow the installation prompts. Accept the default settings unless you have specific requirements.\nmacOS: Open the downloaded .pkg file and follow the installation prompts.\nLinux: Use the terminal and follow the instructions from the CRAN website to add the R repository and install R via your package manager.\n\n\n\n\n1.2 Installing RStudio\nRStudio is an integrated development environment (IDE) for R that makes coding in R easier and more efficient. Follow these steps to install RStudio:\n\nDownload RStudio:\n\nGo to the RStudio website.\nClick on the “Download” button under RStudio Desktop.\nChoose the installer for your operating system (Windows, macOS, or Linux).\n\nInstall RStudio:\n\nWindows: Run the downloaded .exe file and follow the installation prompts.\nmacOS: Open the downloaded .dmg file and drag the RStudio icon to your Applications folder.\nLinux: Open the terminal and follow the instructions provided on the RStudio website to install RStudio via your package manager or download and install the .deb or .rpm package.\n\nLaunch RStudio:\n\nAfter installation, open RStudio. It should automatically detect your R installation. If it doesn’t, you may need to manually configure the path to the R executable in the RStudio settings.\n\n\nBy completing these steps, you will have both R and RStudio installed on your computer, ready for use in data analysis and visualization tasks.\n\n\n\n\nSection 2: Getting Started with R and RStudio\nNow that you have installed R and RStudio, it’s time to get familiar with the environment and start coding.\n\n2.1 Exploring the RStudio Interface\nRStudio provides a user-friendly interface that integrates various tools for working with R. Here’s a quick overview of the main components:\n\nSource Pane:\n\nThis is where you write and edit your R scripts. You can open multiple tabs to work on different scripts simultaneously.\n\nConsole Pane:\n\nThe console pane is where you can directly enter R commands and see the output immediately.\n\nEnvironment/History Pane:\n\nThe Environment tab shows all the objects (data, variables, functions) currently in your workspace.\nThe History tab keeps a record of all the commands you’ve entered in the console.\n\nFiles/Plots/Packages/Help/Viewer Pane:\n\nThe Files tab lets you navigate your file system.\nThe Plots tab displays plots generated by your R code.\nThe Packages tab allows you to manage R packages.\nThe Help tab provides access to R documentation and help files.\nThe Viewer tab can display web content or documents.\n\n\n#todo: insert video here\n\n\n\nSection 3: Downloading and Installing Power BI Desktop\nPower BI Desktop is a powerful data visualization tool that allows you to connect to various data sources, transform data, and create interactive reports. Follow these steps to download and install Power BI Desktop on your computer.\n\n4.1 Downloading Power BI Desktop\n\nVisit the Power BI Website:\n\nGo to the Power BI website.\n\nNavigate to the Download Page:\n\nClick on the Products menu at the top of the page and select Power BI Desktop.\n\nDownload the Installer:\n\nClick on the Download free button to go to the Microsoft Store page for Power BI Desktop.\nIf you prefer to download the installer directly (instead of through the Microsoft Store), click on the See download or language options link and choose the version suitable for your operating system.\n\n\n\n\n4.2 Installing Power BI Desktop\n\nRun the Installer:\n\nIf you downloaded Power BI Desktop from the Microsoft Store, it will install automatically after you click the Get button.\nIf you downloaded the installer directly, locate the downloaded file (typically a .msi file) and double-click it to run the installer.\n\nFollow the Installation Prompts:\n\nClick Next on the welcome screen.\nAccept the license agreement and click Next.\nChoose the installation folder or accept the default path and click Next.\nClick Install to begin the installation process.\n\nComplete the Installation:\n\nOnce the installation is complete, click Finish to close the installer.\n\nLaunch Power BI Desktop:\n\nAfter installation, you can launch Power BI Desktop from the Start menu (Windows) or by searching for it in your applications.\n\n\n\n\n4.3 Initial Setup and Configuration\n\nSign In to Power BI:\n\nWhen you first open Power BI Desktop, you may be prompted to sign in. If you have a Power BI account, sign in with your credentials. If not, you can create a new account or skip this step.\n\nExplore the Interface:\n\nFamiliarize yourself with the Power BI Desktop interface, which includes the following key components:\n\nReport View: Where you create and arrange visualizations.\nData View: Where you can see your data tables.\nModel View: Where you can manage relationships between tables.\nFields Pane: Displays the tables and fields in your data model.\nVisualizations Pane: Contains various types of visualizations you can add to your reports.\n\n\nConnect to Data Sources:\n\nClick on Get Data in the Home ribbon to connect to a variety of data sources, such as Excel, SQL Server, web data, and more.\nFollow the prompts to load your data into Power BI Desktop.\n\n\nBy following these steps, you will have Power BI Desktop installed and ready for use. This tool will enable you to create insightful and interactive data visualizations for your analysis projects. Explore the interface, connect to your data, and start creating your reports to make the most of Power BI Desktop."
  },
  {
    "objectID": "labs/lab-1.html",
    "href": "labs/lab-1.html",
    "title": "Tutorial 1 - Meet + greet + Software Installation",
    "section": "",
    "text": "Warm up\nIntroduce yourself to your instructor and friends.\nYour tutor will show you how to do the following if you are reluctant to read all the text below. 😁\n\n\nSection 1: Sign up and Register for Oracle Live SQL?\n\nOpen your web browser and go to the Oracle Live SQL website: Oracle Live SQL.\nOn the Oracle Live SQL homepage, click the Sign In button located at the top right corner of the page.\nIf you already have an Oracle account, you can simply enter your login credentials (email and password) and click Sign In. If you do not have an Oracle account, click the Create Account link below the Sign In button.\nYou will be redirected to the Oracle account creation page. Fill out the registration form with the required information:\n\nEmail Address: Enter a valid email address that you have access to.\nPassword: Create a strong password that meets the specified criteria.\nCountry: Select your country of residence from the dropdown menu.\nName: Enter your first and last name.\nJob Title: (Optional) Enter your job title.\nWork Phone: (Optional) Enter your work phone number.\nCompany Name: (Optional) Enter your company name.\n\nReview the Oracle Terms of Use and Privacy Policy. If you agree to the terms, check the box indicating your acceptance.\nComplete the CAPTCHA to verify you are not a robot.\nClick the Create Account button to submit your registration.\nAfter submitting your registration, Oracle will send a verification email to the email address you provided. Check your inbox for an email from Oracle with the subject line “Verify your email address.”\nOpen the email and click the verification link to confirm your email address. You will be redirected to the Oracle website, and your account will be activated.\nReturn to the Oracle Live SQL website: Oracle Live SQL.\nClick the Sign In button at the top right corner of the page.\nEnter your Oracle account credentials (email and password) and click Sign In.\nOnce signed in, you will be directed to the Oracle Live SQL main interface.\nYou can now start writing and running SQL and PL/SQL scripts. The interface provides a variety of features, including:\n\nScript Editor: Write and execute your SQL scripts.\nSample Scripts: Explore and run sample scripts provided by Oracle to learn SQL and PL/SQL concepts.\nTutorials: Access educational resources and tutorials to improve your skills.\nCommunity Sharing: Share your scripts with the community or explore scripts shared by other users.\n\n\n\nExercises\nGo to oracle live and select Start Coding as in the screenshot below.\n\nThe database used in this exercise is HR database as in the schema below.\n\n\nWrite a Simple SQL Query:\n\nIn the Script Editor, write a simple SQL query to select all columns from a table. For example:\n\nSELECT * FROM table_name;\n\nClick the Run button to execute the query and view the results.\n\n\nPlay around with the select statement using HR schema. The tables in HR schema can be accessed via Schema &gt; Choose “Human Resources (HR)” from the dropdown list.\n\n\n\n\n\n\nSection 2: Downloading and Installing Power BI Desktop\nPower BI Desktop is a powerful data visualization tool that allows you to connect to various data sources, transform data, and create interactive reports. Follow these steps to download and install Power BI Desktop on your computer.\n\n2.1 Downloading Power BI Desktop\n\nVisit the Power BI Website:\n\nGo to the Power BI website.\n\nNavigate to the Download Page:\n\nClick on the Products menu at the top of the page and select Power BI Desktop.\n\nDownload the Installer:\n\nClick on the Download free button to go to the Microsoft Store page for Power BI Desktop.\nIf you prefer to download the installer directly (instead of through the Microsoft Store), click on the See download or language options link and choose the version suitable for your operating system.\n\n\n\n\n2.2 Installing Power BI Desktop\n\nRun the Installer:\n\nIf you downloaded Power BI Desktop from the Microsoft Store, it will install automatically after you click the Get button.\nIf you downloaded the installer directly, locate the downloaded file (typically a .msi file) and double-click it to run the installer.\n\nFollow the Installation Prompts:\n\nClick Next on the welcome screen.\nAccept the license agreement and click Next.\nChoose the installation folder or accept the default path and click Next.\nClick Install to begin the installation process.\n\nComplete the Installation:\n\nOnce the installation is complete, click Finish to close the installer.\n\nLaunch Power BI Desktop:\n\nAfter installation, you can launch Power BI Desktop from the Start menu (Windows) or by searching for it in your applications.\n\n\n\n\n2.3 Initial Setup and Configuration\n\nSign In to Power BI:\n\nWhen you first open Power BI Desktop, you may be prompted to sign in. If you have a Power BI account, sign in with your credentials. If not, you can create a new account or skip this step.\n\nExplore the Interface:\n\nFamiliarize yourself with the Power BI Desktop interface, which includes the following key components:\n\nReport View: Where you create and arrange visualizations.\nData View: Where you can see your data tables.\nModel View: Where you can manage relationships between tables.\nFields Pane: Displays the tables and fields in your data model.\nVisualizations Pane: Contains various types of visualizations you can add to your reports.\n\n\nConnect to Data Sources:\n\nClick on Get Data in the Home ribbon to connect to a variety of data sources, such as Excel, SQL Server, web data, and more.\nFollow the prompts to load your data into Power BI Desktop.\n\n\n\n\n\nAlternative: MOVE platform!\n\n\nSection 3: Installing R and RStudio\n\nInstalling R\nR is a powerful statistical programming language used for data analysis and visualization. Follow these steps to install R on your computer:\n\nDownload R:\n\nGo to the CRAN R Project website.\nClick on the link for your operating system (Windows, macOS, or Linux).\nFor Windows: Click on “Download R for Windows” and then click on “base”. Click on the download link to get the installer.\nFor macOS: Click on “Download R for (Mac) OS X”. Choose the appropriate file for your macOS version and download the installer.\nFor Linux: Follow the instructions provided for your specific distribution (Debian, Ubuntu, Fedora, etc.).\n\nInstall R:\n\nWindows: Run the downloaded .exe file and follow the installation prompts. Accept the default settings unless you have specific requirements.\nmacOS: Open the downloaded .pkg file and follow the installation prompts.\nLinux: Use the terminal and follow the instructions from the CRAN website to add the R repository and install R via your package manager.\n\n\n\n\nInstalling RStudio\nRStudio is an integrated development environment (IDE) for R that makes coding in R easier and more efficient. Follow these steps to install RStudio:\n\nDownload RStudio:\n\nGo to the RStudio website.\nClick on the “Download” button under RStudio Desktop.\nChoose the installer for your operating system (Windows, macOS, or Linux).\n\nInstall RStudio:\n\nWindows: Run the downloaded .exe file and follow the installation prompts.\nmacOS: Open the downloaded .dmg file and drag the RStudio icon to your Applications folder.\nLinux: Open the terminal and follow the instructions provided on the RStudio website to install RStudio via your package manager or download and install the .deb or .rpm package.\n\nLaunch RStudio:\n\nAfter installation, open RStudio. It should automatically detect your R installation. If it doesn’t, you may need to manually configure the path to the R executable in the RStudio settings.\n\n\nBy completing these steps, you will have both R and RStudio installed on your computer, ready for use in data analysis and visualization tasks.\n\n\n\n\nSection 4: Getting Started with R and RStudio\nNow that you have installed R and RStudio, it’s time to get familiar with the environment and start coding.\n\n4.1 Exploring the RStudio Interface\nRStudio provides a user-friendly interface that integrates various tools for working with R. Here’s a quick overview of the main components:\n\nSource Pane:\n\nThis is where you write and edit your R scripts. You can open multiple tabs to work on different scripts simultaneously.\n\nConsole Pane:\n\nThe console pane is where you can directly enter R commands and see the output immediately.\n\nEnvironment/History Pane:\n\nThe Environment tab shows all the objects (data, variables, functions) currently in your workspace.\nThe History tab keeps a record of all the commands you’ve entered in the console.\n\nFiles/Plots/Packages/Help/Viewer Pane:\n\nThe Files tab lets you navigate your file system.\nThe Plots tab displays plots generated by your R code.\nThe Packages tab allows you to manage R packages.\nThe Help tab provides access to R documentation and help files.\nThe Viewer tab can display web content or documents.\n\n\n\n\n4.2 Running R Code\nTo run R code in RStudio, you can either type commands directly into the console or write scripts in the source pane. Here’s how to run code in RStudio:\n\nRunning Commands in the Console:\n\nType an R command in the console and press Enter to execute it.\nThe output will be displayed below the command.\nFor example, you can type 2 + 3 in the console and press Enter to see the result.\n\nWriting Scripts:\n\nTo write longer scripts or save your code for later use, open a new script file by clicking on File &gt; New File &gt; R Script.\n\n\n\n\nExercises\n\nCalculate the following:\n\n5 + 9\n\\(20\\times 4\\)\n\\(9/2\\times 9\\)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRun the below in your R console and discuss what is the difference between these three\n\n5 * 3 - 7\n(5 * 3) - 7\n5 * (3 - 7)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nDiscuss the results, is it as what you expected?\nPhew….feel overwhelm?\n😓\nThe whole exercise is just to make sure you successfully installed all necessary software in your pc. Once you get it install, everything will become easier later on. Having trouble? Please consult your tutor asap."
  },
  {
    "objectID": "notes/w3.html",
    "href": "notes/w3.html",
    "title": "Lecture Note Week 3",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Understand basic R syntax\nLO2: Understand basic Power BI point and click function"
  },
  {
    "objectID": "notes/w3.html#part-1-introduction-to-power-bi-extended-lecture-notes",
    "href": "notes/w3.html#part-1-introduction-to-power-bi-extended-lecture-notes",
    "title": "Lecture Note 1b",
    "section": "Part 1: Introduction to Power BI (Extended Lecture Notes)",
    "text": "Part 1: Introduction to Power BI (Extended Lecture Notes)\n\n1. What is Power BI? (5 minutes)\nPower BI Overview: - Definition: - Power BI is a suite of business analytics tools by Microsoft that allows you to visualize your data and share insights across your organization or embed them in an app or website. - Key Features: - Data Visualization: - Create visually appealing reports and dashboards with a variety of charts, graphs, and maps. - Real-time Data Updates: - Connect to multiple data sources and refresh your data in real-time or on a schedule. - Interactive Reports: - Use interactive elements like slicers and filters to explore your data dynamically. - Cloud-based and On-premises Deployment: - Access Power BI reports and dashboards from the web or mobile devices. - Benefits: - User-friendly Interface: - Intuitive drag-and-drop functionality makes it easy for non-technical users. - Integration with Other Microsoft Services: - Seamlessly integrates with Excel, Azure, SQL Server, and other Microsoft products. - Scalable and Secure: - Suitable for small businesses to large enterprises with robust security features. - Real-world Applications: - Business Reporting: - Monthly sales reports, financial summaries, and performance dashboards. - Data Analysis: - Customer behavior analysis, market trend analysis, and operational efficiency. - Decision Making: - Data-driven decisions based on insights derived from comprehensive data analysis.\n\n\n\n2. Power BI Interface Overview (10 minutes)\nPower BI Desktop Components:\n\nReport View:\n\nPurpose:\n\nDesign and view your interactive reports.\n\nFeatures:\n\nAdd visuals, format reports, and create multiple report pages.\n\nNavigation:\n\nUse the left-hand pane to switch between Report, Data, and Model views.\n\n\nData View:\n\nPurpose:\n\nExplore and manage your data tables and columns.\n\nFeatures:\n\nView raw data, create new columns, and transform data.\n\nNavigation:\n\nAccess the Data view by clicking the table icon on the left-hand pane.\n\n\nModel View:\n\nPurpose:\n\nDefine and manage relationships between tables.\n\nFeatures:\n\nCreate and edit relationships, view data models, and manage table connections.\n\nNavigation:\n\nAccess the Model view by clicking the diagram icon on the left-hand pane.\n\n\n\nKey Interface Elements:\n\nRibbon:\n\nPurpose:\n\nProvides access to tools and options for report creation and data management.\n\nSections:\n\nHome, View, Modeling, Format, Data, and Help tabs.\n\n\nPanes:\n\nFields Pane:\n\nDisplays available tables and fields for use in your report.\n\nVisualizations Pane:\n\nContains a variety of visual elements (charts, tables, maps) to add to your report.\n\nFilters Pane:\n\nAllows you to apply filters to visuals, pages, or the entire report.\n\n\nCanvas:\n\nPurpose:\n\nDesign area where you create and arrange visuals for your report.\n\nFeatures:\n\nDrag and drop fields, format visuals, and customize the report layout.\n\n\n\n\n\n\n3. Connecting to Data Sources (15 minutes)\nData Connectors: - Overview: - Power BI can connect to a wide range of data sources including files, databases, and online services. - Common Data Sources: - Files: - Excel, CSV, XML, JSON. - Databases: - SQL Server, MySQL, Oracle, PostgreSQL. - Online Services: - Azure, SharePoint, Google Analytics, Salesforce.\nSteps to Import Data: 1. Get Data: - Navigation: - Click on the “Home” tab &gt; “Get Data” button. - Options: - Choose from a list of common data sources or click “More” to see all available connectors. 2. Choose Source: - Example: - Select “Excel” to import data from an Excel file. 3. Load Data: - Navigation: - Browse to the file location, select the file, and click “Open.” - Preview: - Preview the data before loading it into Power BI. - Load/Transform: - Click “Load” to import the data directly or “Transform Data” to open Power Query Editor for data cleaning and transformation.\nDemo: Importing a Sample Dataset: - Step-by-Step Walkthrough: 1. Click “Home” &gt; “Get Data” &gt; “Excel.” 2. Browse and select a sample Excel file. 3. Preview the data in the Navigator window. 4. Click “Load” to import the data into Power BI. 5. Explore the imported data in the Data view."
  },
  {
    "objectID": "slides/lec-1.html#teaching-team-1",
    "href": "slides/lec-1.html#teaching-team-1",
    "title": "Welcome to ETF2121/5912",
    "section": "Teaching Team",
    "text": "Teaching Team\n\n\n\n\n\n\n\nPicture\nEmail 💌\n\n\n\n\n\nDr. Joan Tan  joan.tan@monash.edu  Consultation: Tuesday 12-1pm\n\n\n\nMr. Chin Quek  chin.quek@monash.edu\n\n\n\nMr Elvis Yang  zhixiang.yang@monash.edu\n\n\n\nMr. Krisanat  Krisanat.Anukarnsakulchularp@monash.edu"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Make sure you can successfully login to Oracle Live SQL. If you are not, please refer to tutorial 1.",
    "crumbs": [
      "Weekly Schedule",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#pre-class-activities-own-time",
    "href": "weeks/week-2.html#pre-class-activities-own-time",
    "title": "Week 2",
    "section": "",
    "text": "Make sure you can successfully login to Oracle Live SQL. If you are not, please refer to tutorial 1.",
    "crumbs": [
      "Weekly Schedule",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#lecture-real-time",
    "href": "weeks/week-2.html#lecture-real-time",
    "title": "Week 2",
    "section": "Lecture (real-time)",
    "text": "Lecture (real-time)\n🖥️ Lecture 2 - SQL\n\n\n📋 Note - SQL Resources",
    "crumbs": [
      "Weekly Schedule",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#tutorial-real-time",
    "href": "weeks/week-2.html#tutorial-real-time",
    "title": "Week 2",
    "section": "Tutorial (real-time)",
    "text": "Tutorial (real-time)\n📋 Tutorial 2 - SQL\n📑 Tutorial 2 Solution",
    "crumbs": [
      "Weekly Schedule",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#supplementary-own-time",
    "href": "weeks/week-2.html#supplementary-own-time",
    "title": "Week 2",
    "section": "Supplementary (own-time)",
    "text": "Supplementary (own-time)\n🔕 Please take note that the below resources is just for you to advance the SQL topic and you are not required to complete it.\n\nW3school has a very intensive resources on learning SQL. Click here.\nOracle Live Online - Give “Introduction to SQL” a go if you are interested to learn more SQL.\n\n\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly Schedule",
      "Week 2"
    ]
  },
  {
    "objectID": "labs/lab-5.html",
    "href": "labs/lab-5.html",
    "title": "Lab 4 - The Office",
    "section": "",
    "text": "In today’s lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\nThis is a different data source than the one we’ve used in class last week.\n\n\nBy the end of the lab you will…\n\nengineer features based on episode scripts\ntrain a model\ninterpret model coefficients\nmake predictions\nevaluate model performance on training and testing data\n\n\n\n\n\n\nR is a powerful statistical programming language used for data analysis and visualization. Follow these steps to install R on your computer:\n\nDownload R:\n\nGo to the CRAN R Project website.\nClick on the link for your operating system (Windows, macOS, or Linux).\nFor Windows: Click on “Download R for Windows” and then click on “base”. Click on the download link to get the installer.\nFor macOS: Click on “Download R for (Mac) OS X”. Choose the appropriate file for your macOS version and download the installer.\nFor Linux: Follow the instructions provided for your specific distribution (Debian, Ubuntu, Fedora, etc.).\n\nInstall R:\n\nWindows: Run the downloaded .exe file and follow the installation prompts. Accept the default settings unless you have specific requirements.\nmacOS: Open the downloaded .pkg file and follow the installation prompts.\nLinux: Use the terminal and follow the instructions from the CRAN website to add the R repository and install R via your package manager.\n\n\n\n\n\nRStudio is an integrated development environment (IDE) for R that makes coding in R easier and more efficient. Follow these steps to install RStudio:\n\nDownload RStudio:\n\nGo to the RStudio website.\nClick on the “Download” button under RStudio Desktop.\nChoose the installer for your operating system (Windows, macOS, or Linux).\n\nInstall RStudio:\n\nWindows: Run the downloaded .exe file and follow the installation prompts.\nmacOS: Open the downloaded .dmg file and drag the RStudio icon to your Applications folder.\nLinux: Open the terminal and follow the instructions provided on the RStudio website to install RStudio via your package manager or download and install the .deb or .rpm package.\n\nLaunch RStudio:\n\nAfter installation, open RStudio. It should automatically detect your R installation. If it doesn’t, you may need to manually configure the path to the R executable in the RStudio settings.\n\n\nBy completing these steps, you will have both R and RStudio installed on your computer, ready for use in data analysis and visualization tasks.\n\n\n\n\n\nNow that you have installed R and RStudio, it’s time to get familiar with the environment and start coding.\n\n\nRStudio provides a user-friendly interface that integrates various tools for working with R. Here’s a quick overview of the main components:\n\nSource Pane:\n\nThis is where you write and edit your R scripts. You can open multiple tabs to work on different scripts simultaneously.\n\nConsole Pane:\n\nThe console pane is where you can directly enter R commands and see the output immediately.\n\nEnvironment/History Pane:\n\nThe Environment tab shows all the objects (data, variables, functions) currently in your workspace.\nThe History tab keeps a record of all the commands you’ve entered in the console.\n\nFiles/Plots/Packages/Help/Viewer Pane:\n\nThe Files tab lets you navigate your file system.\nThe Plots tab displays plots generated by your R code.\nThe Packages tab allows you to manage R packages.\nThe Help tab provides access to R documentation and help files.\nThe Viewer tab can display web content or documents.\n\n\n#todo: insert video here"
  },
  {
    "objectID": "labs/lab-5.html#introduction",
    "href": "labs/lab-5.html#introduction",
    "title": "Lab 4 - The Office",
    "section": "",
    "text": "In today’s lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\nThis is a different data source than the one we’ve used in class last week.\n\n\nBy the end of the lab you will…\n\nengineer features based on episode scripts\ntrain a model\ninterpret model coefficients\nmake predictions\nevaluate model performance on training and testing data\n\n\n\n\n\n\nR is a powerful statistical programming language used for data analysis and visualization. Follow these steps to install R on your computer:\n\nDownload R:\n\nGo to the CRAN R Project website.\nClick on the link for your operating system (Windows, macOS, or Linux).\nFor Windows: Click on “Download R for Windows” and then click on “base”. Click on the download link to get the installer.\nFor macOS: Click on “Download R for (Mac) OS X”. Choose the appropriate file for your macOS version and download the installer.\nFor Linux: Follow the instructions provided for your specific distribution (Debian, Ubuntu, Fedora, etc.).\n\nInstall R:\n\nWindows: Run the downloaded .exe file and follow the installation prompts. Accept the default settings unless you have specific requirements.\nmacOS: Open the downloaded .pkg file and follow the installation prompts.\nLinux: Use the terminal and follow the instructions from the CRAN website to add the R repository and install R via your package manager.\n\n\n\n\n\nRStudio is an integrated development environment (IDE) for R that makes coding in R easier and more efficient. Follow these steps to install RStudio:\n\nDownload RStudio:\n\nGo to the RStudio website.\nClick on the “Download” button under RStudio Desktop.\nChoose the installer for your operating system (Windows, macOS, or Linux).\n\nInstall RStudio:\n\nWindows: Run the downloaded .exe file and follow the installation prompts.\nmacOS: Open the downloaded .dmg file and drag the RStudio icon to your Applications folder.\nLinux: Open the terminal and follow the instructions provided on the RStudio website to install RStudio via your package manager or download and install the .deb or .rpm package.\n\nLaunch RStudio:\n\nAfter installation, open RStudio. It should automatically detect your R installation. If it doesn’t, you may need to manually configure the path to the R executable in the RStudio settings.\n\n\nBy completing these steps, you will have both R and RStudio installed on your computer, ready for use in data analysis and visualization tasks.\n\n\n\n\n\nNow that you have installed R and RStudio, it’s time to get familiar with the environment and start coding.\n\n\nRStudio provides a user-friendly interface that integrates various tools for working with R. Here’s a quick overview of the main components:\n\nSource Pane:\n\nThis is where you write and edit your R scripts. You can open multiple tabs to work on different scripts simultaneously.\n\nConsole Pane:\n\nThe console pane is where you can directly enter R commands and see the output immediately.\n\nEnvironment/History Pane:\n\nThe Environment tab shows all the objects (data, variables, functions) currently in your workspace.\nThe History tab keeps a record of all the commands you’ve entered in the console.\n\nFiles/Plots/Packages/Help/Viewer Pane:\n\nThe Files tab lets you navigate your file system.\nThe Plots tab displays plots generated by your R code.\nThe Packages tab allows you to manage R packages.\nThe Help tab provides access to R documentation and help files.\nThe Viewer tab can display web content or documents.\n\n\n#todo: insert video here"
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Tutorial 3 - SQL",
    "section": "",
    "text": "Learning Objective\n\n\n\n\nable to identify prescriptive, descriptive and predictive analytics.\nunderstand database management system.\n\n\n\n\nQuestion 1\nA control chart is a graphical tool to help determine if a process is in control or out of control. The following figure shows a control chart for a production line that fills boxes of cereal. Based on past data, we can calculate the mean weight of a box of cereal when the process is in control. The mean weight is 16.05 ounces. We can also calculate control limits, an upper control limit (UCL) and a lower control limit (LCL). New samples are collected over time and the data indicates that the process is in control so long as the new sample weights are between UCL and LCL. As shown in the chart, only sample 5 is outside of the control limits.\n\na. Is the control chart an example of descriptive, predictive, or prescriptive analytics?\nb. Suppose the control chart is part of a data dashboard and the chart is combined with a rule that does the following. If four consecutive sample mean weights are outside of the control limits, the production line is automatically stopped, and a message appears on the dashboard. The message says “The production line is stopped. The process may be out of control. Please inspect the fill machine.” Is this new enhanced control chart combined with a rule an example of descriptive, predictive, or prescriptive analytics?\n\n\nQuestion 2\nAn example of a response from Amazon when this Camm et al. (2024) you are reading was chosen online, is given below. It indicates that some people who purchased this text also tended to purchase The World Is Flat, Fundamentals of Corporate Finance, and Strategic Marketing Management. In this application of analytics, is Amazon using descriptive, predictive, or prescriptive analytics? Explain.\n\n\n\nQuestion 3\nHuman Resource (HR) Analytics or People analytics are terms used for the use of analytics to manage a company’s workforce. Google, Microsoft, and Walmart, for example, use people analytics to help retain their best people, ensure a diverse workforce, and better understand the strengths and areas needing improvement.\na. One application of people analytics is to build a model that estimates the probability of an employee departing the company within the next six months. Inputs to the model could be market demand for the skills the person possesses, how long the person has been with the company, and a major life event recently occurring for the person (for example, divorce). Is this type of model descriptive, predictive, or prescriptive? Explain.\nb. How could you use the model described in part (a) to help improve the workforce?\n\n\nQuestion 4\nA supermarket has been experiencing long lines during peak periods of the day. The problem is noticeably worse on certain days of the week, and the peak periods are sometimes different according to the day of the week. There are usually enough workers on the job to open all checkout lanes. The problem is knowing when to call some of the workers stocking shelves up to the front of the store to work the checkout lanes.\na. How could analytics be used to help the supermarket?\nb. What data would be needed?\n\n\nQuestion 5\nSetting the right price for a product is an important business decision. Price the product too high and demand could be too low. Set the price too low, demand may be high, but we are potentially leaving money on the table because the revenue per unit is low. Pricing analytics involves finding the right trade-off between the price charged and demand so as to maximize revenue. Suppose we need to set the price for renting a subcompact automobile for one day. Let us outline the decision process:\n\nIdentify and define the problem. We need to set a price per day for a midsize rental car.\nDetermine the criteria that will be used to evaluate alternative solutions. Our goal in setting the price is to maximize revenue.\nDetermine the set of alternative solutions. Based on historical data and the competition, we will consider a broad price range from $10 per day to $60 per day.\nEvaluate the alternatives. We will evaluate proposed prices based on the expected revenue per day.\nChoose an alternative. We will choose the price that maximizes expected revenue.\n\nWe can use data and analytics to complete steps 4 and 5 of the decision process.\n\nBased on historical or test market data, we can estimate a model that gives expected revenue as a function of price, as shown below. The dots represent the data (price and demand combinations) and the estimated model is the line in the chart: Demand = –1.4028 (Price) + 102.65. For example, for the price of $35 Demand = –1.4028(35) + 102.65 = 53.552 vehicles. So, we estimate that at a price of $35 per day, the demand will be about 54 vehicles. Is this estimated equation, a descriptive, predictive, or prescriptive model? Explain.\n\n\n\nWhat is the price that maximizes revenue?\n\nc.   Is step 5, visually expecting the revenue function to find a revenue-maximizing price, descriptive, predictive, or prescriptive analytics? Explain.\n\n\n\nQuestion 6\nYou are managing a small bookstore’s database. The database has the following tables:\nBooks\n\nBookID (Primary Key)\nTitle\nAuthor\nPrice\n\nCustomers\n\nCustomerID (Primary Key)\nName\nEmail\n\nOrders\n\nOrderID (Primary Key)\nCustomerID (Foreign Key)\nOrderDate\n\nOrderDetails\n\nOrderDetailID (Primary Key)\nOrderID (Foreign Key)\nBookID (Foreign Key)\nQuantity\n\n\nExplain why a Database Management System (DBMS) is important for managing data in an organization. List at least three key benefits of using a DBMS compared to traditional file-based systems.\nExplain what a primary key is and why it is important.\nWhat is the purpose of a foreign key in a database?\n\n\n\nReference\nBusiness Analytics by Camm, Cochran, Fry, Ohlmann, Anderson & Sweeney, 2024, 5th Edition."
  },
  {
    "objectID": "labs/lab-3.html#introduction",
    "href": "labs/lab-3.html#introduction",
    "title": "Lab 3 - Coffee ratings",
    "section": "",
    "text": "In today’s lab you will analyze data from over 1,000 different coffees to explore the relationship between a coffee’s aroma and it’s overall quality. You will also begin working with your team and practicing a collaborative data analysis workflow.\n\n\nBy the end of the lab you will…\n\nCreate plots and calculate associated statistics to assess model diagnostics.\nPractice collaborating with others using a single Github repo.\n\n\n\n\nPower BI Desktop is a powerful data visualization tool that allows you to connect to various data sources, transform data, and create interactive reports. Follow these steps to download and install Power BI Desktop on your computer.\n\n\n\nVisit the Power BI Website:\n\nGo to the Power BI website.\n\nNavigate to the Download Page:\n\nClick on the Products menu at the top of the page and select Power BI Desktop.\n\nDownload the Installer:\n\nClick on the Download free button to go to the Microsoft Store page for Power BI Desktop.\nIf you prefer to download the installer directly (instead of through the Microsoft Store), click on the See download or language options link and choose the version suitable for your operating system.\n\n\n\n\n\n\nRun the Installer:\n\nIf you downloaded Power BI Desktop from the Microsoft Store, it will install automatically after you click the Get button.\nIf you downloaded the installer directly, locate the downloaded file (typically a .msi file) and double-click it to run the installer.\n\nFollow the Installation Prompts:\n\nClick Next on the welcome screen.\nAccept the license agreement and click Next.\nChoose the installation folder or accept the default path and click Next.\nClick Install to begin the installation process.\n\nComplete the Installation:\n\nOnce the installation is complete, click Finish to close the installer.\n\nLaunch Power BI Desktop:\n\nAfter installation, you can launch Power BI Desktop from the Start menu (Windows) or by searching for it in your applications.\n\n\n\n\n\n\nSign In to Power BI:\n\nWhen you first open Power BI Desktop, you may be prompted to sign in. If you have a Power BI account, sign in with your credentials. If not, you can create a new account or skip this step.\n\nExplore the Interface:\n\nFamiliarize yourself with the Power BI Desktop interface, which includes the following key components:\n\nReport View: Where you create and arrange visualizations.\nData View: Where you can see your data tables.\nModel View: Where you can manage relationships between tables.\nFields Pane: Displays the tables and fields in your data model.\nVisualizations Pane: Contains various types of visualizations you can add to your reports.\n\n\nConnect to Data Sources:\n\nClick on Get Data in the Home ribbon to connect to a variety of data sources, such as Excel, SQL Server, web data, and more.\nFollow the prompts to load your data into Power BI Desktop."
  },
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Tutorial 2 - Business Analytics & Database Management System",
    "section": "",
    "text": "Question 1 (Camm et al. (2024), Chap 1 - Q6)\nA control chart is a graphical tool to help determine if a process is in control or out of control. The following figure shows a control chart for a production line that fills boxes of cereal. Based on past data, we can calculate the mean weight of a box of cereal when the process is in control. The mean weight is 16.05 ounces. We can also calculate control limits, an upper control limit (UCL) and a lower control limit (LCL). New samples are collected over time and the data indicates that the process is in control so long as the new sample weights are between UCL and LCL. As shown in the chart, only sample 5 is outside of the control limits.\na. Is the control chart an example of descriptive, predictive, or prescriptive analytics?\nb. Suppose the control cart is part of a data dashboard and the chart is combined with a rule that does the following. If four consecutive sample mean weights are outside of the control limits, the production line is automatically stopped, and a message appears on the dashboard. The message says “The production line is stopped. The process may be out of control. Please inspect the fill machine.” Is this new enhanced control chart combined with a rule an example of descriptive, predictive, or prescriptive analytics?\n\n\nQuestion 2 (Camm et al. (2024), Chap 1- Q7)\nAn example of a response from Amazon when this Camm et al. (2024) you are reading was chosen online, is given below. It indicates that some people who purchased this text also tended to purchase The World Is Flat, Fundamentals of Corporate Finance, and Strategic Marketing Management. In this application of analytics, is Amazon using descriptive, predictive, or prescriptive analytics? Explain.\n\n\nQuestion 3 (Camm et al. (2024), Chap 1 - Q8)\nHuman Resource (HR) Analytics or People analytics are terms used for the use of analytics to manage a company’s workforce. Google, Microsoft, and Walmart, for example, use people analytics to help retain their best people, ensure a diverse workforce, and better understand the strengths and areas needing improvement.\na. One application of people analytics is to build a model that estimates the probability of an employee departing the company within the next six months. Inputs to the model could be market demand for the skills the person possesses, how long the person has been with the company, and a major life event recently occurring for the person (for example, divorce). Is this type of model descriptive, predictive, or prescriptive? Explain.\nb. How could you use the model described in part (a) to help improve the workforce?\n\n\nQuestion 4 (Camm et al. (2024), Chap 1 - Q9)\nA supermarket has been experiencing long lines during peak periods of the day. The problem is noticeably worse on certain days of the week, and the peak periods are sometimes different according to the day of the week. There are usually enough workers on the job to open all checkout lanes. The problem is knowing when to call some of the workers stocking shelves up to the front of the store to work the checkout lanes.\na. How could analytics be used to help the supermarket?\nb. What data would be needed?\n\n\nQuestion 5 (Camm et al. (2024), Chap 1 - Q10)\nSetting the right price for a product is an important business decision. Price the product too high and demand could be too low. Set the price too low, demand may be high, but we are potentially leaving money on the table because the revenue per unit is low. Pricing analytics involves finding the right trade-off between the price charged and demand so as to maximize revenue. Suppose we need to set the price for renting a subcompact automobile for one day. Let us outline the decision process:\n\nIdentify and define the problem. We need to set a price per day for a midsize rental car.\nDetermine the criteria that will be used to evaluate alternative solutions. Our goal in setting the price is to maximize revenue.\nDetermine the set of alternative solutions. Based on historical data and the competition, we will consider a broad price range from $10 per day to $60 per day.\nEvaluate the alternatives. We will evaluate proposed prices based on the expected revenue per day.\nChoose an alternative. We will choose the price that maximizes expected revenue.\n\nWe can use data and analytics to complete steps 4 and 5 of the decision process.\na.   Based on historical or test market data, we can estimate a model that gives expected revenue as a function of price, as shown below. The dots represent the data (price and demand combinations) and the estimated model is the line in the chart: Demand = –1.4028 (Price) + 102.65. For example, for the price of $35 Demand = –1.4028(35) + 102.65 = 53.552 vehicles. So, we estimate that at a price of $35 per day, the demand will be about 54 vehicles. Is this estimated equation, a descriptive, predictive, or prescriptive model? Explain.\nOur goal (step 5) is to find the price that maximizes expected revenue. Revenue = demand x price which is (–1.4028(Price) + 102.65)  x (Price) = –1.40228(Price)2 + 102.65(Price). The revenue as a function of price is shown below for $10 increments of price.\nb.   What is the price that maximizes revenue?\nc.   Is step 5, visually expecting the revenue function to find a revenue-maximizing price, descriptive, predictive, or prescriptive analytics? Explain.\n\n\n\n\n\nQuestion 6\nYou are managing a small bookstore’s database. The database has the following tables:\nBooks\n\nBookID (Primary Key)\nTitle\nAuthor\nPrice\n\nCustomers\n\nCustomerID (Primary Key)\nName\nEmail\n\nOrders\n\nOrderID (Primary Key)\nCustomerID (Foreign Key)\nOrderDate\n\nOrderDetails\n\nOrderDetailID (Primary Key)\nOrderID (Foreign Key)\nBookID (Foreign Key)\nQuantity\na. Explain why a Database Management System (DBMS) is important for managing data in an organization. List at least three key benefits of using a DBMS compared to traditional file-based systems.\nb. Explain what a primary key is and why it is important.\nc. What is the purpose of a foreign key in a database?\n\n\n\nReference\nBusiness Analytics by Camm, Cochran, Fry, Ohlmann, Anderson & Sweeney, 2024, 5th Edition."
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4 - The Office",
    "section": "",
    "text": "In today’s lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\n\n\n\n\n\n\nNote\n\n\n\nThis is a different data source than the one we’ve used in class last week.\n\n\n\n\nBy the end of the lab you will…\n\nengineer features based on episode scripts\ntrain a model\ninterpret model coefficients\nmake predictions\nevaluate model performance on training and testing data\n\n\n\n\n\n\nR is a powerful statistical programming language used for data analysis and visualization. Follow these steps to install R on your computer:\n\nDownload R:\n\nGo to the CRAN R Project website.\nClick on the link for your operating system (Windows, macOS, or Linux).\nFor Windows: Click on “Download R for Windows” and then click on “base”. Click on the download link to get the installer.\nFor macOS: Click on “Download R for (Mac) OS X”. Choose the appropriate file for your macOS version and download the installer.\nFor Linux: Follow the instructions provided for your specific distribution (Debian, Ubuntu, Fedora, etc.).\n\nInstall R:\n\nWindows: Run the downloaded .exe file and follow the installation prompts. Accept the default settings unless you have specific requirements.\nmacOS: Open the downloaded .pkg file and follow the installation prompts.\nLinux: Use the terminal and follow the instructions from the CRAN website to add the R repository and install R via your package manager.\n\n\n\n\n\nRStudio is an integrated development environment (IDE) for R that makes coding in R easier and more efficient. Follow these steps to install RStudio:\n\nDownload RStudio:\n\nGo to the RStudio website.\nClick on the “Download” button under RStudio Desktop.\nChoose the installer for your operating system (Windows, macOS, or Linux).\n\nInstall RStudio:\n\nWindows: Run the downloaded .exe file and follow the installation prompts.\nmacOS: Open the downloaded .dmg file and drag the RStudio icon to your Applications folder.\nLinux: Open the terminal and follow the instructions provided on the RStudio website to install RStudio via your package manager or download and install the .deb or .rpm package.\n\nLaunch RStudio:\n\nAfter installation, open RStudio. It should automatically detect your R installation. If it doesn’t, you may need to manually configure the path to the R executable in the RStudio settings.\n\n\nBy completing these steps, you will have both R and RStudio installed on your computer, ready for use in data analysis and visualization tasks.\n\n\n\n\n\nNow that you have installed R and RStudio, it’s time to get familiar with the environment and start coding.\n\n\nRStudio provides a user-friendly interface that integrates various tools for working with R. Here’s a quick overview of the main components:\n\nSource Pane:\n\nThis is where you write and edit your R scripts. You can open multiple tabs to work on different scripts simultaneously.\n\nConsole Pane:\n\nThe console pane is where you can directly enter R commands and see the output immediately.\n\nEnvironment/History Pane:\n\nThe Environment tab shows all the objects (data, variables, functions) currently in your workspace.\nThe History tab keeps a record of all the commands you’ve entered in the console.\n\nFiles/Plots/Packages/Help/Viewer Pane:\n\nThe Files tab lets you navigate your file system.\nThe Plots tab displays plots generated by your R code.\nThe Packages tab allows you to manage R packages.\nThe Help tab provides access to R documentation and help files.\nThe Viewer tab can display web content or documents.\n\n\n#todo: insert video here"
  },
  {
    "objectID": "labs/lab-4.html#introduction",
    "href": "labs/lab-4.html#introduction",
    "title": "Lab 4 - The Office",
    "section": "",
    "text": "In today’s lab you will analyze data from the schrute package to predict IMDB scores for episodes of The Office.\n\n\n\n\n\n\nNote\n\n\n\nThis is a different data source than the one we’ve used in class last week.\n\n\n\n\nBy the end of the lab you will…\n\nengineer features based on episode scripts\ntrain a model\ninterpret model coefficients\nmake predictions\nevaluate model performance on training and testing data\n\n\n\n\n\n\nR is a powerful statistical programming language used for data analysis and visualization. Follow these steps to install R on your computer:\n\nDownload R:\n\nGo to the CRAN R Project website.\nClick on the link for your operating system (Windows, macOS, or Linux).\nFor Windows: Click on “Download R for Windows” and then click on “base”. Click on the download link to get the installer.\nFor macOS: Click on “Download R for (Mac) OS X”. Choose the appropriate file for your macOS version and download the installer.\nFor Linux: Follow the instructions provided for your specific distribution (Debian, Ubuntu, Fedora, etc.).\n\nInstall R:\n\nWindows: Run the downloaded .exe file and follow the installation prompts. Accept the default settings unless you have specific requirements.\nmacOS: Open the downloaded .pkg file and follow the installation prompts.\nLinux: Use the terminal and follow the instructions from the CRAN website to add the R repository and install R via your package manager.\n\n\n\n\n\nRStudio is an integrated development environment (IDE) for R that makes coding in R easier and more efficient. Follow these steps to install RStudio:\n\nDownload RStudio:\n\nGo to the RStudio website.\nClick on the “Download” button under RStudio Desktop.\nChoose the installer for your operating system (Windows, macOS, or Linux).\n\nInstall RStudio:\n\nWindows: Run the downloaded .exe file and follow the installation prompts.\nmacOS: Open the downloaded .dmg file and drag the RStudio icon to your Applications folder.\nLinux: Open the terminal and follow the instructions provided on the RStudio website to install RStudio via your package manager or download and install the .deb or .rpm package.\n\nLaunch RStudio:\n\nAfter installation, open RStudio. It should automatically detect your R installation. If it doesn’t, you may need to manually configure the path to the R executable in the RStudio settings.\n\n\nBy completing these steps, you will have both R and RStudio installed on your computer, ready for use in data analysis and visualization tasks.\n\n\n\n\n\nNow that you have installed R and RStudio, it’s time to get familiar with the environment and start coding.\n\n\nRStudio provides a user-friendly interface that integrates various tools for working with R. Here’s a quick overview of the main components:\n\nSource Pane:\n\nThis is where you write and edit your R scripts. You can open multiple tabs to work on different scripts simultaneously.\n\nConsole Pane:\n\nThe console pane is where you can directly enter R commands and see the output immediately.\n\nEnvironment/History Pane:\n\nThe Environment tab shows all the objects (data, variables, functions) currently in your workspace.\nThe History tab keeps a record of all the commands you’ve entered in the console.\n\nFiles/Plots/Packages/Help/Viewer Pane:\n\nThe Files tab lets you navigate your file system.\nThe Plots tab displays plots generated by your R code.\nThe Packages tab allows you to manage R packages.\nThe Help tab provides access to R documentation and help files.\nThe Viewer tab can display web content or documents.\n\n\n#todo: insert video here"
  },
  {
    "objectID": "notes/w2.html",
    "href": "notes/w2.html",
    "title": "Lecture Note Week 2",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Able to write SQL queries to retrieve data from a database\nLO2: Able to write SQL queries to manipulate data in a database\nLO3: Understand basic SQL syntax"
  },
  {
    "objectID": "notes/w2.html#section",
    "href": "notes/w2.html#section",
    "title": "Lecture Note 2",
    "section": "",
    "text": "SELECT Statement\nThe SELECT statement is used to query the database and retrieve data. It is the most fundamental SQL statement.\nSyntax:\nsql\nCopy code\nSELECT column1, column2, ... FROM table_name;\nExample:\nsql\nCopy code\nSELECT first_name, last_name FROM employees;\nThis query retrieves the first_name and last_name columns from the employees table.\n\n\nDISTINCT Keyword\nThe DISTINCT keyword is used to return only distinct (different) values.\nSyntax:\nsql\nCopy code\nSELECT DISTINCT column1, column2, ... FROM table_name;\nExample:\nsql\nCopy code\nSELECT DISTINCT department_id FROM employees;\nThis query retrieves unique department_id values from the employees table.\n\n\nORDER BY Clause\nThe ORDER BY clause is used to sort the result set in either ascending or descending order.\nSyntax:\nsql\nCopy code\nSELECT column1, column2, ... FROM table_name ORDER BY column1 [ASC|DESC], column2 [ASC|DESC], ...;\nExample:\nsql\nCopy code\nSELECT first_name, last_name FROM employees ORDER BY last_name ASC;\nThis query retrieves the first_name and last_name columns from the employees table and sorts the results by last_name in ascending order.\n\n\nLIMIT Clause\nIn Oracle SQL, the equivalent of LIMIT is the FETCH FIRST clause. It restricts the number of rows returned by a query.\nSyntax:\nsql\nCopy code\nSELECT column1, column2, ... FROM table_name FETCH FIRST number ROWS ONLY;\nExample:\nsql"
  },
  {
    "objectID": "notes/w3.html#section-1-introduction-to-r-language",
    "href": "notes/w3.html#section-1-introduction-to-r-language",
    "title": "Lecture Note Week 3",
    "section": "Section 1: Introduction to R Language",
    "text": "Section 1: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management.\n\n\nBasic of R\n\nR as a Calculator\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nR Objects\nIn R, everything is an object. These objects serve as containers for various types of data. Whether you’re dealing with a single number, a character string (like a word), or a complex structure like the output of a plot or a statistical analysis summary, it’s all represented as an object.\nCreating Objects:\nTo create an object, you simply give it a name. For instance\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this example, an object is created called x and it is assigned the value 5. The &lt;- is the assignment operator. It assigns the value on the right to the object on the left. You can also use = to assign values to objects, but it’s considered bad practice.\nViewing Objects:\nTo view the value of an object, you simply type the name of the object and press enter. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nR keeps track of all objects in the current workspace during the session. You can see all the objects in the current workspace by typing ls() in the console.\nOpearations with Objects:\nYou can perform operations with objects. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nObjects names:\nObject names can contain letters, numbers, periods, and underscores. However, they can only starts with letters or underscore and nothing else. They are case-sensitive, so x and X are different objects. They cannot start with a number or a period. If you would like to insist to have numbers or period as the first character, you can use backticks to define the object name. It is called nonsyntactic names. For instance, you can define the following:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExercises:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCreate an object called a and assign the value 10 to it.\nCreate an object called b and assign the value 20 to it.\n\nCreate an object called star and assign the value a + b to it.\nView the value of star.\nWhich of the following is not the syntactic name for an object?\n\nx\nX\n1x\nx1\nx.y\nx_y\n\n\n\n\nData Types\nR has several data types. The most common data types are:\n\nNumeric (double)\n\n\nrepresents eal numbers (e.g., 3.14, 0.0001, 1000.0).\ncan be positive or negative.\ncan be in scientific notation (e.g., 1.23e-5).\nused for continuous data like measurements, weights, heights, etc.\n\n\nCharacter\n\n\nrepresents text data (e.g., “hello”, “world”, “R is fun”).\nmust be enclosed in quotes.\nused for categorical data (e.g., “High School”, “Primary School”, “University”).\n\n\nLogical (boolean)\n\n\nrepresents binary data (e.g., TRUE or FALSE).\nused for logical operations.\n\n\nInteger\n\n\nrepresents whole numbers (e.g., 1, 2, 3, 1000).\ncan be positive or negative.\nused for counting data like number of students, number of cars, etc.\nsometimes you will see it ends with L (e.g., 1L, 2L, 3L, 1000L). This is to indicate that the number is an integer.\n\n\nFactors\n\n\nrepresents categorical data (e.g., “High School”, “Primary School”, “University”).\nused for categorical data.\n\n\nComplex\n\n\nrepresents complex numbers (e.g., 1 + 2i, 3 + 4i, 5 + 6i).\nused for complex data like electrical engineering, physics, etc.\n\nHowever, we seldom deal with complex data types. We will focus on the first four data types.\nExercises:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat is the data type of 3.14?\nWhat is the data type of \"hello\"?\nWhat is the data type of TRUE?\nWhat is the data type of 1L?\nWhat is the data type of factor(\"High School\")?\nWhat is the data type of 1 + 2i?\nWhat is the data type of \"1\"?\n\nNote: You can check your answers using typeof() function.\n\n\nFunctions\nA function is a block of code that performs a specific task. R has a large number of in-built functions and also allows users to define their own functions. We will learn more about how to create functions in the coming weeks. But so far, we will use some of the in-built functions. Anything that starts with ( and end with ) is a function.\nExercises:\nWhich of the following is a function?\n\nmean()\nmedian\nsd()\nvar()\nsum[]\n\n\n\nVectors\nVectors are the most basic data structure in R. They are one-dimensional arrays that can hold numeric, character, or logical data. You can create a vector using the c() function. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAccessing Elements:\nYou can access the elements of a vector using the index. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExercises:\n\nCreate a vector called a with the values 11, 2, 33, 4, 5.\nCreate a vector called b with the values \"a\", \"bc\", \"c\", \"d\", \"ef\".\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nData Frames\nData frames are the most common data structure in R. They are used to store tabular data. A data frame is a list of vectors of equal length. Each vector represents a column in the data frame, and each element in the vector represents a row in the data frame.\nCreating Data Frames:\nYou can create a data frame using the data.frame() function. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAccessing Data Frames:\nYou can access the elements of a data frame using the row and column indices. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExercises:\n\nCreate a data frame called students with the following columns: name, age, gender, height, and weight. The data should be as follows:\n\n\nname: “Alice”, “Bob”, “Charlie”, “David”\nage: 25, 30, 35, 40\ngender: “F”, “M”, “M”, “M”\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nLists\nLists are another data structure in R that can hold elements of different data types. You can create a list using the list() function. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can access the elements of a list using the index. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExercises:\n\nCreate a list called y with the following elements: 1, \"b\", FALSE.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPackages\nR packages are collections of functions and data sets developed by the community. They increase the power of R by improving existing base R functionalities, or by adding new ones.\nYou can install packages using the install.packages() function, and load them using the library() function. For instance:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou do not need to install packages every time. You just need to install it once, and you can use them directly by loading them using the library() function."
  },
  {
    "objectID": "notes/w5.html",
    "href": "notes/w5.html",
    "title": "Lecture Note Week 5",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Data wrangling with Power BI\nLO2: Data wrangling with R"
  },
  {
    "objectID": "notes/w5.html#section-1-introduction-to-power-bi",
    "href": "notes/w5.html#section-1-introduction-to-power-bi",
    "title": "Lecture Note 3",
    "section": "Section 1: Introduction to Power BI",
    "text": "Section 1: Introduction to Power BI\nWelcome to Power BI!\nPower BI is a powerful business analytics tool that enables you to visualize data, share insights, and make data-driven decisions. This introductory document will guide you through the basics of Power BI and help you get started with creating your first report.\n\nWhat is Power BI?\nPower BI is a suite of business analytics tools by Microsoft that includes software, services, and connectors. It transforms your unrelated sources of data into coherent, visually immersive, and interactive insights. Your data may be an Excel spreadsheet, or a collection of cloud-based and on-premises hybrid data warehouses.\n\n\nGetting Started\n\n1. Download and Install Power BI Desktop\n\nGo to the Power BI website and download the Power BI Desktop.\nFollow the installation instructions to install Power BI Desktop on your computer.\n\n\n\n2. Open Power BI Desktop\nOnce installed, open Power BI Desktop. You will see the following interface:\n\n\n\n3. Connect to Data\nTo start working with data, you need to connect Power BI to your data source. Here’s how:\n\nClick on the “Home” tab.\nClick on the “Get Data” button.\n\n\n\nSelect your data source from the list (e.g., Excel, SQL Server, Web, etc.).\nFollow the prompts to load your data into Power BI.\n\n\n\n4. Transform Data with Power Query Editor\nAfter loading your data, you might need to clean and transform it. Use Power Query Editor for this purpose:\n\nClick on the “Home” tab.\nClick on the “Transform Data” button.\n\n\nThe Power Query Editor will open, where you can clean and prepare your data.\n\n\n\n5. Create Visualizations\nOnce your data is ready, you can start creating visualizations:\n\nGo back to the main Power BI Desktop window.\nIn the “Visualizations” pane, select the type of visualization you want to create (e.g., bar chart, line chart, pie chart, etc.).\nDrag and drop fields from the “Fields” pane to the “Values”, “Axis”, “Legend”, etc., in the “Visualizations” pane.\n\n\n\n\n6. Build Reports\nCombine multiple visualizations into a report:\n\nUse the “Report” view in Power BI Desktop.\nArrange your visualizations on the canvas.\nAdd titles, labels, and other elements to enhance your report.\n\n\n\n7. Save and Publish Your Report\n\nClick on the “File” tab.\nSelect “Save As” to save your report locally.\nTo share your report, click on the “Publish” button.\n\n\nFollow the prompts to publish your report to the Power BI service, where you can share it with others.\n\n\n\nConclusion\nCongratulations! You have successfully created your first Power BI report. Power BI is a versatile tool with many advanced features, so continue exploring and experimenting to unlock its full potential. Happy data analyzing!"
  },
  {
    "objectID": "notes/w5.html#section-2-introduction-to-r-language",
    "href": "notes/w5.html#section-2-introduction-to-r-language",
    "title": "Lecture Note 3",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w1.html#section-1-introduction-to-sql",
    "href": "notes/w1.html#section-1-introduction-to-sql",
    "title": "Lecture Note Week 1",
    "section": "Section 1: Introduction to SQL",
    "text": "Section 1: Introduction to SQL\n\nWhat is SQL?\nSQL stands for Structured Query Language. It is a language used to communicate with databases. It is the standard language for relational database management systems. Why we need SQL? Let’s say you are suppose to design for a system that will store the information of students in a university. How would you store the information of students? You can store the information in a spreadsheet, but what if you have to store the information of thousands of students? It will be very difficult to manage the data in a spreadsheet. Also, where do you want to store the information? In a table? Will a table suffice? What if you have to store the information of the courses that the students are enrolled in? How will you manage the data? This is where databases come in.\nDatabases are used to store large amount of data. SQL is used to communicate with the database!\nThe database can be of any type like MySQL, PostgreSQL, SQLite, etc. By ignoring all those, let’s focus on the SQL language itself, which is used to communicate with the database. Once you master SQL, you can work with any database. The most popular database is MySQL. It is an open-source database. It is used by many companies like Facebook, Twitter, etc. Oracle is also a popular database. It is used by many big companies. SQL is used to communicate with the database. It is used to perform all types of operations on the database. It is used to create a database, create a table in the database, insert data into the table, update the data, delete the data, etc.\n\n\nSQL Syntax\nSQL syntax is the set of rules that defines how a SQL query should be written. It is the set of rules that all SQL queries should follow. SQL syntax is similar to the English language, which makes it easier to write, read, and understand. SQL syntax is divided into several categories. These categories are:\n\nData Definition Language (DDL)\nData Manipulation Language (DML)\nData Query Language (DQL)\nData Control Language (DCL)\nTransaction Control Language (TCL)\n\n\n\nData Definition Language (DDL)\nData Definition Language (DDL) is used to define the structure that holds the data. It is used to create tables, columns, etc. DDL commands are auto-committed. It means that the changes made by the DDL command are saved to the database automatically. The following are the DDL commands:\n\nCREATE\nALTER\nDROP\nTRUNCATE\nCOMMENT\n\n\n\nData Manipulation Language (DML)\nData Manipulation Language (DML) is used to manipulate the data itself. It is used to insert, update, delete, and retrieve data from the database. DML commands are not auto-committed. It means that the changes made by the DML command are not saved to the database automatically. You have to use the COMMIT command to save the changes. The following are the DML commands:\n\nSELECT\nINSERT\nUPDATE\nDELETE\n\n\n\nData Query Language (DQL)\nData Query Language (DQL) is used to retrieve the data from the database. The SELECT statement is the most commonly used DQL command. The following are the DQL commands:\n\nSELECT\nWHERE\nGROUP BY\nHAVING\nORDER BY\n\n\n\nData Control Language (DCL)\nData Control Language (DCL) is used to control the visibility of data. It is used to control the access of data. The following are the DCL commands:\n\nGRANT\nREVOKE\n\n\n\nTransaction Control Language (TCL)\nTransaction Control Language (TCL) is used to manage the transactions in the database. It is used to manage the changes made by the DML commands. The following are the TCL commands:\n\nCOMMIT\nROLLBACK\nSAVEPOINT"
  },
  {
    "objectID": "notes/w1.html#section-2-live-sql-oracle",
    "href": "notes/w1.html#section-2-live-sql-oracle",
    "title": "Lecture Note Week 1",
    "section": "Section 2: Live SQL Oracle",
    "text": "Section 2: Live SQL Oracle\nOracle Live SQL is a free, interactive, web-based tool that allows users to write, run, and share SQL scripts and PL/SQL code. It provides a convenient platform for learning and practicing SQL without needing to install Oracle Database software locally. Key benefits include:\n\nEase of Use: The web-based interface is user-friendly, making it accessible for both beginners and experienced users to write and execute SQL code.\nEducational Resources: Live SQL offers tutorials, sample scripts, and documentation to help users learn SQL and PL/SQL concepts and best practices.\nCollaboration: Users can share their scripts and solutions with others, fostering collaboration and knowledge sharing within the community.\nReal-Time Execution: Scripts run on an Oracle Database in real-time, providing immediate feedback and results.\nAccess Anywhere: As a web-based tool, Live SQL can be accessed from any device with an internet connection, making it convenient for users to practice and develop their SQL skills on the go.\n\nOracle Live SQL is an excellent resource for anyone looking to improve their SQL proficiency, offering hands-on experience with Oracle Database in a flexible and collaborative environment."
  },
  {
    "objectID": "notes/w2.html#sql-statement-hierarchy",
    "href": "notes/w2.html#sql-statement-hierarchy",
    "title": "Lecture Note Week 2",
    "section": "SQL Statement Hierarchy",
    "text": "SQL Statement Hierarchy\n\nSELECT: Specifies the columns to retrieve.\nFROM: Specifies the table(s) to query.\nWHERE: Filters rows based on a condition.\nGROUP BY: Groups rows sharing a property so that aggregate functions can be applied.\nHAVING: Filters groups based on a condition.\nORDER BY: Sorts the result set.\nFETCH FIRST: Limits the number of rows returned.\n\n\nGraphic Representation\n\n\nExample Query with Hierarchy\nSELECT: Specifies the columns to retrieve. FROM: Specifies the table(s) to query. WHERE: Filters rows based on a condition. GROUP BY: Groups rows sharing a property so that aggregate functions can be applied. HAVING: Filters groups based on a condition. ORDER BY: Sorts the result set. FETCH FIRST: Limits the number of rows returned.\nLet’s illustrate this hierarchy with an example query:\nSELECT Country, COUNT(CustomerID) AS CustomerCount, AVG(LENGTH(CustomerName)) AS AvgCustomerNameLength\nFROM Customers\nGROUP BY Country\nHAVING COUNT(CustomerID) &gt; 2\nORDER BY AvgCustomerNameLength DESC\nFETCH FIRST 5 ROWS ONLY;\nExplanation of the Query:\nSELECT: Specifies the columns to retrieve: Country, the count of CustomerID as CustomerCount, and the average length of CustomerName as AvgCustomerNameLength.\nFROM: Indicates the Customers table to query.\nGROUP BY: Groups rows by the Country column.\nHAVING: Filters groups to include only those with more than 2 customers.\nORDER BY: Sorts the results by AvgCustomerNameLength in descending order.\nFETCH FIRST: Limits the number of rows returned to the top 5."
  },
  {
    "objectID": "notes/w2.html#table-join",
    "href": "notes/w2.html#table-join",
    "title": "Lecture Note Week 2",
    "section": "Table Join",
    "text": "Table Join\nTable joins in SQL allow you to combine rows from two or more tables based on a related column between them. Joins are essential for querying data that is spread across multiple tables in a relational database. The HR schema from Oracle provides a good basis for understanding joins, as it contains several related tables such as EMPLOYEES, DEPARTMENTS, JOBS, JOB_HISTORY, LOCATIONS, COUNTRIES, and REGIONS.\n\nTypes of Joins\n\nINNER JOIN\nLEFT JOIN (LEFT OUTER JOIN)\nRIGHT JOIN (RIGHT OUTER JOIN)\nFULL OUTER JOIN\nCROSS JOIN\nSELF JOIN\n\n\n\n\n\nsource: Jitendra Kumar’s LinkedIn\n\n\n\n1. INNER JOIN\nAn INNER JOIN returns records that have matching values in both tables.\nSyntax:\nSELECT columns\nFROM table1\nINNER JOIN table2 ON table1.common_column = table2.common_column;`\nExample: Retrieve employees along with their department names.\nSELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nINNER JOIN departments d ON e.department_id = d.department_id;\nThis query joins the EMPLOYEES table with the DEPARTMENTS table on the department_id column, returning only the rows with matching department_id values.\n\n\n2. LEFT JOIN (LEFT OUTER JOIN)\nA LEFT JOIN returns all records from the left table (table1), and the matched records from the right table (table2). If no match is found, NULL values are returned for columns from the right table.\nSyntax:\nSELECT columns\nFROM table1\nLEFT JOIN table2 ON table1.common_column = table2.common_column;\nExample: Retrieve all departments and their employees, including departments without employees.\nSELECT d.department_name, e.first_name, e.last_name\nFROM departments d\nLEFT JOIN employees e ON d.department_id = e.department_id;\nThis query joins the DEPARTMENTS table with the EMPLOYEES table, including all departments even if they have no employees.\n\n\n3. RIGHT JOIN (RIGHT OUTER JOIN)\nA RIGHT JOIN returns all records from the right table (table2), and the matched records from the left table (table1). If no match is found, NULL values are returned for columns from the left table.\nSyntax:\nSELECT columns\nFROM table1\nRIGHT JOIN table2 ON table1.common_column = table2.common_column;\nExample: Retrieve all employees and their department names, including employees without a department.\nSELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nRIGHT JOIN departments d ON e.department_id = d.department_id;\nThis query joins the EMPLOYEES table with the DEPARTMENTS table, including all employees even if they don’t belong to a department.\n\n\n4. FULL OUTER JOIN\nA FULL OUTER JOIN returns all records when there is a match in either left (table1) or right (table2) table records. If there is no match, NULL values are returned for the non-matching side.\nSyntax:\nSELECT columns\nFROM table1\nFULL OUTER JOIN table2 ON table1.common_column = table2.common_column;\nExample: Retrieve all departments and their employees, including departments without employees and employees without departments.\nSELECT d.department_name, e.first_name, e.last_name\nFROM departments d\nFULL OUTER JOIN employees e ON d.department_id = e.department_id;\nThis query joins the DEPARTMENTS table with the EMPLOYEES table, including all departments and employees even if they don’t have matches.\n\n\n5. CROSS JOIN 👈 (not included in syllabus)\nA CROSS JOIN returns the Cartesian product of the two tables, meaning it returns all possible combinations of rows.\nSyntax:\nSELECT columns\nFROM table1\nCROSS JOIN table2;\nExample:\nRetrieve all possible combinations of employees and departments.\nSELECT e.first_name, e.last_name, d.department_name\nFROM employees e\nCROSS JOIN departments d;\nThis query combines each employee with every department, producing a large number of rows.\n\n\n6. SELF JOIN 👈 (not included in syllabus)\nA SELF JOIN is a regular join, but the table is joined with itself.\nSyntax:\nSELECT a.columns, b.columns\nFROM table a\nINNER JOIN table b ON a.common_column = b.common_column;\nExample: Retrieve employees and their managers.\nSELECT e.first_name AS Employee, m.first_name AS Manager\nFROM employees e\nINNER JOIN employees m ON e.manager_id = m.employee_id;\nThis query joins the EMPLOYEES table with itself to find each employee’s manager.\n\n\n\nPutting It All Together\nCombining multiple concepts into a single query using the HR schema:\nSELECT department_id, COUNT(employee_id) AS employee_count, AVG(salary) AS avg_salary \nFROM employees \nWHERE salary &gt; 3000 \nGROUP BY department_id \nHAVING COUNT(employee_id) &gt; 3 \nORDER BY avg_salary DESC \nLIMIT 5;\nThis query:\n\nSELECT: Retrieves department_id, the count of employee_id as employee_count, and the average salary as avg_salary.\nFROM: Queries the EMPLOYEES table.\nWHERE: Filters employees with a salary greater than 3000.\nGROUP BY: Groups the result by department_id.\nHAVING: Filters groups having more than 3 employees.\nORDER BY: Orders the result by avg_salary in descending order.\nFETCH FIRST: Limits the result to the first 5 rows\n\n🐹 Now you have learn the basic of SQL language, you might not realize that it actually follow a certain hierarchy pattern. 👇"
  },
  {
    "objectID": "weeks/week-1.html#quiz-own-time",
    "href": "weeks/week-1.html#quiz-own-time",
    "title": "Week 1",
    "section": "Quiz (own-time)",
    "text": "Quiz (own-time)\n🧪 Quiz 1",
    "crumbs": [
      "Weekly Schedule",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-2.html#quiz-own-time",
    "href": "weeks/week-2.html#quiz-own-time",
    "title": "Week 2",
    "section": "Quiz (own-time)",
    "text": "Quiz (own-time)\n🧪 Quiz 2",
    "crumbs": [
      "Weekly Schedule",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Make sure you can either have Power BI desktop installed or have access to Power BI via MOVE platform.\nMake sure you can run Rstudio in your computer.",
    "crumbs": [
      "Weekly Schedule",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#pre-class-activities-own-time",
    "href": "weeks/week-3.html#pre-class-activities-own-time",
    "title": "Week 3",
    "section": "",
    "text": "Make sure you can either have Power BI desktop installed or have access to Power BI via MOVE platform.\nMake sure you can run Rstudio in your computer.",
    "crumbs": [
      "Weekly Schedule",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#lecture-real-time",
    "href": "weeks/week-3.html#lecture-real-time",
    "title": "Week 3",
    "section": "Lecture (real-time)",
    "text": "Lecture (real-time)\n🖥️ Lecture 3 - Data Wrangling\n\n\n📋 Note - Data Wrangling using R and Power BI",
    "crumbs": [
      "Weekly Schedule",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#tutorial-real-time",
    "href": "weeks/week-3.html#tutorial-real-time",
    "title": "Week 3",
    "section": "Tutorial (real-time)",
    "text": "Tutorial (real-time)\n📋 Tutorial 3\n📑 Tutorial 3 Solution",
    "crumbs": [
      "Weekly Schedule",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#quiz-own-time",
    "href": "weeks/week-3.html#quiz-own-time",
    "title": "Week 3",
    "section": "Quiz (own-time)",
    "text": "Quiz (own-time)\n🧪 Quiz 3",
    "crumbs": [
      "Weekly Schedule",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#supplementary-own-time",
    "href": "weeks/week-3.html#supplementary-own-time",
    "title": "Week 3",
    "section": "Supplementary (own-time)",
    "text": "Supplementary (own-time)\n🔕 You do not have to complete these but if you wish you can.\n📚 Introduction to R\n💣 Get started with Power BI Desktop\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "notes/w3.html#section-1-introduction-to-power-bi",
    "href": "notes/w3.html#section-1-introduction-to-power-bi",
    "title": "Lecture Note Week 3",
    "section": "Section 1: Introduction to Power BI",
    "text": "Section 1: Introduction to Power BI\nWelcome to Power BI!\nPower BI is a powerful business analytics tool that enables you to visualize data, share insights, and make data-driven decisions. This introductory document will guide you through the basics of Power BI and help you get started with creating your first report.\n\n1. What is Power BI?\nPower BI is a suite of business analytics tools by Microsoft that includes software, services, and connectors. It transforms your unrelated sources of data into coherent, visually immersive, and interactive insights. Your data may be an Excel spreadsheet, or a collection of cloud-based and on-premises hybrid data warehouses.\n\n\nGetting Started\n\n1. Download and Install Power BI Desktop\n\nGo to the Power BI website and download the Power BI Desktop.\nFollow the installation instructions to install Power BI Desktop on your computer.\n\n\n\n2. Open Power BI Desktop\nOnce installed, open Power BI Desktop. You will see the following interface:\n\n\n\n3. Connect to Data\nTo start working with data, you need to connect Power BI to your data source. Here’s how:\n\nClick on the “Home” tab.\nClick on the “Get Data” button.\n\n\n\nSelect your data source from the list (e.g., Excel, SQL Server, Web, etc.).\nFollow the prompts to load your data into Power BI.\n\n\n\n4. Transform Data with Power Query Editor\nAfter loading your data, you might need to clean and transform it. Use Power Query Editor for this purpose:\n\nClick on the “Home” tab.\nClick on the “Transform Data” button.\n\n\nThe Power Query Editor will open, where you can clean and prepare your data.\n\n\n\n5. Create Visualizations\nOnce your data is ready, you can start creating visualizations:\n\nGo back to the main Power BI Desktop window.\nIn the “Visualizations” pane, select the type of visualization you want to create (e.g., bar chart, line chart, pie chart, etc.).\nDrag and drop fields from the “Fields” pane to the “Values”, “Axis”, “Legend”, etc., in the “Visualizations” pane.\n\n\n\n\n6. Build Reports\nCombine multiple visualizations into a report:\n\nUse the “Report” view in Power BI Desktop.\nArrange your visualizations on the canvas.\nAdd titles, labels, and other elements to enhance your report.\n\n\n\n7. Save and Publish Your Report 👈 (This feature will not be taught for this course as it needs a paid version)\n\nClick on the “File” tab.\nSelect “Save As” to save your report locally.\nTo share your report, click on the “Publish” button.\n\n\nFollow the prompts to publish your report to the Power BI service, where you can share it with others.\n\n\n\nConclusion\nCongratulations! You have successfully created your first Power BI report. Power BI is a versatile tool with many advanced features, so continue exploring and experimenting to unlock its full potential. Happy data analyzing!"
  },
  {
    "objectID": "notes/w3.html#section-2.-descriptive-statistics",
    "href": "notes/w3.html#section-2.-descriptive-statistics",
    "title": "Lecture Note Week 3",
    "section": "Section 2. Descriptive Statistics",
    "text": "Section 2. Descriptive Statistics\n\n2. Types of Data\n\nPopulation and Sample Data\n\nPopulation Data: Refers to the entire set of data or items of interest.\n\nExample: All employees in a company.\n\nSample Data: A subset of the population data.\n\nExample: Employees in one department.\n\n\n\n\nQuantitative and Categorical Data\n\nQuantitative (Numerical) Data: Data that represents quantities and can be measured.\n\nExample: Sales figures, ages.\n\nCategorical (Qualitative) Data: Data that represents categories or groups.\n\nExample: Product categories, regions.\n\n\n\n\nCross-Sectional and Time Series Data\n\nCross-Sectional Data: Data collected at a single point in time.\n\nExample: Survey results collected on a specific date.\n\nTime Series Data: Data collected over different time periods.\n\nExample: Monthly sales data.\n\n\n\n\nSorting and Filtering Data\n\nSorting Data:\n\nClick on the column header to sort data in ascending or descending order.\n\nFiltering Data:\n\nUse the filter pane to apply filters to your data.\n\n\n\n\n\n3. Frequency Distributions\n\nCategorical Data\n\nCreating Frequency Distributions:\n\nUse the “Visualizations” pane to create a bar chart.\nDrag the categorical field to the Axis and the Value fields.\n\n\n\n\nQuantitative Data\n\nCreating Frequency Distributions:\n\nUse the “Visualizations” pane to create a histogram.\nDrag the quantitative field to the Axis and the Value fields.\n\nRelative Frequency:\n\nCalculate the relative frequency by dividing the count of each category by the total count.\n\n\n\n\n\n4. Histograms and Cumulative Distributions\n\nHistograms\n\nWhat is a Histogram?\n\nA graphical representation of the distribution of numerical data.\nUsed to visualize the frequency of data points within certain ranges.\n\nCreating Histograms in Power BI:\n\nUse the “Visualizations” pane to create a histogram.\nDrag the quantitative field to the Axis and the Value fields.\n\n\n\n\nCumulative Distributions\n\nWhat is a Cumulative Distribution?\n\nA graph showing the cumulative frequency of data points up to a certain value.\n\nCreating Cumulative Distributions:\n\nUse line charts to visualize cumulative distributions.\nCalculate cumulative frequencies and plot them.\n\n\n\n\n\n5. Measures of Central Tendency\n\nMean\n\nDefinition:\n\nThe average of a set of numbers.\n\nCalculating Mean in Power BI:\n\nUse the “AVERAGE” function in a measure or calculated column.\n\n\n\n\nMedian\n\nDefinition:\n\nThe middle value of a data set when ordered.\n\nCalculating Median in Power BI:\n\nUse the “MEDIAN” function in a measure or calculated column.\n\n\n\n\nMode\n\nDefinition:\n\nThe most frequently occurring value in a data set.\n\nCalculating Mode in Power BI:\n\nUse the “MODE” function in a measure or calculated column.\n\n\n\n\n\n6. Measures of Variability\n\nRange\n\nDefinition:\n\nThe difference between the highest and lowest values in a data set.\n\nCalculating Range:\n\nSubtract the minimum value from the maximum value in a measure or calculated column.\n\n\n\n\nVariance and Standard Deviation\n\nVariance:\n\nMeasures how much the data points deviate from the mean.\n\nStandard Deviation:\n\nThe square root of the variance.\n\nCalculating Variance and Standard Deviation in Power BI:\n\nUse the “VAR” and “STDEV” functions in measures or calculated columns.\n\n\n\n\n\n7. Percentiles and Quartiles\n\nPercentiles\n\nDefinition:\n\nValues below which a certain percentage of data points fall.\n\nCalculating Percentiles in Power BI:\n\nUse the “PERCENTILE.INC” function in measures or calculated columns.\n\n\n\n\nQuartiles\n\nDefinition:\n\nDivides the data into four equal parts.\n\nCalculating Quartiles in Power BI:\n\nUse the “QUARTILE.INC” function in measures or calculated columns.\n\n\n\n\n\n8. Z-Scores and the Empirical Rule\n\nZ-Scores\n\nDefinition:\n\nMeasures how many standard deviations a data point is from the mean.\n\nCalculating Z-Scores in Power BI:\n\nUse the formula: Z=(X−Mean)Standard DeviationZ = Z=Standard Deviation(X−Mean)​ in a calculated column.\n\n\n\n\nEmpirical Rule\n\nDefinition:\n\nStates that for a normal distribution:\n\n68% of data falls within 1 standard deviation of the mean.\n95% of data falls within 2 standard deviations of the mean.\n99.7% of data falls within 3 standard deviations of the mean.\n\n\nApplication:\n\nVisualize data distribution and standard deviations in Power BI.\n\n\n\n\n\n9. Outliers and Boxplots\n\nOutliers\n\nDefinition:\n\nData points significantly different from other observations.\n\nIdentifying Outliers in Power BI:\n\nUse visualizations like scatter plots and boxplots.\n\n\n\n\nBoxplots\n\nWhat is a Boxplot?\n\nA graphical representation of data showing minimum, first quartile, median, third quartile, and maximum.\n\nCreating Boxplots in Power BI:\n\nUse custom visuals or R scripts to create boxplots.\n\n\n\n\n\nConclusion\n\nKey Points:\n\nReviewed Power BI interface and data types.\nExplained frequency distributions, histograms, and cumulative distributions.\nCovered measures of central tendency and variability.\nDiscussed percentiles, quartiles, z-scores, and the empirical rule.\nIdentified outliers and created boxplots."
  },
  {
    "objectID": "notes/w4.html",
    "href": "notes/w4.html",
    "title": "Lecture Note Week 4",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\n\nBe able to use visual principles such as preattentive attributes and the data-ink ratio to enhance chart design.\nBe able to create and use PivotTables and PivotCharts in Power BI to explore and analyze data.\nBe able to create and interpret scatter charts, line charts, bar charts, and column charts to examine relationships between variables.\nBe able to create and interpret advanced visualizations such as trendlines, sorted/clustered/stacked bar charts, bubble charts, scatter chart matrices, and table lenses.\nBe able to design and create effective data dashboards in Power BI for real-time data analysis and decision-making."
  },
  {
    "objectID": "notes/w4.html#section-1-introduction-to-power-bi",
    "href": "notes/w4.html#section-1-introduction-to-power-bi",
    "title": "Lecture Note 3",
    "section": "Section 1: Introduction to Power BI",
    "text": "Section 1: Introduction to Power BI\nWelcome to Power BI!\nPower BI is a powerful business analytics tool that enables you to visualize data, share insights, and make data-driven decisions. This introductory document will guide you through the basics of Power BI and help you get started with creating your first report.\n\nWhat is Power BI?\nPower BI is a suite of business analytics tools by Microsoft that includes software, services, and connectors. It transforms your unrelated sources of data into coherent, visually immersive, and interactive insights. Your data may be an Excel spreadsheet, or a collection of cloud-based and on-premises hybrid data warehouses.\n\n\nGetting Started\n\n1. Download and Install Power BI Desktop\n\nGo to the Power BI website and download the Power BI Desktop.\nFollow the installation instructions to install Power BI Desktop on your computer.\n\n\n\n2. Open Power BI Desktop\nOnce installed, open Power BI Desktop. You will see the following interface:\n\n\n\n3. Connect to Data\nTo start working with data, you need to connect Power BI to your data source. Here’s how:\n\nClick on the “Home” tab.\nClick on the “Get Data” button.\n\n\n\nSelect your data source from the list (e.g., Excel, SQL Server, Web, etc.).\nFollow the prompts to load your data into Power BI.\n\n\n\n4. Transform Data with Power Query Editor\nAfter loading your data, you might need to clean and transform it. Use Power Query Editor for this purpose:\n\nClick on the “Home” tab.\nClick on the “Transform Data” button.\n\n\nThe Power Query Editor will open, where you can clean and prepare your data.\n\n\n\n5. Create Visualizations\nOnce your data is ready, you can start creating visualizations:\n\nGo back to the main Power BI Desktop window.\nIn the “Visualizations” pane, select the type of visualization you want to create (e.g., bar chart, line chart, pie chart, etc.).\nDrag and drop fields from the “Fields” pane to the “Values”, “Axis”, “Legend”, etc., in the “Visualizations” pane.\n\n\n\n\n6. Build Reports\nCombine multiple visualizations into a report:\n\nUse the “Report” view in Power BI Desktop.\nArrange your visualizations on the canvas.\nAdd titles, labels, and other elements to enhance your report.\n\n\n\n7. Save and Publish Your Report\n\nClick on the “File” tab.\nSelect “Save As” to save your report locally.\nTo share your report, click on the “Publish” button.\n\n\nFollow the prompts to publish your report to the Power BI service, where you can share it with others.\n\n\n\nConclusion\nCongratulations! You have successfully created your first Power BI report. Power BI is a versatile tool with many advanced features, so continue exploring and experimenting to unlock its full potential. Happy data analyzing!"
  },
  {
    "objectID": "notes/w4.html#section-2-introduction-to-r-language",
    "href": "notes/w4.html#section-2-introduction-to-r-language",
    "title": "Lecture Note 3",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w12.html",
    "href": "notes/w12.html",
    "title": "Lecture Note Week 12",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Evaluate out-of-sample predictive performance of a model.\nLO2: Compute and interpret performance measures for classification methods.\nLO3: Describe and apply logistic regression.\nLO4: Describe and apply classification trees."
  },
  {
    "objectID": "notes/w12.html#lecture-notes-computing-and-interpreting-performance-measures-for-classification-methods",
    "href": "notes/w12.html#lecture-notes-computing-and-interpreting-performance-measures-for-classification-methods",
    "title": "Lecture Note Week 12",
    "section": "Lecture Notes: Computing and Interpreting Performance Measures for Classification Methods",
    "text": "Lecture Notes: Computing and Interpreting Performance Measures for Classification Methods\n\n1. Introduction\nEvaluating the performance of classification models is crucial for understanding their effectiveness in predicting categorical outcomes. This lecture note covers key performance measures, their computation, and interpretation for classification methods.\n\n\n2. Key Performance Measures\n\n2.1. Confusion Matrix\nA confusion matrix is a table that summarizes the performance of a classification model by comparing the actual and predicted classifications.\n\nTrue Positive (TP): Correctly predicted positive cases.\nTrue Negative (TN): Correctly predicted negative cases.\nFalse Positive (FP): Incorrectly predicted positive cases (Type I error).\nFalse Negative (FN): Incorrectly predicted negative cases (Type II error).\n\n\n\n2.2. Accuracy\nAccuracy measures the proportion of correct predictions (both true positives and true negatives) among the total number of cases.\nAccuracy=TP+TNTP+TN+FP+FN = Accuracy=TP+TN+FP+FNTP+TN​\nr\nCopy code\naccuracy &lt;- (TP + TN) / (TP + TN + FP + FN)\n\n\n2.3. Precision\nPrecision (also called Positive Predictive Value) measures the proportion of true positive predictions among all positive predictions.\nPrecision=TPTP+FP = Precision=TP+FPTP​\nr\nCopy code\nprecision &lt;- TP / (TP + FP)\n\n\n2.4. Recall\nRecall (also called Sensitivity or True Positive Rate) measures the proportion of true positive cases among all actual positive cases.\nRecall=TPTP+FN = Recall=TP+FNTP​\nr\nCopy code\nrecall &lt;- TP / (TP + FN)\n\n\n2.5. F1 Score\nThe F1 Score is the harmonic mean of precision and recall, providing a balance between the two.\nF1 Score=2⋅Precision⋅RecallPrecision+Recall = 2 F1 Score=2⋅Precision+RecallPrecision⋅Recall​\nr\nCopy code\nf1_score &lt;- 2 * (precision * recall) / (precision + recall)\n\n\n2.6. Specificity\nSpecificity (also called True Negative Rate) measures the proportion of true negative cases among all actual negative cases.\nSpecificity=TNTN+FP = Specificity=TN+FPTN​\nr\nCopy code\nspecificity &lt;- TN / (TN + FP)\n\n\n\n3. Implementation in R\nAssuming you have a binary classification model and a dataset with actual and predicted values, here’s how to compute these performance measures in R.\n\nGenerate Predictions:\nr\nCopy code\n# Example using a logistic regression model model &lt;- glm(y ~ ., data = train, family = binomial) predictions &lt;- predict(model, newdata = test, type = \"response\") predicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nCreate a Confusion Matrix:\nr\nCopy code\nlibrary(caret) confusion_matrix &lt;- confusionMatrix(as.factor(predicted_classes), as.factor(test$y))\nExtract Performance Measures:\nr\nCopy code\n`confusion &lt;- confusion_matrix$table TP &lt;- confusion[2, 2] TN &lt;- confusion[1, 1] FP &lt;- confusion[1, 2] FN &lt;- confusion[2, 1]\naccuracy &lt;- (TP + TN) / (TP + TN + FP + FN) precision &lt;- TP / (TP + FP) recall &lt;- TP / (TP + FN) f1_score &lt;- 2 * (precision * recall) / (precision + recall) specificity &lt;- TN / (TN + FP)`\n\n\n\n4. Interpretation\n\nAccuracy: High accuracy indicates that the model correctly classifies most cases. However, it may be misleading if the classes are imbalanced.\nPrecision: High precision means that when the model predicts a positive case, it is usually correct. It is important when the cost of false positives is high.\nRecall: High recall means that the model can identify most actual positive cases. It is important when the cost of false negatives is high.\nF1 Score: A balanced measure that is useful when you need to balance precision and recall.\nSpecificity: High specificity means that the model can identify most actual negative cases. It is important when the cost of false positives is high.\n\n\n\n5. Conclusion\nUnderstanding and correctly interpreting performance measures are essential for evaluating the effectiveness of classification models. Each measure provides unique insights, and the choice of which to prioritize depends on the specific context and costs associated with false positives and false negatives."
  },
  {
    "objectID": "notes/w12.html#logistic-regression",
    "href": "notes/w12.html#logistic-regression",
    "title": "Lecture Note Week 12",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n1. Introduction\nLogistic regression is a powerful statistical method used for binary classification problems. It is widely applied to scenarios where the outcome variable is binary (e.g., yes/no, success/failure). This lecture note will describe the logistic regression model and demonstrate its application using examples inspired by the “Business Analytics” book by Jeffrey D. Camm and others.\n\n\n2. Logistic Regression Model\n\n2.1. Concept\nLogistic regression models the probability of a binary outcome using a logistic function. Unlike linear regression, which predicts continuous outcomes, logistic regression predicts the probability that a given input point belongs to a particular class.\nThe logistic function (also known as the sigmoid function) is defined as:\nP(Y=1∣X)=11+e−(β0+β1X1+β2X2+…+βkXk)P(Y=1|X) = P(Y=1∣X)=1+e−(β0​+β1​X1​+β2​X2​+…+βk​Xk​)1​\nWhere:\n\nP(Y=1∣X)P(Y=1|X)P(Y=1∣X) is the probability that the outcome YYY is 1 given the input XXX.\nβ0,β1,…,βk_0, _1, , _kβ0​,β1​,…,βk​ are the coefficients of the model.\n\n\n\n2.2. Odds and Logit\n\nOdds: The odds of an event are the ratio of the probability of the event occurring to the probability of it not occurring.\n\nOdds=P(Y=1∣X)1−P(Y=1∣X) = Odds=1−P(Y=1∣X)P(Y=1∣X)​\n\nLogit: The logit function is the natural logarithm of the odds.\n\nLogit(P)=log⁡(P1−P)=β0+β1X1+β2X2+…+βkXk(P) = () = _0 + _1X_1 + _2X_2 + + _kX_kLogit(P)=log(1−PP​)=β0​+β1​X1​+β2​X2​+…+βk​Xk​\n\n\n\n3. Applying Logistic Regression in R\n\n3.1. Data Preparation\n\nLoad the Data:\nr\nCopy code\n# Example dataset data &lt;- read.csv(\"path_to_dataset.csv\")\nSplit the Data into Training and Testing Sets:\nr\nCopy code\nset.seed(123) sample &lt;- sample.int(n = nrow(data), size = floor(0.8*nrow(data)), replace = FALSE) train &lt;- data[sample, ] test &lt;- data[-sample, ]\n\n\n\n3.2. Model Training\n\nFit the Logistic Regression Model:\nr\nCopy code\nmodel &lt;- glm(binary_outcome ~ ., data = train, family = binomial)\nView Model Summary:\nr\nCopy code\nsummary(model)\n\n\n\n3.3. Model Evaluation\n\nMake Predictions:\nr\nCopy code\npredictions &lt;- predict(model, newdata = test, type = \"response\") predicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nCreate a Confusion Matrix:\nr\nCopy code\nlibrary(caret) confusion_matrix &lt;- confusionMatrix(as.factor(predicted_classes), as.factor(test$binary_outcome)) print(confusion_matrix)\nCalculate Performance Metrics:\nr\nCopy code\n`confusion &lt;- confusion_matrix$table TP &lt;- confusion[2, 2] TN &lt;- confusion[1, 1] FP &lt;- confusion[1, 2] FN &lt;- confusion[2, 1]\naccuracy &lt;- (TP + TN) / (TP + TN + FP + FN) precision &lt;- TP / (TP + FP) recall &lt;- TP / (TP + FN) f1_score &lt;- 2 * (precision * recall) / (precision + recall) specificity &lt;- TN / (TN + FP)\ncat(“Accuracy:”, accuracy, “”) cat(“Precision:”, precision, “”) cat(“Recall:”, recall, “”) cat(“F1 Score:”, f1_score, “”) cat(“Specificity:”, specificity, “”)`\n\n\n\n\n4. Interpretation\n\nCoefficients: The coefficients (β values) in the logistic regression model indicate the change in the log odds of the outcome for a one-unit increase in the predictor variable.\nOdds Ratio: The exponentiated coefficients (e^β) represent the odds ratio, showing how the odds of the outcome change with a one-unit increase in the predictor.\nPerformance Metrics: The confusion matrix and derived metrics (accuracy, precision, recall, F1 score, specificity) help evaluate the model’s performance on the test data.\n\n\n5.1. Example Scenario: Predicting Customer Churn\n\nDataset: Customer data with features like age, account length, service usage, etc.\nBinary Outcome: Whether the customer churned (1) or not (0).\n\nr\nCopy code\n`# Example code based on a hypothetical dataset data &lt;- read.csv(“customer_churn.csv”) set.seed(123) sample &lt;- sample.int(n = nrow(data), size = floor(0.8*nrow(data)), replace = FALSE) train &lt;- data[sample, ] test &lt;- data[-sample, ]"
  },
  {
    "objectID": "notes/w7.html",
    "href": "notes/w7.html",
    "title": "Lecture Note Week 7",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Take random samples from population.\nLO2: Calculate and interpret point estimates and confidence intervals for population parameters.\nLO3: Calculate and interpret the margin of error for a confidence interval.\nLO4: Identify the sampling distributions for sample means and sample proportions.\nLO5: Formulate and interpret hypotheses for population parameters.\nLO6: Identify Type I and Type II errors."
  },
  {
    "objectID": "notes/w7.html#section-2-introduction-to-r-language",
    "href": "notes/w7.html#section-2-introduction-to-r-language",
    "title": "Lecture Note Week 7",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w8.html",
    "href": "notes/w8.html",
    "title": "Lecture Note Weeek 8",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Use and interpret principal component analysis (PCA)\nLO2: Describe and apply distance measures for observations consisting of quantitative variables.\nLO3: Describe and apply distance measures for observations consisting of qualitative variables.\nLO4: Construct clusters with k-means clustering.\nLO5: Construct clusters with hierarchical clustering."
  },
  {
    "objectID": "notes/w8.html#section-2-introduction-to-r-language",
    "href": "notes/w8.html#section-2-introduction-to-r-language",
    "title": "Lecture Note Weeek 8",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w11.html",
    "href": "notes/w11.html",
    "title": "Lecture Note Week 11",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Evaluate out-of-sample predictive performance of a model.\nLO2: Compute and interpret performance measures for regression methods.\nLO3: Describe and apply regression trees\nLO4: Describe various ways of selecting features for a regression task."
  },
  {
    "objectID": "notes/w11.html#section-2-introduction-to-r-language",
    "href": "notes/w11.html#section-2-introduction-to-r-language",
    "title": "Lecture Note 3",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w10.html",
    "href": "notes/w10.html",
    "title": "Lecture Note Week 11",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Evaluate out-of-sample predictive performance of a model.\nLO2: Compute and interpret performance measures for regression methods.\nLO3: Describe and apply regression trees\nLO4: Describe various ways of selecting features for a regression task."
  },
  {
    "objectID": "notes/w10.html#section-2-introduction-to-r-language",
    "href": "notes/w10.html#section-2-introduction-to-r-language",
    "title": "Lecture Note 3",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w9.html",
    "href": "notes/w9.html",
    "title": "Lecture Note 3",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Estimate simple linear regression model and multiple linear regression model.\nLO2: Assessing the fit of the model.\nLO3: Compute and interpret the coefficients of the model.\nLO4: Use dummy variables to incorporate categorical variables in the model.\nLO5: Use quadratic and interaction terms in the model.\nLO4: Model inference."
  },
  {
    "objectID": "notes/w9.html#section-2-introduction-to-r-language",
    "href": "notes/w9.html#section-2-introduction-to-r-language",
    "title": "Lecture Note 3",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w6.html",
    "href": "notes/w6.html",
    "title": "Lecture Note Week 6",
    "section": "",
    "text": "Note\n\n\n\nLearning Objective:\nLO1: Understand events and probabilities\nLO2: Understand the concept of random variables and probability distributions\nLO3: Understand the concept of sampling distributions\nLO4: Understand and able to identify discrete probability distributions\nLO5: Understand and able to identify continuous probability distributions"
  },
  {
    "objectID": "notes/w6.html#section-2-introduction-to-r-language",
    "href": "notes/w6.html#section-2-introduction-to-r-language",
    "title": "Lecture Note Week 6",
    "section": "Section 2: Introduction to R Language",
    "text": "Section 2: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w5.html#section-1-introduction-to-r-language",
    "href": "notes/w5.html#section-1-introduction-to-r-language",
    "title": "Lecture Note Week 5",
    "section": "Section 1: Introduction to R Language",
    "text": "Section 1: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w5.html#section-2-modeling-uncertainty",
    "href": "notes/w5.html#section-2-modeling-uncertainty",
    "title": "Lecture Note Week 5",
    "section": "Section 2: Modeling Uncertainty",
    "text": "Section 2: Modeling Uncertainty\nUncertainty is a fundamental concept in many fields, including statistics, economics, engineering, and everyday decision-making. It refers to the state of having limited knowledge about an event, outcome, or condition. This lack of certainty means that we cannot predict the outcome with complete confidence.\nProbability and Uncertainty:\nProbability is a mathematical framework used to quantify uncertainty. It provides a way to measure how likely an event is to occur. Here are some key concepts:\n\nProbability of an Event: The probability of an event AAA, denoted as P(A)P(A)P(A), ranges from 0 (the event will not occur) to 1 (the event will certainly occur). For example, the probability of flipping a fair coin and getting heads is P(Heads)=0.5P() = 0.5P(Heads)=0.5.\nRandom Variables: A random variable is a variable whose possible values are numerical outcomes of a random phenomenon. For example, the number of heads in 10 coin tosses is a random variable.\nProbability Distributions: A probability distribution describes how the probabilities are distributed over the values of the random variable. Common distributions include the binomial distribution (for binary outcomes), the normal distribution (for continuous outcomes), and the Poisson distribution (for count data).\n\n\n5.1 Random Experiment and Sample Space\n\nDefinition of a Random Experiment\nA random experiment is a process or procedure that leads to one of several possible outcomes, where the outcome cannot be predicted with certainty beforehand. Each performance of the experiment is independent and can yield any of the possible outcomes.\n\n\nCharacteristics of a Random Experiment\n\nUncertainty: The outcome of the experiment cannot be determined with certainty before it is conducted.\nRepeatability: The experiment can be repeated under identical conditions.\nWell-defined Outcomes: All possible outcomes are known and can be listed.\n\n\n\nSample Space\nThe sample space, denoted by SSS, of a random experiment is the set of all possible outcomes of that experiment. Each possible outcome in the sample space is known as a sample point or an elementary event.\n\n\nExamples of Random Experiments and Sample Spaces\nExample 1: Tossing a Coin\n\nRandom Experiment: Tossing a coin.\nPossible Outcomes (Sample Points): The coin can land on either Head (H) or Tail (T).\nSample Space: The sample space for this experiment is: S={Head,Tail}S = { , }S={Head,Tail}\n\nExample 2: Rolling a Die\n\nRandom Experiment: Rolling a six-sided die.\nPossible Outcomes (Sample Points): The die can land on any one of the six faces, showing 1, 2, 3, 4, 5, or 6.\nSample Space: The sample space for this experiment is: S={1,2,3,4,5,6}S = { 1, 2, 3, 4, 5, 6 }S={1,2,3,4,5,6}\n\n\n\nFurther Examples and Concepts\nExample 3: Drawing a Card from a Deck\n\nRandom Experiment: Drawing a single card from a standard 52-card deck.\nPossible Outcomes (Sample Points): Each card in the deck is a distinct outcome, such as Ace of Spades, 2 of Hearts, etc.\nSample Space: The sample space for this experiment is: S={Ace of Spades,2 of Hearts,…,King of Clubs}S = { , , , }S={Ace of Spades,2 of Hearts,…,King of Clubs} Here, SSS contains 52 elements, representing each card in the deck.\n\nExample 4: Flipping Two Coins\n\nRandom Experiment: Flipping two coins simultaneously.\nPossible Outcomes (Sample Points): The outcomes can be both heads, both tails, head on the first coin and tail on the second, or tail on the first coin and head on the second.\nSample Space: The sample space for this experiment is: \\(S={(Head, Head),(Head, Tail),(Tail, Head),(Tail, Tail)}S = \\{ (\\text{Head, Head}), (\\text{Head, Tail}), (\\text{Tail, Head}), (\\text{Tail, Tail}) \\}\\)={(Head, Head),(Head, Tail),(Tail, Head),(Tail, Tail)}\n\nExample 5: Rolling Two Dice\n\nRandom Experiment: Rolling two six-sided dice simultaneously.\nPossible Outcomes (Sample Points): Each die can show a number from 1 to 6, resulting in pairs such as (1,1), (1,2), …, (6,6).\nSample Space: The sample space for this experiment is: S={(1,1),(1,2),…,(6,6)}S = { (1,1), (1,2), , (6,6) }S={(1,1),(1,2),…,(6,6)} Here, SSS contains 36 elements, representing each possible pair of outcomes."
  },
  {
    "objectID": "notes/w11.html#introduction",
    "href": "notes/w11.html#introduction",
    "title": "Lecture Note Week 11",
    "section": "Introduction",
    "text": "Introduction\nPredictive data mining, also known as supervised learning, is a powerful set of techniques used to forecast future outcomes based on historical data. In supervised learning, the process involves utilizing known values of an outcome variable to guide the learning algorithm in identifying patterns and relationships with the input features. By doing so, predictive models are developed that can make accurate predictions on new, unseen data.\nThe predictive data mining process includes several critical steps:\n\nData Sampling: Selecting a representative subset of data to ensure the models can generalize well to new data.\nData Preparation: Cleaning and transforming the data to address missing values, errors, and refining the set of variables.\nData Partitioning: Dividing the dataset into training, validation, and test sets to evaluate model performance.\nModel Construction and Assessment: Building and assessing various predictive models to select the best one based on performance metrics.\n\nBy following these steps, predictive data mining aims to create robust models that can provide valuable insights and predictions, driving informed decision-making across various fields and applications."
  },
  {
    "objectID": "notes/w11.html#data-sampling",
    "href": "notes/w11.html#data-sampling",
    "title": "Lecture Note Week 11",
    "section": "Data Sampling",
    "text": "Data Sampling\nData sampling involves selecting a representative subset of the data from a dataset. This step is crucial because it ensures that the models trained later can generalize well to new, unseen data. Sampling should be done carefully to maintain the statistical properties of the original dataset.\n\n1. Random Sampling\nDefinition: Selecting a subset randomly from the entire dataset. Each data point has an equal chance of being included in the sample.\nExample: Suppose you have a dataset of 10,000 customer records from a retail store. To create a sample, you randomly select 1,000 records. This random selection ensures that the sample is representative of the entire customer base, avoiding any bias.\nRandom sampling is simple and effective for large datasets. It’s commonly used because it maintains the statistical properties of the original dataset. However, if there are important subgroups within the data, random sampling might not capture them proportionally.\n\n\n2. Stratified Sampling\nDefinition: Ensuring that the sample represents different subgroups (strata) within the population. Strata are defined based on one or more attributes of the data.\nExample: Consider the same dataset of 10,000 customer records, but this time the data includes a categorical variable indicating customer segments (e.g., Regular, Premium, and VIP). If you want to ensure each segment is proportionately represented in your sample, you use stratified sampling. If Regular customers make up 70% of the total, Premium 20%, and VIP 10%, your sample of 1,000 should contain 700 Regular, 200 Premium, and 100 VIP customers.\nStratified sampling is particularly useful when certain subgroups are of interest, or when there is a risk that these groups might be underrepresented in a random sample. This technique ensures that the sample accurately reflects the population’s diversity.\n\n\n3. Systematic Sampling\nDefinition: Selecting every \\(n^{th}\\) item from a list or sequence. The starting point is chosen randomly, and then every \\(n^{th}\\) item is included in the sample.\nExample: Imagine you have a dataset sorted by customer ID. To perform systematic sampling on a dataset of 10,000 customers to get a sample of 1,000, you could randomly choose a starting point between 1 and 10 (say you pick 5). You would then select every 10th customer (5, 15, 25, 35, etc.) until you reach 1,000 customers.\nSystematic sampling is straightforward and easy to implement, especially with ordered data. It’s efficient when dealing with large datasets. However, if there is a hidden pattern in the data that corresponds to the sampling interval, it could introduce bias. For instance, if every \\(10^{th}\\) customer tends to be from a particular region due to how the data was collected, this could skew the sample.\n\n\nPractical Considerations\n\nRandom Sampling: Best for simplicity and general representativeness.\nStratified Sampling: Ideal when subgroups are crucial to the analysis and need proportional representation.\nSystematic Sampling: Useful for ordered data and when quick, evenly spaced samples are needed."
  },
  {
    "objectID": "notes/w11.html#data-preparation",
    "href": "notes/w11.html#data-preparation",
    "title": "Lecture Note Week 11",
    "section": "Data preparation",
    "text": "Data preparation\nThis is a critical step in the data mining process that involves cleaning and transforming raw data into a format suitable for modeling. This stage addresses various issues such as missing data, errors, and outliers, and refines the set of variables (features) to enhance the performance of the model.\n\n1. Handling Missing Data\nHandling missing data involves imputing or removing missing values to ensure the dataset is complete and suitable for analysis.\nMethods and Examples:\n\nMean/Mode Imputation: Replace missing numerical values with the mean or categorical values with the mode of the available data.\n\nExample: If a dataset of house prices has missing values for the number of rooms, replace those missing values with the mean number of rooms from the dataset.\n\nMedian Imputation: Replace missing values with the median value, which is useful when the data has outliers.\n\nExample: For a dataset with highly skewed income data, using the median income to replace missing values can be more robust than the mean.\n\nK-Nearest Neighbors (KNN) Imputation: Use the average value of the k-nearest neighbors to impute missing data.\n\nExample: In a dataset of customer demographics, if the age is missing, find the k-nearest neighbors (e.g., 5 neighbors) based on other features (like income, education) and use their average age to fill in the missing value.\n\nInterpolation: Estimate missing values using linear or polynomial interpolation based on adjacent data points.\n\nExample: In a time series dataset of temperature readings, use linear interpolation to estimate missing temperature values.\n\n\n\n\n2. Data Clearning\nData cleaning involves identifying and correcting errors, inconsistencies, and outliers in the data to ensure its integrity.\nMethods and Examples:\n\nOutlier Detection and Treatment: Identify and handle outliers using statistical methods or domain knowledge.\n\nExample: In a dataset of employee salaries, identify outliers by calculating the z-scores and removing or capping extreme values that fall beyond three standard deviations from the mean.\n\nConsistency Checks: Ensure that data entries are consistent across different records.\n\nExample: In a customer database, ensure that phone numbers are consistently formatted (e.g., all in the format +1-XXX-XXX-XXXX).\n\nDuplicate Removal: Identify and remove duplicate records.\n\nExample: In an e-commerce transaction dataset, check for and remove duplicate transactions to avoid double-counting sales.\n\n\n\n\n3. Feature Engineering\nFeature engineering involves creating new features from existing data to improve model performance.\nMethods and Examples:\n\nInteraction Terms: Create features that capture interactions between existing variables.\n\nExample: In a housing dataset, create an interaction term between the number of rooms and the size of the garden to capture their combined effect on house price.\n\nPolynomial Features: Generate polynomial terms from existing features.\n\nExample: For a dataset predicting car prices, create a squared term for mileage (mileage^2) to capture the non-linear relationship between mileage and price.\n\nBinning: Transform continuous variables into categorical variables by grouping values into bins.\n\nExample: Convert age into age groups (e.g., 0-18, 19-35, 36-50, 51+) to simplify the model.\n\n\n\n\n4. Normalization/Scaling\nNormalization and scaling involve transforming features to a similar scale, which is crucial for algorithms sensitive to the scale of input data.\nMethods and Examples:\n\nStandardization (Z-score Normalization): Transform features to have a mean of 0 and a standard deviation of 1.\n\nExample: Standardize height and weight data in a health dataset to ensure they have the same scale when used in a k-nearest neighbors algorithm.\nFormula: \\(z=\\frac{x - \\bar{x}}{SD_{x}}\\)\n\nRobust Scaling: Scale features using the median and the interquartile range (IQR) to reduce the impact of outliers.\n\nExample: For a dataset with income data that has outliers, use robust scaling to transform the data by subtracting the median and dividing by the interquartile range (IQR).\nFormula: \\(z=\\frac{x - Median_{x}}{IQR_{x}}\\)"
  },
  {
    "objectID": "notes/w11.html#step-1-data-sampling",
    "href": "notes/w11.html#step-1-data-sampling",
    "title": "Lecture Note Week 11",
    "section": "Step 1: Data Sampling",
    "text": "Step 1: Data Sampling\nData sampling involves selecting a representative subset of the data from a dataset. This step is crucial because it ensures that the models trained later can generalize well to new, unseen data. Sampling should be done carefully to maintain the statistical properties of the original dataset.\n\n1. Random Sampling\nDefinition: Selecting a subset randomly from the entire dataset. Each data point has an equal chance of being included in the sample.\nExample: Suppose you have a dataset of 10,000 customer records from a retail store. To create a sample, you randomly select 1,000 records. This random selection ensures that the sample is representative of the entire customer base, avoiding any bias.\nRandom sampling is simple and effective for large datasets. It’s commonly used because it maintains the statistical properties of the original dataset. However, if there are important subgroups within the data, random sampling might not capture them proportionally.\n\n\n2. Stratified Sampling\nDefinition: Ensuring that the sample represents different subgroups (strata) within the population. Strata are defined based on one or more attributes of the data.\nExample: Consider the same dataset of 10,000 customer records, but this time the data includes a categorical variable indicating customer segments (e.g., Regular, Premium, and VIP). If you want to ensure each segment is proportionately represented in your sample, you use stratified sampling. If Regular customers make up 70% of the total, Premium 20%, and VIP 10%, your sample of 1,000 should contain 700 Regular, 200 Premium, and 100 VIP customers.\nStratified sampling is particularly useful when certain subgroups are of interest, or when there is a risk that these groups might be underrepresented in a random sample. This technique ensures that the sample accurately reflects the population’s diversity.\n\n\n3. Systematic Sampling\nDefinition: Selecting every \\(n^{th}\\) item from a list or sequence. The starting point is chosen randomly, and then every \\(n^{th}\\) item is included in the sample.\nExample: Imagine you have a dataset sorted by customer ID. To perform systematic sampling on a dataset of 10,000 customers to get a sample of 1,000, you could randomly choose a starting point between 1 and 10 (say you pick 5). You would then select every 10th customer (5, 15, 25, 35, etc.) until you reach 1,000 customers.\nSystematic sampling is straightforward and easy to implement, especially with ordered data. It’s efficient when dealing with large datasets. However, if there is a hidden pattern in the data that corresponds to the sampling interval, it could introduce bias. For instance, if every \\(10^{th}\\) customer tends to be from a particular region due to how the data was collected, this could skew the sample.\n\n\nPractical Considerations\n\nRandom Sampling: Best for simplicity and general representativeness.\nStratified Sampling: Ideal when subgroups are crucial to the analysis and need proportional representation.\nSystematic Sampling: Useful for ordered data and when quick, evenly spaced samples are needed."
  },
  {
    "objectID": "notes/w11.html#step-2-data-preparation",
    "href": "notes/w11.html#step-2-data-preparation",
    "title": "Lecture Note Week 11",
    "section": "Step 2: Data preparation",
    "text": "Step 2: Data preparation\nThis is a critical step in the data mining process that involves cleaning and transforming raw data into a format suitable for modeling. This stage addresses various issues such as missing data, errors, and outliers, and refines the set of variables (features) to enhance the performance of the model.\n\n1. Handling Missing Data\nHandling missing data involves imputing or removing missing values to ensure the dataset is complete and suitable for analysis.\nMethods and Examples:\n\nMean/Mode Imputation: Replace missing numerical values with the mean or categorical values with the mode of the available data.\n\nExample: If a dataset of house prices has missing values for the number of rooms, replace those missing values with the mean number of rooms from the dataset.\n\nMedian Imputation: Replace missing values with the median value, which is useful when the data has outliers.\n\nExample: For a dataset with highly skewed income data, using the median income to replace missing values can be more robust than the mean.\n\nK-Nearest Neighbors (KNN) Imputation: Use the average value of the k-nearest neighbors to impute missing data.\n\nExample: In a dataset of customer demographics, if the age is missing, find the k-nearest neighbors (e.g., 5 neighbors) based on other features (like income, education) and use their average age to fill in the missing value.\n\nInterpolation: Estimate missing values using linear or polynomial interpolation based on adjacent data points.\n\nExample: In a time series dataset of temperature readings, use linear interpolation to estimate missing temperature values.\n\n\n\n\n2. Data Clearning\nData cleaning involves identifying and correcting errors, inconsistencies, and outliers in the data to ensure its integrity.\nMethods and Examples:\n\nOutlier Detection and Treatment: Identify and handle outliers using statistical methods or domain knowledge.\n\nExample: In a dataset of employee salaries, identify outliers by calculating the z-scores and removing or capping extreme values that fall beyond three standard deviations from the mean.\n\nConsistency Checks: Ensure that data entries are consistent across different records.\n\nExample: In a customer database, ensure that phone numbers are consistently formatted (e.g., all in the format +1-XXX-XXX-XXXX).\n\nDuplicate Removal: Identify and remove duplicate records.\n\nExample: In an e-commerce transaction dataset, check for and remove duplicate transactions to avoid double-counting sales.\n\n\n\n\n3. Feature Engineering\nFeature engineering involves creating new features from existing data to improve model performance.\nMethods and Examples:\n\nInteraction Terms: Create features that capture interactions between existing variables.\n\nExample: In a housing dataset, create an interaction term between the number of rooms and the size of the garden to capture their combined effect on house price.\n\nPolynomial Features: Generate polynomial terms from existing features.\n\nExample: For a dataset predicting car prices, create a squared term for mileage (mileage^2) to capture the non-linear relationship between mileage and price.\n\nBinning: Transform continuous variables into categorical variables by grouping values into bins.\n\nExample: Convert age into age groups (e.g., 0-18, 19-35, 36-50, 51+) to simplify the model.\n\n\n\n\n4. Normalization/Scaling\nNormalization and scaling involve transforming features to a similar scale, which is crucial for algorithms sensitive to the scale of input data.\nMethods and Examples:\n\nStandardization (Z-score Normalization): Transform features to have a mean of 0 and a standard deviation of 1.\n\nExample: Standardize height and weight data in a health dataset to ensure they have the same scale when used in a k-nearest neighbors algorithm.\nFormula: \\(z=\\frac{x - \\bar{x}}{SD_{x}}\\)\n\nRobust Scaling: Scale features using the median and the interquartile range (IQR) to reduce the impact of outliers.\n\nExample: For a dataset with income data that has outliers, use robust scaling to transform the data by subtracting the median and dividing by the interquartile range (IQR).\nFormula: \\(z=\\frac{x - Median_{x}}{IQR_{x}}\\)\n\n\n\n\nStep 3. Data Partitioning\nData partitioning divides the dataset into distinct subsets used for training, validating, and testing the model. This ensures that the model’s performance is evaluated on data that it has not seen during training, which helps in assessing its generalizability.\n\nHoldout method\n\nTraining set (Construction):\n\nDuring this phase, candidate models were built by training them on available data.\nThe model learns from the training data to recognize patterns and make predictions.\nTypically about 60% of the data.\n\nValidation set (Performance Comparison):\n\nIn this step, the performance of different candidate models were compared.\nThe validation set helps fine-tune hyperparameters and assess how well each model generalizes to unseen data.\nTypically comprises 20-30% of the data.\n\nTest set (Future Performance Assessment):\n\nOnce the model is selected, have to evaluate its performance on a separate testing dataset.\nThis assessment provides insights into how the model will perform in real-world scenarios.\nTypically comprises 10-20% of the data.\n\n\n\n\nk-Fold cross validation\nStep-by-step guide:\n\nDataset Partitioning:\n\nThe dataset is randomly divided into k subsets, called folds.\nCommon choices for k are 5 or 10.\n\nPre-Processing Step:\n\nIf an unbiased estimate of the final model is desired, set aside a specific number of observations to create a test set.\n\nTraining and Validation:\n\nIn each iteration, k-1 folds are combined to form the training set.\nThe remaining fold is used as the validation set.\n\nIteration Process:\n\nThis process is repeated k times, each time with a different fold as the validation set.\n\nResult Aggregation:\n\nThe results from the k iterations are aggregated and averaged to provide an overall performance estimate.\n\n\n\n\nLeave-One-Out Cross-Validation (LOOCV)\n\nA special case of k-fold cross-validation.\nThe number of folds equals the number of observations.\nEach iteration uses a single observation as the validation set and the rest as the training set."
  },
  {
    "objectID": "notes/w11.html#introduction-to-predictive-data-mining",
    "href": "notes/w11.html#introduction-to-predictive-data-mining",
    "title": "Lecture Note Week 11",
    "section": "Introduction to Predictive Data Mining",
    "text": "Introduction to Predictive Data Mining\nPredictive data mining, also known as supervised learning, is a powerful set of techniques used to forecast future outcomes based on historical data. In supervised learning, the process involves utilizing known values of an outcome variable to guide the learning algorithm in identifying patterns and relationships with the input features. By doing so, predictive models are developed that can make accurate predictions on new, unseen data.\nThe predictive data mining process includes several critical steps:\n\nData Sampling: Selecting a representative subset of data to ensure the models can generalize well to new data.\nData Preparation: Cleaning and transforming the data to address missing values, errors, and refining the set of variables.\nData Partitioning: Dividing the dataset into training, validation, and test sets to evaluate model performance.\nModel Construction and Assessment: Building and assessing various predictive models to select the best one based on performance metrics.\n\nBy following these steps, predictive data mining aims to create robust models that can provide valuable insights and predictions, driving informed decision-making across various fields and applications."
  },
  {
    "objectID": "notes/w11.html#section-1-introduction-to-predictive-data-mining",
    "href": "notes/w11.html#section-1-introduction-to-predictive-data-mining",
    "title": "Lecture Note Week 11",
    "section": "Section 1: Introduction to Predictive Data Mining",
    "text": "Section 1: Introduction to Predictive Data Mining\nPredictive data mining, also known as supervised learning, is a powerful set of techniques used to forecast future outcomes based on historical data. In supervised learning, the process involves utilizing known values of an outcome variable to guide the learning algorithm in identifying patterns and relationships with the input features. By doing so, predictive models are developed that can make accurate predictions on new, unseen data.\nThe predictive data mining process includes several critical steps:\n\nData Sampling: Selecting a representative subset of data to ensure the models can generalize well to new data.\nData Preparation: Cleaning and transforming the data to address missing values, errors, and refining the set of variables.\nData Partitioning: Dividing the dataset into training, validation, and test sets to evaluate model performance.\nModel Construction and Assessment: Building and assessing various predictive models to select the best one based on performance metrics.\n\nBy following these steps, predictive data mining aims to create robust models that can provide valuable insights and predictions, driving informed decision-making across various fields and applications.\n\nStep 1: Data Sampling\nData sampling involves selecting a representative subset of the data from a dataset. This step is crucial because it ensures that the models trained later can generalize well to new, unseen data. Sampling should be done carefully to maintain the statistical properties of the original dataset.\n\n1. Random Sampling\nDefinition: Selecting a subset randomly from the entire dataset. Each data point has an equal chance of being included in the sample.\nExample: Suppose you have a dataset of 10,000 customer records from a retail store. To create a sample, you randomly select 1,000 records. This random selection ensures that the sample is representative of the entire customer base, avoiding any bias.\nRandom sampling is simple and effective for large datasets. It’s commonly used because it maintains the statistical properties of the original dataset. However, if there are important subgroups within the data, random sampling might not capture them proportionally.\n\n\n2. Stratified Sampling\nDefinition: Ensuring that the sample represents different subgroups (strata) within the population. Strata are defined based on one or more attributes of the data.\nExample: Consider the same dataset of 10,000 customer records, but this time the data includes a categorical variable indicating customer segments (e.g., Regular, Premium, and VIP). If you want to ensure each segment is proportionately represented in your sample, you use stratified sampling. If Regular customers make up 70% of the total, Premium 20%, and VIP 10%, your sample of 1,000 should contain 700 Regular, 200 Premium, and 100 VIP customers.\nStratified sampling is particularly useful when certain subgroups are of interest, or when there is a risk that these groups might be underrepresented in a random sample. This technique ensures that the sample accurately reflects the population’s diversity.\n\n\n3. Systematic Sampling\nDefinition: Selecting every \\(n^{th}\\) item from a list or sequence. The starting point is chosen randomly, and then every \\(n^{th}\\) item is included in the sample.\nExample: Imagine you have a dataset sorted by customer ID. To perform systematic sampling on a dataset of 10,000 customers to get a sample of 1,000, you could randomly choose a starting point between 1 and 10 (say you pick 5). You would then select every 10th customer (5, 15, 25, 35, etc.) until you reach 1,000 customers.\nSystematic sampling is straightforward and easy to implement, especially with ordered data. It’s efficient when dealing with large datasets. However, if there is a hidden pattern in the data that corresponds to the sampling interval, it could introduce bias. For instance, if every \\(10^{th}\\) customer tends to be from a particular region due to how the data was collected, this could skew the sample.\n\n\n\nPractical Considerations\n\nRandom Sampling: Best for simplicity and general representativeness.\nStratified Sampling: Ideal when subgroups are crucial to the analysis and need proportional representation.\nSystematic Sampling: Useful for ordered data and when quick, evenly spaced samples are needed.\n\n\n\nStep 2: Data preparation\nThis is a critical step in the data mining process that involves cleaning and transforming raw data into a format suitable for modeling. This stage addresses various issues such as missing data, errors, and outliers, and refines the set of variables (features) to enhance the performance of the model.\n\n1. Handling Missing Data\nHandling missing data involves imputing or removing missing values to ensure the dataset is complete and suitable for analysis.\nMethods and Examples:\n\nMean/Mode Imputation: Replace missing numerical values with the mean or categorical values with the mode of the available data.\n\nExample: If a dataset of house prices has missing values for the number of rooms, replace those missing values with the mean number of rooms from the dataset.\n\nMedian Imputation: Replace missing values with the median value, which is useful when the data has outliers.\n\nExample: For a dataset with highly skewed income data, using the median income to replace missing values can be more robust than the mean.\n\nK-Nearest Neighbors (KNN) Imputation: Use the average value of the k-nearest neighbors to impute missing data.\n\nExample: In a dataset of customer demographics, if the age is missing, find the k-nearest neighbors (e.g., 5 neighbors) based on other features (like income, education) and use their average age to fill in the missing value.\n\nInterpolation: Estimate missing values using linear or polynomial interpolation based on adjacent data points.\n\nExample: In a time series dataset of temperature readings, use linear interpolation to estimate missing temperature values.\n\n\n\n\n2. Data Cleaning\nData cleaning involves identifying and correcting errors, inconsistencies, and outliers in the data to ensure its integrity.\nMethods and Examples:\n\nOutlier Detection and Treatment: Identify and handle outliers using statistical methods or domain knowledge.\n\nExample: In a dataset of employee salaries, identify outliers by calculating the z-scores and removing or capping extreme values that fall beyond three standard deviations from the mean.\n\nConsistency Checks: Ensure that data entries are consistent across different records.\n\nExample: In a customer database, ensure that phone numbers are consistently formatted (e.g., all in the format +1-XXX-XXX-XXXX).\n\nDuplicate Removal: Identify and remove duplicate records.\n\nExample: In an e-commerce transaction dataset, check for and remove duplicate transactions to avoid double-counting sales.\n\n\n\n\n3. Feature Engineering\nFeature engineering involves creating new features from existing data to improve model performance.\nMethods and Examples:\n\nInteraction Terms: Create features that capture interactions between existing variables.\n\nExample: In a housing dataset, create an interaction term between the number of rooms and the size of the garden to capture their combined effect on house price.\n\nPolynomial Features: Generate polynomial terms from existing features.\n\nExample: For a dataset predicting car prices, create a squared term for mileage (mileage^2) to capture the non-linear relationship between mileage and price.\n\nBinning: Transform continuous variables into categorical variables by grouping values into bins.\n\nExample: Convert age into age groups (e.g., 0-18, 19-35, 36-50, 51+) to simplify the model.\n\nNormalization/Scaling\n\nNormalization and scaling involve transforming features to a similar scale, which is crucial for algorithms sensitive to the scale of input data.\n\n\nMethods and Examples:\n-   **Standardization (Z-score Normalization):** Transform features to have a mean of 0 and a standard deviation of 1.\n\n    -   **Example:** Standardize height and weight data in a health dataset to ensure they have the same scale when used in a k-nearest neighbors algorithm.\n\n    -   Formula: $z=\\frac{x - \\bar{x}}{SD_{x}}$\n\n-   **Robust Scaling:** Scale features using the median and the interquartile range (IQR) to reduce the impact of outliers.\n\n    -   **Example:** For a dataset with income data that has outliers, use robust scaling to transform the data by subtracting the median and dividing by the interquartile range (IQR).\n    -   Formula: $z=\\frac{x - Median_{x}}{IQR_{x}}$\n\n\n\nStep 3. Data Partitioning\nData partitioning divides the dataset into distinct subsets used for training, validating, and testing the model. This ensures that the model’s performance is evaluated on data that it has not seen during training, which helps in assessing its generalizability.\n\nHoldout method\n\nTraining set (Construction):\n\nDuring this phase, candidate models were built by training them on available data.\nThe model learns from the training data to recognize patterns and make predictions.\nTypically about 60% of the data.\n\nValidation set (Performance Comparison):\n\nIn this step, the performance of different candidate models were compared.\nThe validation set helps fine-tune hyperparameters and assess how well each model generalizes to unseen data.\nTypically comprises 20-30% of the data.\n\nTest set (Future Performance Assessment):\n\nOnce the model is selected, have to evaluate its performance on a separate testing dataset.\nThis assessment provides insights into how the model will perform in real-world scenarios.\nTypically comprises 10-20% of the data.\n\n\n\n\nk-Fold cross validation\nStep-by-step guide:\n\nDataset Partitioning:\n\nThe dataset is randomly divided into k subsets, called folds.\nCommon choices for k are 5 or 10.\n\nPre-Processing Step:\n\nIf an unbiased estimate of the final model is desired, set aside a specific number of observations to create a test set.\n\nTraining and Validation:\n\nIn each iteration, k-1 folds are combined to form the training set.\nThe remaining fold is used as the validation set.\n\nIteration Process:\n\nThis process is repeated k times, each time with a different fold as the validation set.\n\nResult Aggregation:\n\nThe results from the k iterations are aggregated and averaged to provide an overall performance estimate.\n\n\n\n\nLeave-One-Out Cross-Validation (LOOCV)\n\nA special case of k-fold cross-validation.\nThe number of folds equals the number of observations.\nEach iteration uses a single observation as the validation set and the rest as the training set.\n\n\n\n\nStep 4: Model Selection\n\nsee section 2.\n\n\n\nStep 5: Assessing Model Performance\n\nMean Squared Error (MSE)\n\nDefinition: MSE measures the average squared difference between the predicted values and the actual values.\nFormula: \\(MSE=1/n\\sum_{i=1}^{n}(y_i-\\hat{y_{i}})^2\\)\n\nWhere nnn is the number of observations, yiy_iyi​ is the actual value, and y^i_iy^​i​ is the predicted value.\n\nInterpretation: MSE provides an indication of the average error magnitude, with a greater penalty on larger errors due to the squaring of differences. Lower MSE values indicate better model performance, with zero being a perfect score.\nExample: If predicting house prices, MSE would represent how far off, on average, the model’s predictions are from the actual prices, emphasizing larger discrepancies more heavily.\n\n\n\nRoot Mean Squared Error (RMSE)\n\nDefinition: RMSE is the square root of MSE, providing an error measurement in the same units as the outcome variable.\nFormula: \\(RMSE=\\sqrt{1/n\\sum_{i=1}^{n}(y_i-\\hat{y_{i}})^2}\\)\nInterpretation: RMSE is more interpretable than MSE because it is in the same unit as the outcome variable. It still penalizes larger errors more significantly. Lower RMSE values indicate a better fit.\nExample: For house price predictions, an RMSE of $20,000 would mean that, on average, the model’s predictions are off by $20,000 from the actual prices.\n\n\n\nMean Absolute Error (MAE)\n\nDefinition: MAE measures the average absolute difference between the predicted values and the actual values.\nFormula:\n\n\\(RMSE=\\sqrt{1/n\\sum_{i=1}^{n}(y_i-\\hat{y_{i}})^2}\\)\n\nInterpretation: MAE provides a straightforward measure of model accuracy by calculating the average magnitude of errors in the predictions, without considering their direction (i.e., whether the predictions are under or over). It is less sensitive to outliers than MSE and RMSE. Lower MAE values indicate better model performance.\nExample: For house price predictions, an MAE of $15,000 means that, on average, the model’s predictions are off by $15,000 from the actual prices."
  },
  {
    "objectID": "notes/w11.html#section-2-predictive-model-regression-trees",
    "href": "notes/w11.html#section-2-predictive-model-regression-trees",
    "title": "Lecture Note Week 11",
    "section": "Section 2: Predictive Model: Regression Trees",
    "text": "Section 2: Predictive Model: Regression Trees\n\nIntroduction to Regression Trees\nA regression tree is a type of predictive model that employs a series of decision rules applied to the input features. This method is useful for predicting continuous outcomes based on various input variables.\n\nHow Regression Trees Work?\n\nSequential Splitting:\n\nThe model starts by asking a series of questions at each node, which sequentially split the data into two more homogeneous groups (branches). Each question involves comparing the value of an input variable to a certain threshold.\n\nSelection of Input Variables and Thresholds:\n\nAt each node, the model selects the input variable and threshold value that best minimize the overall variance in the outcome variable. This ensures that each split results in groups that are as homogeneous as possible.\n\nBuilding the Tree:\n\nThis process of splitting and minimizing variance continues until a predefined stopping criterion is met, such as a minimum number of observations in each terminal node or a maximum depth of the tree.\n\nApplication to Validation Set:\n\nOnce the tree is built, it is used to classify observations in a validation set. Each observation is guided down the tree, following the rules at each node until it reaches a terminal node.\n\nPrediction:\n\nThe predicted value for each observation is the average value of the outcome variable for all observations in the terminal node to which it belongs.\n\n\n\n\nAdvantages of Regression Trees\n\nInterpretability: The decision rules at each node are straightforward, making the model easy to interpret and understand.\nFlexibility: Regression trees can handle both numerical and categorical data and can model complex, non-linear relationships between the input and output variables.\nNo Assumptions: Unlike linear regression models, regression trees do not assume a specific form of the relationship between the input and output variables."
  },
  {
    "objectID": "notes/w11.html#section-3-case-problem---housing-bubble",
    "href": "notes/w11.html#section-3-case-problem---housing-bubble",
    "title": "Lecture Note Week 11",
    "section": "Section 3: Case Problem - Housing Bubble 🏘️",
    "text": "Section 3: Case Problem - Housing Bubble 🏘️\nClick here."
  },
  {
    "objectID": "notes/w10.html#section-1-introduction-to-predictive-data-mining",
    "href": "notes/w10.html#section-1-introduction-to-predictive-data-mining",
    "title": "Lecture Note Week 11",
    "section": "Section 1: Introduction to Predictive Data Mining",
    "text": "Section 1: Introduction to Predictive Data Mining\nPredictive data mining, also known as supervised learning, is a powerful set of techniques used to forecast future outcomes based on historical data. In supervised learning, the process involves utilizing known values of an outcome variable to guide the learning algorithm in identifying patterns and relationships with the input features. By doing so, predictive models are developed that can make accurate predictions on new, unseen data.\nThe predictive data mining process includes several critical steps:\n\nData Sampling: Selecting a representative subset of data to ensure the models can generalize well to new data.\nData Preparation: Cleaning and transforming the data to address missing values, errors, and refining the set of variables.\nData Partitioning: Dividing the dataset into training, validation, and test sets to evaluate model performance.\nModel Construction and Assessment: Building and assessing various predictive models to select the best one based on performance metrics.\n\nBy following these steps, predictive data mining aims to create robust models that can provide valuable insights and predictions, driving informed decision-making across various fields and applications.\n\nStep 1: Data Sampling\nData sampling involves selecting a representative subset of the data from a dataset. This step is crucial because it ensures that the models trained later can generalize well to new, unseen data. Sampling should be done carefully to maintain the statistical properties of the original dataset.\n\n1. Random Sampling\nDefinition: Selecting a subset randomly from the entire dataset. Each data point has an equal chance of being included in the sample.\nExample: Suppose you have a dataset of 10,000 customer records from a retail store. To create a sample, you randomly select 1,000 records. This random selection ensures that the sample is representative of the entire customer base, avoiding any bias.\nRandom sampling is simple and effective for large datasets. It’s commonly used because it maintains the statistical properties of the original dataset. However, if there are important subgroups within the data, random sampling might not capture them proportionally.\n\n\n2. Stratified Sampling\nDefinition: Ensuring that the sample represents different subgroups (strata) within the population. Strata are defined based on one or more attributes of the data.\nExample: Consider the same dataset of 10,000 customer records, but this time the data includes a categorical variable indicating customer segments (e.g., Regular, Premium, and VIP). If you want to ensure each segment is proportionately represented in your sample, you use stratified sampling. If Regular customers make up 70% of the total, Premium 20%, and VIP 10%, your sample of 1,000 should contain 700 Regular, 200 Premium, and 100 VIP customers.\nStratified sampling is particularly useful when certain subgroups are of interest, or when there is a risk that these groups might be underrepresented in a random sample. This technique ensures that the sample accurately reflects the population’s diversity.\n\n\n3. Systematic Sampling\nDefinition: Selecting every \\(n^{th}\\) item from a list or sequence. The starting point is chosen randomly, and then every \\(n^{th}\\) item is included in the sample.\nExample: Imagine you have a dataset sorted by customer ID. To perform systematic sampling on a dataset of 10,000 customers to get a sample of 1,000, you could randomly choose a starting point between 1 and 10 (say you pick 5). You would then select every 10th customer (5, 15, 25, 35, etc.) until you reach 1,000 customers.\nSystematic sampling is straightforward and easy to implement, especially with ordered data. It’s efficient when dealing with large datasets. However, if there is a hidden pattern in the data that corresponds to the sampling interval, it could introduce bias. For instance, if every \\(10^{th}\\) customer tends to be from a particular region due to how the data was collected, this could skew the sample.\n\n\n\nPractical Considerations\n\nRandom Sampling: Best for simplicity and general representativeness.\nStratified Sampling: Ideal when subgroups are crucial to the analysis and need proportional representation.\nSystematic Sampling: Useful for ordered data and when quick, evenly spaced samples are needed.\n\n\n\nStep 2: Data preparation\nThis is a critical step in the data mining process that involves cleaning and transforming raw data into a format suitable for modeling. This stage addresses various issues such as missing data, errors, and outliers, and refines the set of variables (features) to enhance the performance of the model.\n\n1. Handling Missing Data\nHandling missing data involves imputing or removing missing values to ensure the dataset is complete and suitable for analysis.\nMethods and Examples:\n\nMean/Mode Imputation: Replace missing numerical values with the mean or categorical values with the mode of the available data.\n\nExample: If a dataset of house prices has missing values for the number of rooms, replace those missing values with the mean number of rooms from the dataset.\n\nMedian Imputation: Replace missing values with the median value, which is useful when the data has outliers.\n\nExample: For a dataset with highly skewed income data, using the median income to replace missing values can be more robust than the mean.\n\nK-Nearest Neighbors (KNN) Imputation: Use the average value of the k-nearest neighbors to impute missing data.\n\nExample: In a dataset of customer demographics, if the age is missing, find the k-nearest neighbors (e.g., 5 neighbors) based on other features (like income, education) and use their average age to fill in the missing value.\n\nInterpolation: Estimate missing values using linear or polynomial interpolation based on adjacent data points.\n\nExample: In a time series dataset of temperature readings, use linear interpolation to estimate missing temperature values.\n\n\n\n\n2. Data Cleaning\nData cleaning involves identifying and correcting errors, inconsistencies, and outliers in the data to ensure its integrity.\nMethods and Examples:\n\nOutlier Detection and Treatment: Identify and handle outliers using statistical methods or domain knowledge.\n\nExample: In a dataset of employee salaries, identify outliers by calculating the z-scores and removing or capping extreme values that fall beyond three standard deviations from the mean.\n\nConsistency Checks: Ensure that data entries are consistent across different records.\n\nExample: In a customer database, ensure that phone numbers are consistently formatted (e.g., all in the format +1-XXX-XXX-XXXX).\n\nDuplicate Removal: Identify and remove duplicate records.\n\nExample: In an e-commerce transaction dataset, check for and remove duplicate transactions to avoid double-counting sales.\n\n\n\n\n3. Feature Engineering\nFeature engineering involves creating new features from existing data to improve model performance.\nMethods and Examples:\n\nInteraction Terms: Create features that capture interactions between existing variables.\n\nExample: In a housing dataset, create an interaction term between the number of rooms and the size of the garden to capture their combined effect on house price.\n\nPolynomial Features: Generate polynomial terms from existing features.\n\nExample: For a dataset predicting car prices, create a squared term for mileage (mileage^2) to capture the non-linear relationship between mileage and price.\n\nBinning: Transform continuous variables into categorical variables by grouping values into bins.\n\nExample: Convert age into age groups (e.g., 0-18, 19-35, 36-50, 51+) to simplify the model.\n\nNormalization/Scaling\n\nNormalization and scaling involve transforming features to a similar scale, which is crucial for algorithms sensitive to the scale of input data.\n\n\nMethods and Examples:\n-   **Standardization (Z-score Normalization):** Transform features to have a mean of 0 and a standard deviation of 1.\n\n    -   **Example:** Standardize height and weight data in a health dataset to ensure they have the same scale when used in a k-nearest neighbors algorithm.\n\n    -   Formula: $z=\\frac{x - \\bar{x}}{SD_{x}}$\n\n-   **Robust Scaling:** Scale features using the median and the interquartile range (IQR) to reduce the impact of outliers.\n\n    -   **Example:** For a dataset with income data that has outliers, use robust scaling to transform the data by subtracting the median and dividing by the interquartile range (IQR).\n    -   Formula: $z=\\frac{x - Median_{x}}{IQR_{x}}$\n\n\n\nStep 3. Data Partitioning\nData partitioning divides the dataset into distinct subsets used for training, validating, and testing the model. This ensures that the model’s performance is evaluated on data that it has not seen during training, which helps in assessing its generalizability.\n\nHoldout method\n\nTraining set (Construction):\n\nDuring this phase, candidate models were built by training them on available data.\nThe model learns from the training data to recognize patterns and make predictions.\nTypically about 60% of the data.\n\nValidation set (Performance Comparison):\n\nIn this step, the performance of different candidate models were compared.\nThe validation set helps fine-tune hyperparameters and assess how well each model generalizes to unseen data.\nTypically comprises 20-30% of the data.\n\nTest set (Future Performance Assessment):\n\nOnce the model is selected, have to evaluate its performance on a separate testing dataset.\nThis assessment provides insights into how the model will perform in real-world scenarios.\nTypically comprises 10-20% of the data.\n\n\n\n\nk-Fold cross validation\nStep-by-step guide:\n\nDataset Partitioning:\n\nThe dataset is randomly divided into k subsets, called folds.\nCommon choices for k are 5 or 10.\n\nPre-Processing Step:\n\nIf an unbiased estimate of the final model is desired, set aside a specific number of observations to create a test set.\n\nTraining and Validation:\n\nIn each iteration, k-1 folds are combined to form the training set.\nThe remaining fold is used as the validation set.\n\nIteration Process:\n\nThis process is repeated k times, each time with a different fold as the validation set.\n\nResult Aggregation:\n\nThe results from the k iterations are aggregated and averaged to provide an overall performance estimate.\n\n\n\n\nLeave-One-Out Cross-Validation (LOOCV)\n\nA special case of k-fold cross-validation.\nThe number of folds equals the number of observations.\nEach iteration uses a single observation as the validation set and the rest as the training set.\n\n\n\n\nStep 4: Model Selection\n\nsee section 2.\n\n\n\nStep 5: Assessing Model Performance\n\nMean Squared Error (MSE)\n\nDefinition: MSE measures the average squared difference between the predicted values and the actual values.\nFormula: \\(MSE=1/n\\sum_{i=1}^{n}(y_i-\\hat{y_{i}})^2\\)\n\nWhere nnn is the number of observations, yiy_iyi​ is the actual value, and y^i_iy^​i​ is the predicted value.\n\nInterpretation: MSE provides an indication of the average error magnitude, with a greater penalty on larger errors due to the squaring of differences. Lower MSE values indicate better model performance, with zero being a perfect score.\nExample: If predicting house prices, MSE would represent how far off, on average, the model’s predictions are from the actual prices, emphasizing larger discrepancies more heavily.\n\n\n\nRoot Mean Squared Error (RMSE)\n\nDefinition: RMSE is the square root of MSE, providing an error measurement in the same units as the outcome variable.\nFormula: \\(RMSE=\\sqrt{1/n\\sum_{i=1}^{n}(y_i-\\hat{y_{i}})^2}\\)\nInterpretation: RMSE is more interpretable than MSE because it is in the same unit as the outcome variable. It still penalizes larger errors more significantly. Lower RMSE values indicate a better fit.\nExample: For house price predictions, an RMSE of $20,000 would mean that, on average, the model’s predictions are off by $20,000 from the actual prices.\n\n\n\nMean Absolute Error (MAE)\n\nDefinition: MAE measures the average absolute difference between the predicted values and the actual values.\nFormula:\n\n\\(RMSE=\\sqrt{1/n\\sum_{i=1}^{n}(y_i-\\hat{y_{i}})^2}\\)\n\nInterpretation: MAE provides a straightforward measure of model accuracy by calculating the average magnitude of errors in the predictions, without considering their direction (i.e., whether the predictions are under or over). It is less sensitive to outliers than MSE and RMSE. Lower MAE values indicate better model performance.\nExample: For house price predictions, an MAE of $15,000 means that, on average, the model’s predictions are off by $15,000 from the actual prices."
  },
  {
    "objectID": "notes/w10.html#section-2-predictive-model-regression-trees",
    "href": "notes/w10.html#section-2-predictive-model-regression-trees",
    "title": "Lecture Note Week 11",
    "section": "Section 2: Predictive Model: Regression Trees",
    "text": "Section 2: Predictive Model: Regression Trees\n\nIntroduction to Regression Trees\nA regression tree is a type of predictive model that employs a series of decision rules applied to the input features. This method is useful for predicting continuous outcomes based on various input variables.\n\nHow Regression Trees Work?\n\nSequential Splitting:\n\nThe model starts by asking a series of questions at each node, which sequentially split the data into two more homogeneous groups (branches). Each question involves comparing the value of an input variable to a certain threshold.\n\nSelection of Input Variables and Thresholds:\n\nAt each node, the model selects the input variable and threshold value that best minimize the overall variance in the outcome variable. This ensures that each split results in groups that are as homogeneous as possible.\n\nBuilding the Tree:\n\nThis process of splitting and minimizing variance continues until a predefined stopping criterion is met, such as a minimum number of observations in each terminal node or a maximum depth of the tree.\n\nApplication to Validation Set:\n\nOnce the tree is built, it is used to classify observations in a validation set. Each observation is guided down the tree, following the rules at each node until it reaches a terminal node.\n\nPrediction:\n\nThe predicted value for each observation is the average value of the outcome variable for all observations in the terminal node to which it belongs.\n\n\n\n\nAdvantages of Regression Trees\n\nInterpretability: The decision rules at each node are straightforward, making the model easy to interpret and understand.\nFlexibility: Regression trees can handle both numerical and categorical data and can model complex, non-linear relationships between the input and output variables.\nNo Assumptions: Unlike linear regression models, regression trees do not assume a specific form of the relationship between the input and output variables."
  },
  {
    "objectID": "notes/w10.html#section-3-case-problem---housing-bubble",
    "href": "notes/w10.html#section-3-case-problem---housing-bubble",
    "title": "Lecture Note Week 11",
    "section": "Section 3: Case Problem - Housing Bubble 🏘️",
    "text": "Section 3: Case Problem - Housing Bubble 🏘️\nClick here."
  },
  {
    "objectID": "notes/w1.html#section-1-business-analytics",
    "href": "notes/w1.html#section-1-business-analytics",
    "title": "Lecture Note Week 1",
    "section": "Section 1: Business Analytics",
    "text": "Section 1: Business Analytics\n\nWhat is Business Analytics?\nBusiness analytics is the practice of using data and statistical methods to analyze and interpret business performance. It involves the use of data, statistical analysis, and predictive modeling to gain insights into business operations and make informed decisions. Business analytics can help organizations identify trends, patterns, and opportunities in their data, enabling them to optimize processes, improve performance, and drive growth.\nBusiness analytics can support decision-making by providing data-driven insights that help organizations understand their customers, markets, and operations better. By analyzing data from various sources, organizations can identify key performance indicators, track progress towards goals, and make data-driven decisions to improve business outcomes.\n\n\nTypes of Busienss Analytics\nThere are three main types of business analytics:\n\nDescriptive Analytics: Descriptive analytics involves analyzing historical data to understand what has happened in the past. It focuses on summarizing and visualizing data to identify trends, patterns, and relationships. Descriptive analytics provides insights into past performance and helps organizations understand the factors that have influenced their results.\nPredictive Analytics: Predictive analytics involves using statistical models and machine learning algorithms to forecast future outcomes. It uses historical data to predict future trends, behaviors, and events. Predictive analytics can help organizations anticipate customer behavior, identify risks and opportunities, and make informed decisions based on data-driven predictions.\nPrescriptive Analytics: It is the final stage of busienss analytics. Prescriptive analytics involves using optimization and simulation techniques to recommend actions that will lead to the best possible outcomes. It goes beyond predicting what will happen to recommend what should be done. Prescriptive analytics can help organizations optimize processes, allocate resources efficiently, and make strategic decisions that maximize value.\n\n\n\nImportance of Business Analytics\nBusiness analytics is essential for organizations to gain a competitive advantage in today’s data-driven world. By leveraging data and analytics, organizations can: - Identify trends and patterns in their data - Make data-driven decisions - Optimize processes and operations - Improve customer satisfaction - Increase revenue and profitability - Mitigate risks and identify opportunities - Drive innovation and growth\n\n\nBusiness Analytics in Practice\n\nAccounting Analytics\n\nDescriptive Analytics: This involves data visualization techniques to monitor financial performance metrics such as stock returns, trading volumes, and market volatility indicators.\nPredictive Analytics: Predictive models are employed to forecast financial performance, evaluate the risk associated with investment portfolios and projects, and develop financial instruments such as derivatives.\nPrescriptive Analytics: These models are used for optimizing investment portfolios, allocating assets efficiently, and creating optimal capital budgeting plans.\n\nHuman Resource Analytics\n\nThe HR function is responsible for ensuring that the organization has the necessary mix of skill sets to meet its needs, hires the highest-quality talent, provides an environment to retain it, and achieves its organizational diversity goals.\nGoal: Effective HR practices ensure that the organization is equipped with skilled professionals who can drive the company’s success while fostering a supportive and inclusive workplace.\nGoogle uses “people analytics” to analyze data on its employees to determine the characteristics of great leaders, assess factors contributing to productivity and evaluate potential new hires.\n\nMarketing Analytics\n\nPredictive models and optimization are used to better align advertising to specific target audiences making marketing efforts more effective and efficient. Sentiment analysis allows companies to monitor better “the voice of the customer” and use the data to adjust their services and products.\n\nSupply Chain Analytics\n\nTo UPS and FedEx, the optimal sorting of goods, vehicle and staff scheduling, and vehicle routing are all key to profitability. Supply chain problems caused by the COVID-19 pandemic and world conflicts focused on using analytics to increase the resiliency of the supply chain.\nDescriptive analytics is used to monitor supply chain performance.\nPredictive analytics is used to quantify risk.\nPrescriptive analytics with scenario analysis is used to prepare supply chain solutions that can handle a high degree of disruption.\n\nAnalytics for Government and Nonprofits\n\nGovernment agencies use analytics to increase the effectiveness and accountability of programs. The U.S. Internal Revenue Service uses data mining to identify patterns that distinguish questionable annual personal income tax filings. Non profit agencies use analytics to ensure their effectiveness and accountability to donors and clients.\nDescriptive and predictive analytics monitor agency performance, track donor behavior, and forecast donations.\nData mining helps identify potential donors and minimize donor attrition. Optimization allocates scarce resources in capital budgeting.\n\nSports Analytics\n\nProfessional sports teams use analytics to assess players for the amateur drafts, decide how much to offer players in contract negotiations, and assist with on-field decisions.\nSports franchises also use analytics for off-the-field business decisions. Based on fan survey data, a predictive technique known as conjoint analysis is used to design stadium premium seating.\nPrescriptive analytics is used to adjust ticket prices throughout the season dynamically."
  },
  {
    "objectID": "notes/w1.html#section-2-introduction-to-database-management-system",
    "href": "notes/w1.html#section-2-introduction-to-database-management-system",
    "title": "Lecture Note Week 1",
    "section": "Section 2: Introduction to Database Management System",
    "text": "Section 2: Introduction to Database Management System\n\nWhat is a Database Management System?\nA database management system (DBMS) is a software system that enables users to define, create, maintain, and control access to a database. It is a collection of programs that enables users to create and maintain a database. A database is an organized collection of data that can be accessed, managed, and updated. A DBMS provides an interface for users to interact with the database, allowing them to perform operations such as querying, updating, and deleting data.\n\n\nWhy do we need a Database Management System?\nA DBMS provides several benefits over traditional file-based systems:\n\nData Integrity: A DBMS enforces data integrity constraints to ensure that data is accurate and consistent. It prevents data duplication and enforces referential integrity between related data.\nData Security: A DBMS provides access control mechanisms to restrict unauthorized access to data. It allows users to define roles and permissions to control who can access, modify, and delete data.\nData Consistency: A DBMS ensures that data is consistent across the database. It enforces constraints to maintain data consistency and prevents anomalies such as data duplication and inconsistency.\nData Recovery: A DBMS provides mechanisms for data backup and recovery. It allows users to restore data to a previous state in case of data loss or corruption.\nData Scalability: A DBMS can scale to handle large volumes of data and users. It provides mechanisms for data partitioning, replication, and clustering to distribute data across multiple servers.\nData Performance: A DBMS optimizes data access and retrieval to improve performance. It uses indexing, caching, and query optimization techniques to speed up data processing.\nData Accessibility: A DBMS provides a centralized repository for data storage and management. It allows users to access data from multiple applications and locations.\n\n\n\nWhat is a Database?\nA database is an organized collection of data that can be accessed, managed, and updated. It is a structured set of data stored in a computer system. A database typically consists of one or more tables that contain rows and columns of data. Each table represents a different entity or object, and each row represents a record or instance of that entity. A database can store data in various formats, such as text, numbers, dates, images, and multimedia files.\n\n\nTypes of Databases\nThere are several types of databases, including:\n\nRelational Databases: Relational databases store data in tables that are related to each other through common fields. They use structured query language (SQL) to query and manipulate data. Examples of relational databases include MySQL, PostgreSQL, Oracle, and SQL Server.\nNoSQL Databases: NoSQL databases store data in a non-tabular format, such as key-value pairs, document stores, column-family stores, and graph databases. They are designed to handle large volumes of unstructured data and provide high scalability and performance. Examples of NoSQL databases include MongoDB, Cassandra, Redis, and Neo4j.\nObject-Oriented Databases: Object-oriented databases store data as objects, which encapsulate data and behavior. They are designed to model complex data structures and relationships. Examples of object-oriented databases include db4o, ObjectDB, and ObjectStore.\n\n\n\nComponents of a Database\nA database consists of several components, including:\n\nTables: Tables are the primary storage structures in a database. They store data in rows and columns, where each row represents a record and each column represents a field.\nColumns: Columns are the vertical elements in a table that define the attributes of the data. Each column has a data type that specifies the type of data it can store.\nRows: Rows are the horizontal elements in a table that represent individual records. Each row contains data values for each column in the table.\n\n\n\nKeys\nA key is a field or combination of fields that uniquely identifies a record in a table. There are several types of keys, including:\n\nPrimary Key: A primary key is a unique identifier for a record in a table. It enforces data integrity and ensures that each record is uniquely identified.\nForeign Key: A foreign key is a field in a table that references the primary key of another table. It establishes a relationship between two tables and enforces referential integrity.\nComposite Key: A composite key is a combination of two or more fields that uniquely identifies a record in a table. It is used when a single field cannot uniquely identify a record."
  },
  {
    "objectID": "notes/w1.html#section-3-introduction-to-sql",
    "href": "notes/w1.html#section-3-introduction-to-sql",
    "title": "Lecture Note Week 1",
    "section": "Section 3: Introduction to SQL",
    "text": "Section 3: Introduction to SQL\n\nWhat is SQL?\nSQL stands for Structured Query Language. It is a language used to communicate with databases. It is the standard language for relational database management systems. Why we need SQL? Let’s say you are suppose to design for a system that will store the information of students in a university. How would you store the information of students? You can store the information in a spreadsheet, but what if you have to store the information of thousands of students? It will be very difficult to manage the data in a spreadsheet. Also, where do you want to store the information? In a table? Will a table suffice? What if you have to store the information of the courses that the students are enrolled in? How will you manage the data? This is where databases come in.\nDatabases are used to store large amount of data. SQL is used to communicate with the database!\nThe database can be of any type like MySQL, PostgreSQL, SQLite, etc. By ignoring all those, let’s focus on the SQL language itself, which is used to communicate with the database. Once you master SQL, you can work with any database. The most popular database is MySQL. It is an open-source database. It is used by many companies like Facebook, Twitter, etc. Oracle is also a popular database. It is used by many big companies. SQL is used to communicate with the database. It is used to perform all types of operations on the database. It is used to create a database, create a table in the database, insert data into the table, update the data, delete the data, etc.\n\n\nSQL Syntax\nSQL syntax is the set of rules that defines how a SQL query should be written. It is the set of rules that all SQL queries should follow. SQL syntax is similar to the English language, which makes it easier to write, read, and understand. SQL syntax is divided into several categories. These categories are:\n\nData Definition Language (DDL)\nData Manipulation Language (DML)\nData Query Language (DQL)\nData Control Language (DCL)\nTransaction Control Language (TCL)\n\n\n\nData Definition Language (DDL)\nData Definition Language (DDL) is used to define the structure that holds the data. It is used to create tables, columns, etc. DDL commands are auto-committed. It means that the changes made by the DDL command are saved to the database automatically. The following are the DDL commands:\n\nCREATE\nALTER\nDROP\nTRUNCATE\nCOMMENT\n\n\n\nData Manipulation Language (DML) 👈 Most Important!\nData Manipulation Language (DML) is used to manipulate the data itself. It is used to insert, update, delete, and retrieve data from the database. DML commands are not auto-committed. It means that the changes made by the DML command are not saved to the database automatically. You have to use the COMMIT command to save the changes. The following are the DML commands:\n\nSELECT\nINSERT\nUPDATE\nDELETE\n\n\n\nData Query Language (DQL)\nData Query Language (DQL) is used to retrieve the data from the database. The SELECT statement is the most commonly used DQL command. The following are the DQL commands:\n\nSELECT\nWHERE\nGROUP BY\nHAVING\nORDER BY\n\n\n\nData Control Language (DCL)\nData Control Language (DCL) is used to control the visibility of data. It is used to control the access of data. The following are the DCL commands:\n\nGRANT\nREVOKE"
  },
  {
    "objectID": "notes/w1.html#basic-sql-syntax",
    "href": "notes/w1.html#basic-sql-syntax",
    "title": "Lecture Note Week 1",
    "section": "Basic SQL Syntax",
    "text": "Basic SQL Syntax\nThe SELECT statement is used to retrieve data from a database. The basic syntax of the SELECT statement is as follows:\nSELECT column1, column2, ...\nFROM table_name;\n\nSELECT: Keyword used to select data from a database.\n\ncolumn1, column2, ...: Columns that you want to retrieve from the database.\nFROM: Keyword used to specify the table from which you want to retrieve data.\ntable_name: Name of the table from which you want to retrieve data."
  },
  {
    "objectID": "notes/w1.html#appendix-live-sql-oracle",
    "href": "notes/w1.html#appendix-live-sql-oracle",
    "title": "Lecture Note Week 1",
    "section": "Appendix: Live SQL Oracle",
    "text": "Appendix: Live SQL Oracle\nOracle Live SQL is a free, interactive, web-based tool that allows users to write, run, and share SQL scripts and PL/SQL code. It provides a convenient platform for learning and practicing SQL without needing to install Oracle Database software locally. Key benefits include:\n\nEase of Use: The web-based interface is user-friendly, making it accessible for both beginners and experienced users to write and execute SQL code.\nEducational Resources: Live SQL offers tutorials, sample scripts, and documentation to help users learn SQL and PL/SQL concepts and best practices.\nCollaboration: Users can share their scripts and solutions with others, fostering collaboration and knowledge sharing within the community.\nReal-Time Execution: Scripts run on an Oracle Database in real-time, providing immediate feedback and results.\nAccess Anywhere: As a web-based tool, Live SQL can be accessed from any device with an internet connection, making it convenient for users to practice and develop their SQL skills on the go.\n\nOracle Live SQL is an excellent resource for anyone looking to improve their SQL proficiency, offering hands-on experience with Oracle Database in a flexible and collaborative environment."
  },
  {
    "objectID": "notes/w1.html#section-1-introduction-to-database-management-system",
    "href": "notes/w1.html#section-1-introduction-to-database-management-system",
    "title": "Lecture Note Week 1",
    "section": "Section 1: Introduction to Database Management System",
    "text": "Section 1: Introduction to Database Management System\n\nWhat is a Database Management System?\nA database management system (DBMS) is a software system that enables users to define, create, maintain, and control access to a database. It is a collection of programs that enables users to create and maintain a database. A database is an organized collection of data that can be accessed, managed, and updated. A DBMS provides an interface for users to interact with the database, allowing them to perform operations such as querying, updating, and deleting data.\n\n\nWhy do we need a Database Management System?\nA DBMS provides several benefits over traditional file-based systems:\n\nData Integrity: A DBMS enforces data integrity constraints to ensure that data is accurate and consistent. It prevents data duplication and enforces referential integrity between related data.\nData Security: A DBMS provides access control mechanisms to restrict unauthorized access to data. It allows users to define roles and permissions to control who can access, modify, and delete data.\nData Consistency: A DBMS ensures that data is consistent across the database. It enforces constraints to maintain data consistency and prevents anomalies such as data duplication and inconsistency.\nData Recovery: A DBMS provides mechanisms for data backup and recovery. It allows users to restore data to a previous state in case of data loss or corruption.\nData Scalability: A DBMS can scale to handle large volumes of data and users. It provides mechanisms for data partitioning, replication, and clustering to distribute data across multiple servers.\nData Performance: A DBMS optimizes data access and retrieval to improve performance. It uses indexing, caching, and query optimization techniques to speed up data processing.\nData Accessibility: A DBMS provides a centralized repository for data storage and management. It allows users to access data from multiple applications and locations.\n\n\n\nWhat is a Database?\nA database is an organized collection of data that can be accessed, managed, and updated. It is a structured set of data stored in a computer system. A database typically consists of one or more tables that contain rows and columns of data. Each table represents a different entity or object, and each row represents a record or instance of that entity. A database can store data in various formats, such as text, numbers, dates, images, and multimedia files.\n\n\nTypes of Databases\nThere are several types of databases, including:\n\nRelational Databases: Relational databases store data in tables that are related to each other through common fields. They use structured query language (SQL) to query and manipulate data. Examples of relational databases include MySQL, PostgreSQL, Oracle, and SQL Server.\nNoSQL Databases: NoSQL databases store data in a non-tabular format, such as key-value pairs, document stores, column-family stores, and graph databases. They are designed to handle large volumes of unstructured data and provide high scalability and performance. Examples of NoSQL databases include MongoDB, Cassandra, Redis, and Neo4j.\nObject-Oriented Databases: Object-oriented databases store data as objects, which encapsulate data and behavior. They are designed to model complex data structures and relationships. Examples of object-oriented databases include db4o, ObjectDB, and ObjectStore.\n\n\n\nComponents of a Database\nA database consists of several components, including:\n\nTables: Tables are the primary storage structures in a database. They store data in rows and columns, where each row represents a record and each column represents a field.\nColumns: Columns are the vertical elements in a table that define the attributes of the data. Each column has a data type that specifies the type of data it can store.\nRows: Rows are the horizontal elements in a table that represent individual records. Each row contains data values for each column in the table.\n\n\n\nKeys\nA key is a field or combination of fields that uniquely identifies a record in a table. There are several types of keys, including:\n\nPrimary Key: A primary key is a unique identifier for a record in a table. It enforces data integrity and ensures that each record is uniquely identified.\nForeign Key: A foreign key is a field in a table that references the primary key of another table. It establishes a relationship between two tables and enforces referential integrity.\nComposite Key: A composite key is a combination of two or more fields that uniquely identifies a record in a table. It is used when a single field cannot uniquely identify a record."
  },
  {
    "objectID": "notes/w6.html#section-1-introduction-to-r-language",
    "href": "notes/w6.html#section-1-introduction-to-r-language",
    "title": "Lecture Note Week 6",
    "section": "Section 1: Introduction to R Language",
    "text": "Section 1: Introduction to R Language\n\nWhat is R?\nR is a programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis.\n\n\nWhy R?\n\nR is open-source and free\nR is a powerful tool for data analysis\nR has a large and active community\nR has a large number of packages for data analysis\nR is a programming language and can be used for automating data analysis tasks\nR is a great tool for reproducible research\n\n\n\nRStudio\nRsudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management."
  },
  {
    "objectID": "notes/w6.html#section-2-modeling-uncertainty",
    "href": "notes/w6.html#section-2-modeling-uncertainty",
    "title": "Lecture Note Week 6",
    "section": "Section 2: Modeling Uncertainty",
    "text": "Section 2: Modeling Uncertainty\nUncertainty is a fundamental concept in many fields, including statistics, economics, engineering, and everyday decision-making. It refers to the state of having limited knowledge about an event, outcome, or condition. This lack of certainty means that we cannot predict the outcome with complete confidence.\nProbability and Uncertainty:\nProbability is a mathematical framework used to quantify uncertainty. It provides a way to measure how likely an event is to occur. Here are some key concepts:\n\nProbability of an Event: The probability of an event AAA, denoted as P(A)P(A)P(A), ranges from 0 (the event will not occur) to 1 (the event will certainly occur). For example, the probability of flipping a fair coin and getting heads is P(Heads)=0.5P() = 0.5P(Heads)=0.5.\nRandom Variables: A random variable is a variable whose possible values are numerical outcomes of a random phenomenon. For example, the number of heads in 10 coin tosses is a random variable.\nProbability Distributions: A probability distribution describes how the probabilities are distributed over the values of the random variable. Common distributions include the binomial distribution (for binary outcomes), the normal distribution (for continuous outcomes), and the Poisson distribution (for count data).\n\n\n5.1 Random Experiment and Sample Space\n\nDefinition of a Random Experiment\nA random experiment is a process or procedure that leads to one of several possible outcomes, where the outcome cannot be predicted with certainty beforehand. Each performance of the experiment is independent and can yield any of the possible outcomes.\n\n\nCharacteristics of a Random Experiment\n\nUncertainty: The outcome of the experiment cannot be determined with certainty before it is conducted.\nRepeatability: The experiment can be repeated under identical conditions.\nWell-defined Outcomes: All possible outcomes are known and can be listed.\n\n\n\nSample Space\nThe sample space, denoted by SSS, of a random experiment is the set of all possible outcomes of that experiment. Each possible outcome in the sample space is known as a sample point or an elementary event.\n\n\nExamples of Random Experiments and Sample Spaces\nExample 1: Tossing a Coin\n\nRandom Experiment: Tossing a coin.\nPossible Outcomes (Sample Points): The coin can land on either Head (H) or Tail (T).\nSample Space: The sample space for this experiment is: S={Head,Tail}S = { , }S={Head,Tail}\n\nExample 2: Rolling a Die\n\nRandom Experiment: Rolling a six-sided die.\nPossible Outcomes (Sample Points): The die can land on any one of the six faces, showing 1, 2, 3, 4, 5, or 6.\nSample Space: The sample space for this experiment is: S={1,2,3,4,5,6}S = { 1, 2, 3, 4, 5, 6 }S={1,2,3,4,5,6}\n\n\n\nFurther Examples and Concepts\nExample 3: Drawing a Card from a Deck\n\nRandom Experiment: Drawing a single card from a standard 52-card deck.\nPossible Outcomes (Sample Points): Each card in the deck is a distinct outcome, such as Ace of Spades, 2 of Hearts, etc.\nSample Space: The sample space for this experiment is: S={Ace of Spades,2 of Hearts,…,King of Clubs}S = { , , , }S={Ace of Spades,2 of Hearts,…,King of Clubs} Here, SSS contains 52 elements, representing each card in the deck.\n\nExample 4: Flipping Two Coins\n\nRandom Experiment: Flipping two coins simultaneously.\nPossible Outcomes (Sample Points): The outcomes can be both heads, both tails, head on the first coin and tail on the second, or tail on the first coin and head on the second.\nSample Space: The sample space for this experiment is: \\(S={(Head, Head),(Head, Tail),(Tail, Head),(Tail, Tail)}S = \\{ (\\text{Head, Head}), (\\text{Head, Tail}), (\\text{Tail, Head}), (\\text{Tail, Tail}) \\}\\)={(Head, Head),(Head, Tail),(Tail, Head),(Tail, Tail)}\n\nExample 5: Rolling Two Dice\n\nRandom Experiment: Rolling two six-sided dice simultaneously.\nPossible Outcomes (Sample Points): Each die can show a number from 1 to 6, resulting in pairs such as (1,1), (1,2), …, (6,6).\nSample Space: The sample space for this experiment is: S={(1,1),(1,2),…,(6,6)}S = { (1,1), (1,2), , (6,6) }S={(1,1),(1,2),…,(6,6)} Here, SSS contains 36 elements, representing each possible pair of outcomes."
  },
  {
    "objectID": "hw/hw-1.html",
    "href": "hw/hw-1.html",
    "title": "ETF2121/5912: Individual Assignment",
    "section": "",
    "text": "Learning goals 🏈\n\n\n\nIn this assignment, you will learn…\n\nable to use SQL effectively to\n\nWrite SQL queries to extract data from a database\nUse SQL to perform data cleaning and transformation\nUse SQL to perform data analysis and derive insights\n\nable to use Power BI effectively to\n\nImport and clean data in Power BI\nCreate calculated columns and measures\nDesign various types of charts and graphs\nBuild an interactive dashboard\nDerive and present meaningful insights from data\nThis assignment constitute 20% of the total mark of this course.",
    "crumbs": [
      "Assessment",
      "Individual Assignment"
    ]
  },
  {
    "objectID": "hw/hw-1.html#introduction",
    "href": "hw/hw-1.html#introduction",
    "title": "Individual Assignment",
    "section": "",
    "text": "You will be provided with a CSV file containing sales data for a fictional global retail company. The dataset includes information on sales transactions, products, customers, and locations.\n\n\nDemonstrate your proficiency in data wrangling and visualization using Power BI by creating a comprehensive dashboard from a given dataset.\n\n\n\nIn this assignment, you will learn…\n\nImport and clean data in Power BI\nCreate calculated columns and measures\nDesign various types of charts and graphs\nBuild an interactive dashboard\nDerive and present meaningful insights from data"
  },
  {
    "objectID": "hw/hw-1.html#tasks",
    "href": "hw/hw-1.html#tasks",
    "title": "Individual Assignment",
    "section": "Tasks:",
    "text": "Tasks:\n\nData Wrangling (40% of total marks)\n\nImport the CSV file into Power BI.\nClean the data, addressing any inconsistencies, missing values, or errors.\nCreate at least 3 calculated measures using DAX. Ensure that your calculated measures are meaningful and correctly implemented. At least two of these measures should be used effectively in your dashboard visualizations.\nDocument your data wrangling process within the Power BI file.\n\nDashboard Creation (50% of total marks) Create a dashboard with at least 5 different types of visualizations. Ensure that your visualizations are interactive and provide meaningful insights.\nPresentation and Usability (10% of total marks)\n\nOrganize your dashboard logically and aesthetically.\nInclude slicers or filters for user interactivity.\nProvide clear titles and labels for all visualizations.\n\n\n\nSubmit:\n\nYour Power BI file (.pbix) showing your data preparation, calculated measures, and final dashboard"
  },
  {
    "objectID": "hw/hw-1.html#individual-assignment",
    "href": "hw/hw-1.html#individual-assignment",
    "title": "Individual Assignment",
    "section": "",
    "text": "In this assignment, you will learn…\n\nable to use SQL effectively to\n\nWrite SQL queries to extract data from a database\nUse SQL to perform data cleaning and transformation\nUse SQL to perform data analysis and derive insights\n\nable to use Power BI effectively to\n\nImport and clean data in Power BI\nCreate calculated columns and measures\nDesign various types of charts and graphs\nBuild an interactive dashboard\nDerive and present meaningful insights from data"
  },
  {
    "objectID": "hw/hw-1.html#section-1-sql",
    "href": "hw/hw-1.html#section-1-sql",
    "title": "ETF2121/5912: Individual Assignment",
    "section": "Section 1: SQL 🍀",
    "text": "Section 1: SQL 🍀\n\nObjective\nDemonstrate your proficiency in SQL by writing queries to extract, clean, and analyze data from a given database.\n\nDataset: Student Accomodation\nYou are given the dataset here. Please upload the sql script into oracle live sql and continue to work from there. Please watch the video above on how to upload the script. You can proceed to answer the related question once all the tables are created successfully.\nThe detail of the dataset as below:\nTables and Columns:\n\nOWNER\n\nColumns: OWNER_NUM (PK), LAST_NAME, FIRST_NAME, ADDRESS, CITY, STATE, ZIP_CODE\n\nPROPERTY\n\nColumns: PROPERTY_ID (PK), OFFICE_NUM, ADDRESS, SQR_FT, BDRMS, FLOORS, MONTHLY_RENT, OWNER_NUM (FK)\n\nOFFICE\n\nColumns: OFFICE_NUM (PK), OFFICE_NAME, ADDRESS, AREA, CITY, STATE, ZIP_CODE\n\nSERVICE_CATEGORY\n\nColumns: CATEGORY_NUM (PK), CATEGORY_DESCRIPTION\n\nSERVICE_REQUEST\n\nColumns: SERVICE_ID (PK), PROPERTY_ID (FK), CATEGORY_NUMBER (FK), OFFICE_ID (FK), DESCRIPTION, STATUS, EST_HOURS, SPENT_HOURS, NEXT_SERVICE_DATE\n\nRESIDENTS\n\nColumns: RESIDENT_ID (PK), FIRST_NAME, SURNAME, PROPERTY_ID (FK)\n\n\nNotes: FK stands for foreign key and PK stands for primary key in the corresponding table.\n\n\n\nTasks:\n\nBased on the information given above, draft a schema diagram to show the relationship between all tables.\nWhich properties have a monthly rent greater than $1500?\nHow many properties are there in each state?\nWhat is the total number of bedrooms in all properties?\nWhat is the average estimated and spent hours on service requests?\nHow many properties are managed by each office?\nWhat is the average monthly rent of properties in each city?\nWhich properties have service requests where the spent hours exceed the estimated hours?\nWhat is the percentage distribution of service requests by their status?\nWhat are the average monthly rent and average square footage per bedroom in each city?\n\nShow your sql queries for each question and sunmit the sql script file.",
    "crumbs": [
      "Assessment",
      "Individual Assignment"
    ]
  },
  {
    "objectID": "hw/hw-1.html#section-2-power-bi",
    "href": "hw/hw-1.html#section-2-power-bi",
    "title": "ETF2121/5912: Individual Assignment",
    "section": "Section 2: Power BI 🖼",
    "text": "Section 2: Power BI 🖼\nYou will be provided with a CSV file containing sales data for a hotel booking. The dataset includes information on sales transactions, products, customers, and locations.\n\nObjective\nDemonstrate your proficiency in data wrangling and visualization using Power BI by creating a comprehensive dashboard from this dataset. (Source: Hotel Booking Demand)\nCreate a comprehensive dashboard using Power BI to assist hotel management in making data-driven decisions. Your dashboard should provide insights into key performance indicators (KPIs) and trends that can help improve hotel operations, guest satisfaction, and revenue management.\n\nDataset:\nYou will be using the provided dataset which contains information on hotel bookings. The dataset includes the following columns:\n\nhotel: Type of hotel (Resort Hotel or City Hotel)\nis_canceled: Booking cancellation status (0: Not Canceled, 1: Canceled)\nlead_time: Number of days before the actual arrival date when the booking was made\narrival_date_year, arrival_date_month, arrival_date_week_number, arrival_date_day_of_month: Information on the arrival date\nstays_in_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\nstays_in_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\nadults, children, babies: Number of adults, children, and babies\nmeal: Type of meal booked\ncountry: Country of origin of the guest\nmarket_segment: Market segment designation\ndistribution_channel: Booking distribution channel\nis_repeated_guest: Whether the guest is a repeated guest (0: No, 1: Yes)\nprevious_cancellations, previous_bookings_not_canceled: Number of previous bookings that were canceled or not canceled\nreserved_room_type, assigned_room_type: Code of room type reserved and assigned\nbooking_changes: Number of changes made to the booking\ndeposit_type: Type of deposit made\nagent, company: ID of the travel agent and company that made the booking\ndays_in_waiting_list: Number of days the booking was in the waiting list\ncustomer_type: Type of customer (Transient, Contract, Group, Transient-Party)\nadr: Average Daily Rate\nrequired_car_parking_spaces: Number of car parking spaces required\ntotal_of_special_requests: Total number of special requests made by the guest\nreservation_status: Reservation last status (Canceled, Check-Out, No-Show)\nreservation_status_date: Date at which the last status was set\n\n\n\n\nTasks:\n\nData Wrangling (40% of total marks)\n\nImport the CSV file into Power BI.\nClean the data, addressing any inconsistencies, missing values, or errors.\nCreate at least 3 calculated measures. Ensure that your calculated measures are meaningful and correctly implemented.\nDocument your data wrangling process within the Power BI file.\n\nDashboard Creation (50% of total marks).\nCreate a dashboard with at least 5 different types of visualizations. Use appropriate chart types (bar charts, line charts, pie charts, etc.) to display the data effectively. Include slicers and filters to allow users to interact with the data (e.g., filter by hotel type, date range, market segment). Ensure that your visualizations are interactive and provide meaningful insights. For example, you can identify key performance indicators (KPIs) relevant to hotel management. Examples include: Occupancy Rate, Cancellation Rate, Average Daily Rate (ADR), Revenue Per Available Room (RevPAR), Number of Repeat Guests, Average Lead Time\nPresentation and Usability (10% of total marks)\n\nOrganize your dashboard logically and aesthetically.\nProvide clear titles and labels for all visualizations.\nBased on the visualized data, provide insights and actionable recommendations for hotel management. Add this to the last page of your dashboard that should serve as a report to highlight any trends, anomalies, or areas that require attention.\n\nSubmit one Power BI file (.pbix) showing your data preparation, calculated measures, and final dashboard.\n\nSubmit\n\n\n\n\n\n\nImportant\n\n\n\nNotes: This assignment requires you to submit 2 files: 1 .sql file, 1 .pbix file.",
    "crumbs": [
      "Assessment",
      "Individual Assignment"
    ]
  },
  {
    "objectID": "notes/w1.html#section-2-introduction-to-sql",
    "href": "notes/w1.html#section-2-introduction-to-sql",
    "title": "Lecture Note Week 1",
    "section": "Section 2: Introduction to SQL",
    "text": "Section 2: Introduction to SQL\n\nWhat is SQL?\nSQL stands for Structured Query Language. It is a language used to communicate with databases. It is the standard language for relational database management systems. Why we need SQL? Let’s say you are suppose to design for a system that will store the information of students in a university. How would you store the information of students? You can store the information in a spreadsheet, but what if you have to store the information of thousands of students? It will be very difficult to manage the data in a spreadsheet. Also, where do you want to store the information? In a table? Will a table suffice? What if you have to store the information of the courses that the students are enrolled in? How will you manage the data? This is where databases come in.\nDatabases are used to store large amount of data. SQL is used to communicate with the database!\nThe database can be of any type like MySQL, PostgreSQL, SQLite, etc. By ignoring all those, let’s focus on the SQL language itself, which is used to communicate with the database. Once you master SQL, you can work with any database. The most popular database is MySQL. It is an open-source database. It is used by many companies like Facebook, Twitter, etc. Oracle is also a popular database. It is used by many big companies. SQL is used to communicate with the database. It is used to perform all types of operations on the database. It is used to create a database, create a table in the database, insert data into the table, update the data, delete the data, etc.\n\n\nSQL Syntax\nSQL syntax is the set of rules that defines how a SQL query should be written. It is the set of rules that all SQL queries should follow. SQL syntax is similar to the English language, which makes it easier to write, read, and understand. SQL syntax is divided into several categories. These categories are:\n\nData Definition Language (DDL)\nData Manipulation Language (DML)\nData Query Language (DQL)\nData Control Language (DCL)\nTransaction Control Language (TCL)\n\n\n\nData Definition Language (DDL)\nData Definition Language (DDL) is used to define the structure that holds the data. It is used to create tables, columns, etc. DDL commands are auto-committed. It means that the changes made by the DDL command are saved to the database automatically. The following are the DDL commands:\n\nCREATE\nALTER\nDROP\nTRUNCATE\nCOMMENT\n\n\n\nData Manipulation Language (DML) 👈 Most Important!\nData Manipulation Language (DML) is used to manipulate the data itself. It is used to insert, update, delete, and retrieve data from the database. DML commands are not auto-committed. It means that the changes made by the DML command are not saved to the database automatically. You have to use the COMMIT command to save the changes. The following are the DML commands:\n\nSELECT\nINSERT\nUPDATE\nDELETE\n\n\n\nData Query Language (DQL)\nData Query Language (DQL) is used to retrieve the data from the database. The SELECT statement is the most commonly used DQL command. The following are the DQL commands:\n\nSELECT\nWHERE\nGROUP BY\nHAVING\nORDER BY\n\n\n\nData Control Language (DCL)\nData Control Language (DCL) is used to control the visibility of data. It is used to control the access of data. The following are the DCL commands:\n\nGRANT\nREVOKE"
  },
  {
    "objectID": "notes/w1.html#section-3-live-sql-oracle",
    "href": "notes/w1.html#section-3-live-sql-oracle",
    "title": "Lecture Note Week 1",
    "section": "Section 3: Live SQL Oracle",
    "text": "Section 3: Live SQL Oracle\nOracle Live SQL is a free, interactive, web-based tool that allows users to write, run, and share SQL scripts and PL/SQL code. It provides a convenient platform for learning and practicing SQL without needing to install Oracle Database software locally. Key benefits include:\n\nEase of Use: The web-based interface is user-friendly, making it accessible for both beginners and experienced users to write and execute SQL code.\nEducational Resources: Live SQL offers tutorials, sample scripts, and documentation to help users learn SQL and PL/SQL concepts and best practices.\nCollaboration: Users can share their scripts and solutions with others, fostering collaboration and knowledge sharing within the community.\nReal-Time Execution: Scripts run on an Oracle Database in real-time, providing immediate feedback and results.\nAccess Anywhere: As a web-based tool, Live SQL can be accessed from any device with an internet connection, making it convenient for users to practice and develop their SQL skills on the go.\n\nOracle Live SQL is an excellent resource for anyone looking to improve their SQL proficiency, offering hands-on experience with Oracle Database in a flexible and collaborative environment."
  },
  {
    "objectID": "labs/lab-2a.html",
    "href": "labs/lab-2a.html",
    "title": "Tutorial 2 - SQL",
    "section": "",
    "text": "Instructions:\nUse the hr schema from Oracle Live SQL to complete the following exercises.\n\n\nLog in and click Start Coding as screenshot below.\n\n\n\nClick on SQL worksheet\n\n\n\nStart solving the questions below by typing in the SQL code in the editor. Remember to prefix your table name with hr.table_name to represent the table is from hr schema.\n\n\n\nSection 1: Easy\n\nRetrieve the first name and last name of all employees.\nList all distinct job IDs from the employees table.\nRetrieve the first name and last name of all employees, sorted by last name in ascending order.\nFind all employees who work in department 50.\nFind all employees whose first name starts with ‘J’.\n\n\n\nSection 2: Intermediate\n\nFind all employees who work in departments 10, 20, or 30.\nFind all employees who work in department 50 and have a salary greater than 5000.\nCount the number of employees in department 50.\nCalculate the total salary of all employees in department 50.\nCalculate the average salary of employees in department 50.\n\n\n\nSection 3: Hard\n\nCount the number of employees in each department.\nFind departments with more than 3 employees.\nRetrieve the top 5 highest paid employees.\nList all employees along with their department names.\nFind employees who earn more than the average salary.\n\n\n\nSection 4: Challenge\n\nRetrieve employees along with their department names and job titles.\nFind employees who work in the ‘Sales’ department.\nList all departments and the number of employees in each department, including departments without employees.\nList all employees and the number of employees in their departments, including employees without a department.\nFind departments with more than 5 employees and list the department names and employee counts."
  },
  {
    "objectID": "notes/w4.html#data-visualization",
    "href": "notes/w4.html#data-visualization",
    "title": "Lecture Note 4",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n1. Preattentive Attributes and Data-Ink Ratio\nPreattentive Attributes:\n\nColor: Use contrasting colors to highlight key data points.\nSize: Larger elements draw attention.\nOrientation: Different shapes and orientations can guide focus.\nPosition: Place important data at prominent positions.\n\nData-Ink Ratio:\n\nDefinition: The ratio of data-ink (ink used to present data) to the total ink used in the chart.\nGoal: Maximize the data-ink ratio by minimizing non-essential ink.\nImplementation in Power BI:\n\nUse minimalist design principles.\nAvoid unnecessary gridlines and borders.\nFocus on essential data representation.\n\n\n\n\n2. PivotTables and PivotCharts\nPurpose:\n\nPivotTables: Summarize, analyze, explore, and present summary data.\nPivotCharts: Visualize the data from PivotTables.\n\nCreating PivotTables in Power BI:\n\nImport Data: Load your dataset into Power BI.\n\nGo to Home &gt; Get Data &gt; Excel (or choose your data source).\nSelect your file and click Load.\n\nCreate PivotTable:\n\nGo to Modeling &gt; New Table.\nWrite a DAX formula to create your PivotTable. For example:\nDAX\nSUMMARIZE(   'Sales',   'Sales'[Region],   'Sales'[Category],   \"Total Sales\", SUM('Sales'[SalesAmount]) )\n\nVisualize Data:\n\nDrag fields into Rows, Columns, and Values areas in the Visualizations pane.\nAdjust the fields to refine your table.\n\n\nCreating PivotCharts in Power BI:\n\nSelect Data:\n\nClick on your PivotTable to activate it.\nGo to Visualizations pane and choose a chart type (e.g., Clustered Column Chart).\n\nCustomize Chart:\n\nDrag and drop fields into the Axis, Legend, and Values areas.\nCustomize the chart using the Format pane.\n\n\n\n\n3. Scatter Charts\nPurpose: Examine relationships between two variables.\nCreating Scatter Charts in Power BI:\n\nSelect Scatter Chart:\n\nGo to the Visualizations pane.\nClick on the Scatter Chart icon.\n\nAdd Data Fields:\n\nDrag numerical fields to the X-axis and Y-axis.\nOptionally, add a third numerical field to the Size field well.\n\nCustomize and Interpret:\n\nUse the Format pane to customize the chart’s appearance.\nInterpret the relationship between variables by examining the pattern of points.\n\n\n\n\n4. Line Charts, Bar Charts, and Column Charts\nPurpose:\n\nLine Charts: Show trends over time.\nBar Charts: Compare quantities across categories.\nColumn Charts: Similar to bar charts, but vertical.\n\nCreating Line Charts in Power BI:\n\nSelect Line Chart:\n\nGo to the Visualizations pane.\nClick on the Line Chart icon.\n\nAdd Data Fields:\n\nDrag a date field to the X-axis and a numerical field to the Y-axis.\n\n\nCreating Bar and Column Charts in Power BI:\n\nSelect Bar/Column Chart:\n\nGo to the Visualizations pane.\nClick on the Bar Chart or Column Chart icon.\n\nAdd Data Fields:\n\nDrag a categorical field to the Axis area and a numerical field to the Values area.\n\n\n\n\n5. Trendlines\nPurpose: Identify trends within scatter and line charts.\nAdding Trendlines in Power BI:\n\nSelect Chart:\n\nClick on the chart where you want to add a trendline (e.g., scatter chart or line chart).\n\nAdd Trendline:\n\nGo to the Analytics pane.\nClick on Trend Line, then click + Add.\n\nCustomize Trendline:\n\nSet properties such as color, style, and transparency.\n\n\n\n\n6. Sorted, Clustered, and Stacked Bar (Column) Charts\nPurpose:\n\nSorted Bar/Column Charts: Rank data in ascending or descending order.\nClustered Bar/Column Charts: Compare multiple categories across a single axis.\nStacked Bar/Column Charts: Show composition and compare totals.\n\nCreating Sorted Bar/Column Charts:\n\nSort Data:\n\nClick on the ellipsis (…) in the top right corner of the chart.\nSelect Sort by and choose the desired field.\n\n\nCreating Clustered Bar/Column Charts:\n\nSelect Clustered Chart:\n\nGo to the Visualizations pane.\nClick on the Clustered Bar Chart or Clustered Column Chart icon.\n\nAdd Data Fields:\n\nDrag multiple categorical fields to the Axis area and a numerical field to the Values area.\n\n\nCreating Stacked Bar/Column Charts:\n\nSelect Stacked Chart:\n\nGo to the Visualizations pane.\nClick on the Stacked Bar Chart or Stacked Column Chart icon.\n\nAdd Data Fields:\n\nDrag a categorical field to the Axis area and another categorical field to the Legend area.\n\n\n\n\n7. Bubble Charts, Scatter Chart Matrices, and Table Lenses\nPurpose:\n\nBubble Charts: Visualize three dimensions of data.\nScatter Chart Matrices: Show relationships across multiple variables.\nTable Lenses: Combine tabular data with visual elements.\n\nCreating Bubble Charts in Power BI:\n\nSelect Bubble Chart:\n\nGo to the Visualizations pane.\nClick on the Scatter Chart icon and add a field to the Size well.\n\nAdd Data Fields:\n\nDrag fields to the X-axis, Y-axis, and Size areas.\n\n\nCreating Scatter Chart Matrices in Power BI:\n\nSelect Matrix:\n\nGo to the Visualizations pane.\nClick on the Matrix icon.\n\nAdd Data Fields:\n\nDrag fields to the Rows, Columns, and Values areas.\n\n\nCreating Table Lenses in Power BI:\n\nSelect Table:\n\nGo to the Visualizations pane.\nClick on the Table icon.\n\nAdd Data Fields:\n\nDrag fields to the Values area.\nUse conditional formatting to add visual cues.\n\n\n\n\n8. Advanced Visualization Types\nPurpose:\n\nHeat Maps: Show data density or intensity.\nSparklines: Show trends in a small space.\nTreemaps: Visualize hierarchical data.\nWaterfall Charts: Show cumulative effects.\nStock Charts: Display financial data.\nParallel Coordinates Plots: Visualize multi-dimensional data.\n\nCreating Heat Maps in Power BI:\n\nSelect Heat Map:\n\nUse the Shape Map visual in the Visualizations pane.\n\nAdd Data Fields:\n\nDrag fields to the Location and Value areas.\n\n\nCreating Sparklines in Power BI:\n\nAdd Sparklines:\n\nGo to Visualizations pane.\nUse the Line and clustered column chart visual.\n\nAdd Data Fields:\n\nDrag fields to the Line Values area.\n\n\nCreating Treemaps in Power BI:\n\nSelect Treemap:\n\nGo to the Visualizations pane.\nClick on the Treemap icon.\n\nAdd Data Fields:\n\nDrag hierarchical fields to the Group and Values areas.\n\n\nCreating Waterfall Charts in Power BI:\n\nSelect Waterfall Chart:\n\nGo to the Visualizations pane.\nClick on the Waterfall Chart icon.\n\nAdd Data Fields:\n\nDrag fields to the Category and Y-axis areas.\n\n\nCreating Stock Charts in Power BI:\n\nSelect Stock Chart:\n\nUse the Candlestick chart visual from a custom visual marketplace.\n\nAdd Data Fields:\n\nDrag fields to the Open, High, Low, and Close areas.\n\n\nCreating Parallel Coordinates Plots in Power BI:\n\nSelect Parallel Coordinates Plot:\n\nUse a custom visual from the marketplace.\n\nAdd Data Fields:\n\nDrag multiple numerical fields to the Values area.\n\n\n\n\n9. Geospatial Data Visualization\nPurpose:\n\nChoropleth Maps: Visualize data across geographical regions.\nCartograms: Distort map shapes to represent data values.\n\nCreating Choropleth Maps in Power BI:\n\nSelect Map:\n\nGo to the Visualizations pane.\nClick on the Map or Filled Map icon.\n\nAdd Data Fields:\n\nDrag geographical fields to the Location area and data fields to the Values area.\n\n\nCreating Cartograms in Power BI:\n\nSelect Cartogram:\n\nUse a custom visual from the marketplace.\n\nAdd Data Fields:\n\nDrag geographical fields to the Location area and data fields to the Values area.\n\n\n\n\n10. Effective Data Dashboards\nPrinciples:\n\nClarity: Ensure the dashboard is easy to read.\nConciseness: Present only the most important data.\nCustomization: Allow users to interact with the data.\n\nDesign Suggestions:\n\nLayout: Organize visualizations logically.\nInteractivity: Use slicers and filters.\nReal-Time Data: Connect to live data sources for up-to-date information.\n\nCreating Dashboards in Power BI:\n\nCreate Visualizations:\n\nIn the Report view, select the type of visualization you want from the Visualizations pane.\nDrag and drop fields from the Fields pane onto the visualization.\n\nCustomize Visualizations:\n\nUse the Format pane to customize the appearance of each chart (e.g., colors, labels, titles).\nAdd filters by dragging fields to the Filters area."
  },
  {
    "objectID": "hw/hw-1.html#section-1-10-sql",
    "href": "hw/hw-1.html#section-1-10-sql",
    "title": "ETF2121/5912: Individual Assignment",
    "section": "Section 1 (10%): SQL 🍀",
    "text": "Section 1 (10%): SQL 🍀\n\nObjective\nDemonstrate your proficiency in SQL by writing queries to extract, clean, and analyze data from a given database.\n\nDataset: Student Accomodation\nYou are given the dataset here. Please upload the sql script into oracle live sql and continue to work from there. Please watch the video below on how to upload the script. You can proceed to answer the related question once all the tables are created successfully.\n\n\nHow to upload the dataset\n\n\nThe detail of the dataset as below:\nTables and Columns:\n\nOWNER\n\nColumns: OWNER_NUM (PK), LAST_NAME, FIRST_NAME, ADDRESS, CITY, STATE, ZIP_CODE\n\nPROPERTY\n\nColumns: PROPERTY_ID (PK), OFFICE_NUM, ADDRESS, SQR_FT, BDRMS, FLOORS, MONTHLY_RENT, OWNER_NUM (FK)\n\nOFFICE\n\nColumns: OFFICE_NUM (PK), OFFICE_NAME, ADDRESS, AREA, CITY, STATE, ZIP_CODE\n\nSERVICE_CATEGORY\n\nColumns: CATEGORY_NUM (PK), CATEGORY_DESCRIPTION\n\nSERVICE_REQUEST\n\nColumns: SERVICE_ID (PK), PROPERTY_ID (FK), CATEGORY_NUMBER (FK), OFFICE_ID (FK), DESCRIPTION, STATUS, EST_HOURS, SPENT_HOURS, NEXT_SERVICE_DATE\n\nRESIDENTS\n\nColumns: RESIDENT_ID (PK), FIRST_NAME, SURNAME, PROPERTY_ID (FK)\n\n\nNotes: FK stands for foreign key and PK stands for primary key in the corresponding table.\n\n\n\nTasks:\n\nBased on the information given above, draft a schema diagram to show the relationship between all tables.\nWhich properties have a monthly rent between $1000 and $2500?\nHow many properties are there in each state?\nWhat is the total number of bedrooms in all properties?\nWhat is the average estimated and spent hours on service requests?\nHow many properties are managed by each office?\nWhat is the average monthly rent of properties in each city?\nWhich properties have service requests where the spent hours exceed the estimated hours?\nWhat is the percentage distribution of service requests by their status?\nWhich owners own more than one property. List their owner number, last name, first name, and the total number of count.\n\nShow your sql queries for each question and submit. Paste your sql command and the screenshot of your result in a .docx file.",
    "crumbs": [
      "Assessment",
      "Individual Assignment"
    ]
  },
  {
    "objectID": "hw/hw-1.html#section-2-10-power-bi",
    "href": "hw/hw-1.html#section-2-10-power-bi",
    "title": "ETF2121/5912: Individual Assignment",
    "section": "Section 2 (10%): Power BI 🖼",
    "text": "Section 2 (10%): Power BI 🖼\n💝 This is a competition assignment, the best top 3 get a mystery gift! 🎁\nYou will be provided with a CSV file containing sales data for a hotel booking. The dataset includes information on sales transactions, products, customers, and locations.\n\nObjective\nDemonstrate your proficiency in data wrangling and visualization using Power BI by creating a comprehensive dashboard from this dataset. (Source: Hotel Booking Demand)\nCreate a comprehensive dashboard using Power BI to assist hotel management in making data-driven decisions. Your dashboard should provide insights into key performance indicators (KPIs) and trends that can help improve hotel operations, guest satisfaction, and revenue management.\n\nDataset:\nYou will be using the provided dataset which contains information on hotel bookings. The dataset includes the following columns:\n\nhotel: Type of hotel (Resort Hotel or City Hotel)\nis_canceled: Booking cancellation status (0: Not Canceled, 1: Canceled)\nlead_time: Number of days before the actual arrival date when the booking was made\narrival_date_year, arrival_date_month, arrival_date_week_number, arrival_date_day_of_month: Information on the arrival date\nstays_in_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\nstays_in_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\nadults, children, babies: Number of adults, children, and babies\nmeal: Type of meal booked\ncountry: Country of origin of the guest\nmarket_segment: Market segment designation\ndistribution_channel: Booking distribution channel\nis_repeated_guest: Whether the guest is a repeated guest (0: No, 1: Yes)\nprevious_cancellations, previous_bookings_not_canceled: Number of previous bookings that were canceled or not canceled\nreserved_room_type, assigned_room_type: Code of room type reserved and assigned\nbooking_changes: Number of changes made to the booking\ndeposit_type: Type of deposit made\nagent, company: ID of the travel agent and company that made the booking\ndays_in_waiting_list: Number of days the booking was in the waiting list\ncustomer_type: Type of customer (Transient, Contract, Group, Transient-Party)\nadr: Average Daily Rate\nrequired_car_parking_spaces: Number of car parking spaces required\ntotal_of_special_requests: Total number of special requests made by the guest\nreservation_status: Reservation last status (Canceled, Check-Out, No-Show)\nreservation_status_date: Date at which the last status was set\n\n\n\n\nTasks:\n\nData Wrangling (40%)\n\nImport the CSV file into Power BI.\nClean the data, addressing any inconsistencies, missing values, or errors.\nCreate at least 3 calculated measures. Ensure that your calculated measures are meaningful and correctly implemented.\nDocument your data wrangling process within the Power BI file.\n\nDashboard Creation (50%).\nCreate a dashboard with at least 5 different types of visualizations. Use appropriate chart types (bar charts, line charts, pie charts, etc.) to display the data effectively. Include slicers and filters to allow users to interact with the data (e.g., filter by hotel type, date range, market segment). Ensure that your visualizations are interactive and provide meaningful insights. For example, you can identify key performance indicators (KPIs) relevant to hotel management. Examples include: Occupancy Rate, Cancellation Rate, Average Daily Rate (ADR), Revenue Per Available Room (RevPAR), Number of Repeat Guests, Average Lead Time\nPresentation and Usability (10%)\n\nOrganize your dashboard logically and aesthetically.\nProvide clear titles and labels for all visualizations.\nBased on the visualized data, provide insights and actionable recommendations for hotel management. Add this to the last page of your dashboard that should serve as a report to highlight any trends, anomalies, or areas that require attention.\n\nSubmit one Power BI file (.pbix) showing your data preparation, calculated measures, and final dashboard.\n\nSubmit\n\n\n\n\n\n\nImportant\n\n\n\nNotes: This assignment requires you to submit 2 files: 1 .sql file, 1 .pbix file.",
    "crumbs": [
      "Assessment",
      "Individual Assignment"
    ]
  },
  {
    "objectID": "weeks/week-3.html#supplementary-own-timeclose-document-tab",
    "href": "weeks/week-3.html#supplementary-own-timeclose-document-tab",
    "title": "Week 3",
    "section": "Supplementary (own-time)",
    "text": "Supplementary (own-time)\n🔕 You do not have to complete these but if you wish you can.\n📚 Introduction to R\n💣 Get started with Power BI Desktop\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly Schedule",
      "Week 3"
    ]
  },
  {
    "objectID": "notes/w4.html#data-visualization-with-power-bi",
    "href": "notes/w4.html#data-visualization-with-power-bi",
    "title": "Lecture Note Week 4",
    "section": "Data Visualization with Power BI",
    "text": "Data Visualization with Power BI\n\n\n1. PivotTables\nPurpose:\n\nPivotTables: Summarize, analyze, explore, and present summary data.\nPivotCharts: Visualize the data from PivotTables.\n\nCreating PivotTables in Power BI:\n\nImport Data: Load your dataset into Power BI.\n\nGo to Home &gt; Get Data &gt; Excel (or choose your data source).\nSelect your file and click Load.\n\nCreate PivotTable:\n\nGo to Visualization &gt; Build Visual &gt; Choose Table.\n\nVisualize Data:\n\nDrag fields into Rows, Columns, and Values areas in the Visualizations pane.\nAdjust the fields to refine your table.\n\n\n\n\n2. Scatter Charts\nPurpose: Examine relationships between two variables.\nCreating Scatter Charts in Power BI:\n\nSelect Scatter Chart:\n\nGo to the Visualizations pane.\nClick on the Scatter Chart icon.\n\nAdd Data Fields:\n\nDrag numerical fields to the X-axis and Y-axis.\nOptionally, add a third numerical field to the Size field well.\n\nCustomize and Interpret:\n\n\nUse the Format pane to customize the chart’s appearance.\nInterpret the relationship between variables by examining the pattern of points.\n\n\n\n\n3. Line Charts, Bar Charts, and Column Charts\nPurpose:\n\nLine Charts: Show trends over time.\nBar Charts: Compare quantities across categories.\nColumn Charts: Similar to bar charts, but vertical.\n\nCreating Line Charts in Power BI:\n\nSelect Line Chart:\n\nGo to the Visualizations pane.\nClick on the Line Chart icon.\n\nAdd Data Fields:\n\nDrag a date field to the X-axis and a numerical field to the Y-axis.\n\n\nCreating Bar and Column Charts in Power BI:\n\nSelect Bar/Column Chart:\n\nGo to the Visualizations pane.\nClick on the Bar Chart or Column Chart icon.\n\nAdd Data Fields:\n\nDrag a categorical field to the Axis area and a numerical field to the Values area.\n\n\n\n\n4. Sorted, Clustered, and Stacked Bar (Column) Charts\nPurpose:\n\nSorted Bar/Column Charts: Rank data in ascending or descending order.\nClustered Bar/Column Charts: Compare multiple categories across a single axis.\nStacked Bar/Column Charts: Show composition and compare totals.\n\nCreating Sorted Bar/Column Charts:\n\nSort Data:\n\nClick on the ellipsis (…) in the top right corner of the chart.\nSelect Sort axis and choose the desired field.\n\n\nCreating Clustered Bar/Column Charts:\n\nSelect Clustered Chart:\n\nGo to the Visualizations pane.\nClick on the Clustered Bar Chart or Clustered Column Chart icon.\n\nAdd Data Fields:\n\nDrag multiple categorical fields to the Axis area and a numerical field to the Values area.\n\n\nCreating Stacked Bar/Column Charts:\n\nSelect Stacked Chart:\n\nGo to the Visualizations pane.\nClick on the Stacked Bar Chart or Stacked Column Chart icon.\n\nAdd Data Fields:\n\nDrag a categorical field to the Axis area and another categorical field to the Legend area.\n\n\n\n\n5. Bubble Charts\nPurpose:\n\nBubble Charts: Visualize three dimensions of data.\n\nCreating Bubble Charts in Power BI:\n\nSelect Bubble Chart:\n\nGo to the Visualizations pane.\nClick on the Scatter Chart icon and add a field to the Size well.\n\nAdd Data Fields:\n\nDrag fields to the X-axis, Y-axis, and Size areas.\n\n\n\n\n6. Advanced Visualization Types\nPurpose:\n\nHeat Maps: Show data density or intensity.\nSparklines: Show trends in a small space.\nTreemaps: Visualize hierarchical data.\nWaterfall Charts: Show cumulative effects.\nStock Charts: Display financial data.\nParallel Coordinates Plots: Visualize multi-dimensional data.\n\nCreating Heat Maps in Power BI:\n\nSelect Heat Map:\n\nUse the Shape Map visual in the Visualizations pane.\n\nAdd Data Fields:\n\nDrag fields to the Location and Value areas.\n\n\nCreating Sparklines in Power BI:\n\nAdd Sparklines:\n\nGo to Visualizations pane.\nUse the Line and clustered column chart visual.\n\nAdd Data Fields:\n\nDrag fields to the Line Values area.\n\n\nCreating Treemaps in Power BI:\n\nSelect Treemap:\n\nGo to the Visualizations pane.\nClick on the Treemap icon.\n\nAdd Data Fields:\n\nDrag hierarchical fields to the Group and Values areas.\n\n\nCreating Waterfall Charts in Power BI:\n\nSelect Waterfall Chart:\n\nGo to the Visualizations pane.\nClick on the Waterfall Chart icon.\n\nAdd Data Fields:\n\nDrag fields to the Category and Y-axis areas.\n\n\n\n\n7. Geospatial Data Visualization\nPurpose:\n\nChoropleth Maps: Visualize data across geographical regions.\nCartograms: Distort map shapes to represent data values.\n\nCreating Choropleth Maps in Power BI:\n\nSelect Map:\n\nGo to the Visualizations pane.\nClick on the Map or Filled Map icon.\n\nAdd Data Fields:\n\nDrag geographical fields to the Location area and data fields to the Values area."
  },
  {
    "objectID": "notes/w4.html#data-visualization-with-r",
    "href": "notes/w4.html#data-visualization-with-r",
    "title": "Lecture Note Week 4",
    "section": "Data Visualization with R",
    "text": "Data Visualization with R\n\n1. Introduction to Base Plotting in R\nBase R provides a powerful system for creating a wide range of visualizations without the need for additional packages. While ggplot2 and other libraries have gained popularity, base plotting remains an essential tool due to its simplicity, speed, and integration with the rest of R’s functionality.\n\n\n2. Basic Plot Types\n\nScatter Plot (plot)\n\nThe plot() function is versatile and can create different types of plots depending on the input data. It’s most commonly used for scatter plots.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLine Plot (plot with type = “l”)\n\nSetting type = \"l\" in the plot() function produces a line plot.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHistogram (hist)\n\nThe hist() function creates a histogram, useful for visualizing the distribution of a dataset.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBoxplot (boxplot)\n\nThe boxplot() function provides a visualization of the distribution of data across different categories.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBarplot (barplot)\n\nThe barplot() function is used to create bar charts.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3. Customizing Plots\n\nTitles and Labels\n\nYou can add titles, axis labels, and customize them using main, xlab, and ylab arguments.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nColors and Point Types\n\nCustomize the appearance of points and lines using arguments like col, pch, and lty.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMultiple Plots\n\nYou can create multiple plots in a single window using par(mfrow = c(nrows, ncols)).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLegends\n\nAdd legends using the legend() function to describe different elements in your plot.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n4. Advanced Customizations\n\nPlot Annotations\n\nUse text() to add text annotations to your plots.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAxes Customization\n\nYou can control axis limits, labels, and ticks using xlim, ylim, axes = FALSE, and axis().\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAdding Gridlines\n\nUse the grid() function to add gridlines to a plot.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n5. Saving Plots\n\nSaving to File\n\nUse functions like pdf(), png(), jpeg(), etc., to save plots to files.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n6. Conclusion\nBase R plotting is a flexible and powerful tool for creating a wide range of visualizations. While it may lack the more advanced features of libraries like ggplot2, its simplicity and speed make it an essential tool for quick data exploration and visualization."
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Week 4",
    "section": "",
    "text": "Read textbook Chapter 3.1, 3.2, 3.6.",
    "crumbs": [
      "Weekly Schedule",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#pre-class-activities-own-time",
    "href": "weeks/week-4.html#pre-class-activities-own-time",
    "title": "Week 4",
    "section": "",
    "text": "Read textbook Chapter 3.1, 3.2, 3.6.",
    "crumbs": [
      "Weekly Schedule",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#lecture-real-time",
    "href": "weeks/week-4.html#lecture-real-time",
    "title": "Week 4",
    "section": "Lecture (real-time)",
    "text": "Lecture (real-time)\n🖥️ Lecture 4 - Data Visualization\n\n\n📋 Note - Data Wrangling using R and Power BI",
    "crumbs": [
      "Weekly Schedule",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#tutorial-real-time",
    "href": "weeks/week-4.html#tutorial-real-time",
    "title": "Week 4",
    "section": "Tutorial (real-time)",
    "text": "Tutorial (real-time)\n📋 Tutorial 4",
    "crumbs": [
      "Weekly Schedule",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#quiz-own-time",
    "href": "weeks/week-4.html#quiz-own-time",
    "title": "Week 4",
    "section": "Quiz (own-time)",
    "text": "Quiz (own-time)\n🧪 Quiz 4",
    "crumbs": [
      "Weekly Schedule",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#supplementary-own-time",
    "href": "weeks/week-4.html#supplementary-own-time",
    "title": "Week 4",
    "section": "Supplementary (own-time)",
    "text": "Supplementary (own-time)\n🔕 You do not have to complete these but if you wish you can.\nPlease remember that in order to master to fullness on data visualization in R, you might want to learn ggplot2 package. Since ggplot2 is extremely useful, I will stroungly encourage you to take ETX2250/ETF5922.\nI have created the learning material for Clayton student years ago and if you are just interested to know how it works. The following link might help.\nhttps://ebsmonash.shinyapps.io/ida-W3/\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly Schedule",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-5.html#quiz-own-time",
    "href": "weeks/week-5.html#quiz-own-time",
    "title": "Week 5",
    "section": "Quiz (own-time)",
    "text": "Quiz (own-time)\n🧪 Quiz 5",
    "crumbs": [
      "Weekly Schedule",
      "Week 5"
    ]
  },
  {
    "objectID": "weeks/week-4.html#power-bi-instructions-video-own-time",
    "href": "weeks/week-4.html#power-bi-instructions-video-own-time",
    "title": "Week 4",
    "section": "Power BI Instructions Video (own-time)",
    "text": "Power BI Instructions Video (own-time)\n\nCreate Relationship\n\n\n\n\nCreating Dashboard\n\n\n\n\nInserting Textbox\n\n\n\n\nDuplicating Page\n\n\n\n\nAction Button\n\n\n\n\nCard Visual\n\n\n\n\nColumn Chart\n\n\n\n\nDoughnut Chart\n\n\n\n\nTable Visual\n\n\n\n\nInteractivity\n\n\n\n\nSmall Multiples\n\n\n\n\nSlicers\n\n\n\n\nMap\n\n\n\n\nDrill Down",
    "crumbs": [
      "Weekly Schedule",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Week 6",
    "section": "",
    "text": "Important\n\n\n\n\nIndividual assignment 1 due in 1 weeks!",
    "crumbs": [
      "Weekly Schedule",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#pre-class-activities-own-time",
    "href": "weeks/week-6.html#pre-class-activities-own-time",
    "title": "Week 6",
    "section": "Pre-class activities (own-time)",
    "text": "Pre-class activities (own-time)\n📖 Read Business Analytics, 5th Edition: Chapter 5",
    "crumbs": [
      "Weekly Schedule",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#lecture-real-time",
    "href": "weeks/week-6.html#lecture-real-time",
    "title": "Week 6",
    "section": "Lecture (real-time)",
    "text": "Lecture (real-time)\n🖥️ Lecture 6 - Introduction to Modelling Uncertainty",
    "crumbs": [
      "Weekly Schedule",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#tutorial-real-time",
    "href": "weeks/week-6.html#tutorial-real-time",
    "title": "Week 6",
    "section": "Tutorial (real-time)",
    "text": "Tutorial (real-time)\n📋 Tutorial 6 - Descriptive Statistics",
    "crumbs": [
      "Weekly Schedule",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#quiz-own-time",
    "href": "weeks/week-6.html#quiz-own-time",
    "title": "Week 6",
    "section": "Quiz (own-time)",
    "text": "Quiz (own-time)\n🧪 Quiz 6",
    "crumbs": [
      "Weekly Schedule",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week-6.html#supplementary-own-time",
    "href": "weeks/week-6.html#supplementary-own-time",
    "title": "Week 6",
    "section": "Supplementary (own-time)",
    "text": "Supplementary (own-time)\n🔕 Revisit your first year statistics on the topic of probability.\n\n\nBack to course schedule ⏎",
    "crumbs": [
      "Weekly Schedule",
      "Week 6"
    ]
  }
]